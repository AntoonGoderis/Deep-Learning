{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntoonGoderis/Deep-Learning/blob/main/assignment_2_random_interpret.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV06oWXBSQT7",
        "outputId": "eb915c6c-986e-4836-e3cc-e3c181f53941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.11/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import json\n",
        "import ast\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.optimizers import SGD, AdamW\n",
        "\n",
        "!pip install keras-tuner\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Input\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import keras_tuner as kt\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import zipfile\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as pi_efficient\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as pi_resnet\n",
        "\n",
        "from tensorflow.keras import layers, models, preprocessing\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8kzGlxEWT3M",
        "outputId": "9ccd4d90-c853-4ef2-ee0d-a09227158205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Replace 'path/to/your_file.zip' with the path to your zip file in Google Drive\n",
        "zip_file_path = '/content/drive/My Drive/Assign2/training_data.zip'\n",
        "extract_path = '/content/simpsons-mnist-master/dataset/rgb/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "zip_file_path = '/content/drive/My Drive/Assign2/test_data.zip'\n",
        "extract_path = '/content/simpsons-mnist-master/dataset/rgb/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "Cd00NcLYSQUA",
        "outputId": "8b07a10b-6200-460a-f57e-0524f7942640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          image_path          label\n",
            "0  /content/simpsons-mnist-master/dataset/rgb/tra...  marge_simpson\n",
            "1  /content/simpsons-mnist-master/dataset/rgb/tra...  marge_simpson\n",
            "2  /content/simpsons-mnist-master/dataset/rgb/tra...  marge_simpson\n",
            "3  /content/simpsons-mnist-master/dataset/rgb/tra...  marge_simpson\n",
            "4  /content/simpsons-mnist-master/dataset/rgb/tra...  marge_simpson\n",
            "                                          image_path          label\n",
            "0  /content/simpsons-mnist-master/dataset/rgb/tes...  marge_simpson\n",
            "1  /content/simpsons-mnist-master/dataset/rgb/tes...  marge_simpson\n",
            "2  /content/simpsons-mnist-master/dataset/rgb/tes...  marge_simpson\n",
            "3  /content/simpsons-mnist-master/dataset/rgb/tes...  marge_simpson\n",
            "4  /content/simpsons-mnist-master/dataset/rgb/tes...  marge_simpson\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor class_name in os.listdir(test_rotated_path):\\n\\n    class_path = os.path.join(test_rotated_path, class_name)\\n\\n    if os.path.isdir(class_path):\\n        for img_name in os.listdir(class_path):\\n            test_r_data.append((os.path.join(class_path, img_name), class_name))\\n\\ndf_test_r = pd.DataFrame(test_r_data, columns=[\"image_path\", \"label\"])\\nprint(df_test_r.head())\\n\\nfor class_name in os.listdir(test_zoomed_in_path):\\n\\n    class_path = os.path.join(test_zoomed_in_path, class_name)\\n\\n    if os.path.isdir(class_path):\\n        for img_name in os.listdir(class_path):\\n            test_zi_data.append((os.path.join(class_path, img_name), class_name))\\n\\ndf_test_zi = pd.DataFrame(test_zi_data, columns=[\"image_path\", \"label\"])\\nprint(df_test_zi.head())\\n\\nfor class_name in os.listdir(test_zoomed_out_path):\\n\\n    class_path = os.path.join(test_zoomed_out_path, class_name)\\n\\n    if os.path.isdir(class_path):\\n        for img_name in os.listdir(class_path):\\n            test_zo_data.append((os.path.join(class_path, img_name), class_name))\\n\\ndf_test_zo = pd.DataFrame(test_zo_data, columns=[\"image_path\", \"label\"])\\nprint(df_test_zo.head())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "base_dir = os.getcwd()\n",
        "\n",
        "train_path = os.path.join(base_dir, \"simpsons-mnist-master\", \"dataset\", \"rgb\", \"train\")\n",
        "test_path = os.path.join(base_dir, \"simpsons-mnist-master\", \"dataset\", \"rgb\", \"test\")\n",
        "#test_rotated_path = os.path.join(base_dir, \"simpsons-mnist-master\", \"dataset\", \"rgb\", \"test_rotated\")\n",
        "#test_zoomed_in_path = os.path.join(base_dir, \"simpsons-mnist-master\", \"dataset\", \"rgb\", \"test_zoomed_in\")\n",
        "#test_zoomed_out_path = os.path.join(base_dir, \"simpsons-mnist-master\", \"dataset\", \"rgb\", \"test_zoomed_out\")\n",
        "\n",
        "image_data = []\n",
        "test_data = []\n",
        "test_r_data = []\n",
        "test_zi_data = []\n",
        "test_zo_data = []\n",
        "\n",
        "for class_name in os.listdir(train_path):\n",
        "\n",
        "    class_path = os.path.join(train_path, class_name)\n",
        "\n",
        "    if os.path.isdir(class_path):\n",
        "        for img_name in os.listdir(class_path):\n",
        "            image_data.append((os.path.join(class_path, img_name), class_name))\n",
        "\n",
        "df_train = pd.DataFrame(image_data, columns=[\"image_path\", \"label\"])\n",
        "print(df_train.head())\n",
        "\n",
        "for class_name in os.listdir(test_path):\n",
        "\n",
        "    class_path = os.path.join(test_path, class_name)\n",
        "\n",
        "    if os.path.isdir(class_path):\n",
        "        for img_name in os.listdir(class_path):\n",
        "            test_data.append((os.path.join(class_path, img_name), class_name))\n",
        "\n",
        "\n",
        "df_test = pd.DataFrame(test_data, columns=[\"image_path\", \"label\"])\n",
        "print(df_test.head())\n",
        "\n",
        "\"\"\"\n",
        "for class_name in os.listdir(test_rotated_path):\n",
        "\n",
        "    class_path = os.path.join(test_rotated_path, class_name)\n",
        "\n",
        "    if os.path.isdir(class_path):\n",
        "        for img_name in os.listdir(class_path):\n",
        "            test_r_data.append((os.path.join(class_path, img_name), class_name))\n",
        "\n",
        "df_test_r = pd.DataFrame(test_r_data, columns=[\"image_path\", \"label\"])\n",
        "print(df_test_r.head())\n",
        "\n",
        "for class_name in os.listdir(test_zoomed_in_path):\n",
        "\n",
        "    class_path = os.path.join(test_zoomed_in_path, class_name)\n",
        "\n",
        "    if os.path.isdir(class_path):\n",
        "        for img_name in os.listdir(class_path):\n",
        "            test_zi_data.append((os.path.join(class_path, img_name), class_name))\n",
        "\n",
        "df_test_zi = pd.DataFrame(test_zi_data, columns=[\"image_path\", \"label\"])\n",
        "print(df_test_zi.head())\n",
        "\n",
        "for class_name in os.listdir(test_zoomed_out_path):\n",
        "\n",
        "    class_path = os.path.join(test_zoomed_out_path, class_name)\n",
        "\n",
        "    if os.path.isdir(class_path):\n",
        "        for img_name in os.listdir(class_path):\n",
        "            test_zo_data.append((os.path.join(class_path, img_name), class_name))\n",
        "\n",
        "df_test_zo = pd.DataFrame(test_zo_data, columns=[\"image_path\", \"label\"])\n",
        "print(df_test_zo.head())\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXNkUZtnSQUC",
        "outputId": "15f753a4-b1b3-4982-c61b-4348d874da21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28, 3)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "def load_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Could not load image at {img_path}\")\n",
        "        return None\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "df_train[\"image_array\"] = df_train[\"image_path\"].apply(lambda x: load_image(x))\n",
        "df_test[\"image_array\"] = df_test[\"image_path\"].apply(lambda x: load_image(x))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def load_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "df_train[\"image_array\"] = df_train[\"image_path\"].apply(lambda x: load_image(x))\n",
        "\n",
        "df_test[\"image_array\"] = df_test[\"image_path\"].apply(lambda x: load_image(x))\n",
        "\n",
        "df_test_r[\"image_array\"] = df_test_r[\"image_path\"].apply(lambda x: load_image(x))\n",
        "\n",
        "df_test_zi[\"image_array\"] = df_test_zi[\"image_path\"].apply(lambda x: load_image(x))\n",
        "\n",
        "df_test_zo[\"image_array\"] = df_test_zo[\"image_path\"].apply(lambda x: load_image(x))\n",
        "\"\"\"\n",
        "\n",
        "sample_img = df_train[\"image_array\"][0]\n",
        "print(sample_img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEE309y4R39q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T059PBYWSQUC",
        "outputId": "f8f414c6-d896-4cba-d137-1ef3c1346f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ],
      "source": [
        "num_classes = df_train.label.nunique()\n",
        "print(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJsnNwOXSQUC",
        "outputId": "70c8a53b-1f4d-4fe4-d27e-90f7bf00a572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8000, 28, 28, 3)\n",
            "(8000, 10)\n"
          ]
        }
      ],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "df_train[\"label_encoded\"] = label_encoder.fit_transform(df_train[\"label\"])\n",
        "df_test[\"label_encoded\"] = label_encoder.transform(df_test[\"label\"])\n",
        "\"\"\"\n",
        "df_test_r[\"label_encoded\"] = label_encoder.transform(df_test_r[\"label\"])\n",
        "df_test_zi[\"label_encoded\"] = label_encoder.transform(df_test_zi[\"label\"])\n",
        "df_test_zo[\"label_encoded\"] = label_encoder.transform(df_test_zo[\"label\"])\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "X_train_valid = np.stack(df_train[\"image_array\"].values)\n",
        "\n",
        "\n",
        "y_train_valid = to_categorical(df_train[\"label_encoded\"], num_classes=num_classes)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.3, random_state=42)\n",
        "\n",
        "X_test = np.stack(df_test[\"image_array\"].values)\n",
        "y_test = to_categorical(df_test[\"label_encoded\"], num_classes=num_classes)\n",
        "\"\"\"\n",
        "X_test_r = np.stack(df_test_r[\"image_array\"].values)\n",
        "y_test_r = to_categorical(df_test_r[\"label_encoded\"], num_classes=num_classes)\n",
        "\n",
        "X_test_zi = np.stack(df_test_zi[\"image_array\"].values)\n",
        "y_test_zi = to_categorical(df_test_zi[\"label_encoded\"], num_classes=num_classes)\n",
        "\n",
        "X_test_zo = np.stack(df_test_zo[\"image_array\"].values)\n",
        "y_test_zo = to_categorical(df_test_zo[\"label_encoded\"], num_classes=num_classes)\n",
        "\"\"\"\n",
        "\n",
        "print(X_train_valid.shape)\n",
        "print(y_train_valid.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POZH9WcYd38P"
      },
      "outputs": [],
      "source": [
        "# first random search / broad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUTUxJo-d4jT",
        "outputId": "d32fd033-522c-40ed-acb1-3d545d09bb0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Model with 6 layers, 272 neurons, L2=0.001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 404ms/step - accuracy: 0.1814 - loss: 3.7062 - val_accuracy: 0.1246 - val_loss: 3.6915\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.3423 - loss: 2.8561 - val_accuracy: 0.1067 - val_loss: 3.1301\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4358 - loss: 2.3164 - val_accuracy: 0.1758 - val_loss: 2.9293\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4769 - loss: 2.0318 - val_accuracy: 0.2167 - val_loss: 2.8386\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5241 - loss: 1.8028 - val_accuracy: 0.1704 - val_loss: 2.5362\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5438 - loss: 1.7158 - val_accuracy: 0.4404 - val_loss: 2.1082\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5826 - loss: 1.6401 - val_accuracy: 0.5500 - val_loss: 1.7445\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5969 - loss: 1.5409 - val_accuracy: 0.4371 - val_loss: 2.1549\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6374 - loss: 1.4811 - val_accuracy: 0.5546 - val_loss: 1.7796\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.6314 - loss: 1.5009 - val_accuracy: 0.4954 - val_loss: 1.9489\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6538 - loss: 1.4440 - val_accuracy: 0.5092 - val_loss: 2.0797\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6620 - loss: 1.4478 - val_accuracy: 0.6150 - val_loss: 1.6069\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6801 - loss: 1.4061 - val_accuracy: 0.5150 - val_loss: 1.9672\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6891 - loss: 1.3863 - val_accuracy: 0.5517 - val_loss: 1.9006\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6715 - loss: 1.4071 - val_accuracy: 0.4975 - val_loss: 2.5874\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6929 - loss: 1.3545 - val_accuracy: 0.3929 - val_loss: 4.1543\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6866 - loss: 1.3877 - val_accuracy: 0.5383 - val_loss: 1.8646\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7035 - loss: 1.3467 - val_accuracy: 0.5038 - val_loss: 1.9263\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7019 - loss: 1.3671 - val_accuracy: 0.6404 - val_loss: 1.6891\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7237 - loss: 1.3169 - val_accuracy: 0.6467 - val_loss: 1.5136\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7279 - loss: 1.3115 - val_accuracy: 0.5408 - val_loss: 2.0858\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7180 - loss: 1.3161 - val_accuracy: 0.6042 - val_loss: 1.7558\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7120 - loss: 1.3280 - val_accuracy: 0.6737 - val_loss: 1.3932\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7215 - loss: 1.3044 - val_accuracy: 0.6212 - val_loss: 1.6541\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7298 - loss: 1.3070 - val_accuracy: 0.5021 - val_loss: 2.3649\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7213 - loss: 1.3385 - val_accuracy: 0.4417 - val_loss: 2.8498\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7333 - loss: 1.2952 - val_accuracy: 0.6946 - val_loss: 1.4830\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7384 - loss: 1.2696 - val_accuracy: 0.4292 - val_loss: 3.1978\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7305 - loss: 1.2815 - val_accuracy: 0.6942 - val_loss: 1.4744\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7450 - loss: 1.2627 - val_accuracy: 0.6596 - val_loss: 1.5269\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7251 - loss: 1.3238 - val_accuracy: 0.4613 - val_loss: 3.3776\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7364 - loss: 1.2850 - val_accuracy: 0.6983 - val_loss: 1.4027\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7555 - loss: 1.2309 - val_accuracy: 0.6029 - val_loss: 1.9740\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7623 - loss: 1.2179 - val_accuracy: 0.6908 - val_loss: 1.4305\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7382 - loss: 1.2681 - val_accuracy: 0.6842 - val_loss: 1.4892\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7503 - loss: 1.2453 - val_accuracy: 0.6383 - val_loss: 1.6921\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7686 - loss: 1.2333 - val_accuracy: 0.6925 - val_loss: 1.4573\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7615 - loss: 1.2234 - val_accuracy: 0.5383 - val_loss: 2.1586\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7538 - loss: 1.2344 - val_accuracy: 0.5679 - val_loss: 1.9252\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7593 - loss: 1.2211 - val_accuracy: 0.7175 - val_loss: 1.4124\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7666 - loss: 1.2087 - val_accuracy: 0.7138 - val_loss: 1.4615\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7659 - loss: 1.1850 - val_accuracy: 0.6167 - val_loss: 1.9259\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7666 - loss: 1.2058 - val_accuracy: 0.6633 - val_loss: 1.5656\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7646 - loss: 1.1972 - val_accuracy: 0.6787 - val_loss: 1.5363\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7816 - loss: 1.1599 - val_accuracy: 0.6496 - val_loss: 1.6717\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7724 - loss: 1.1812 - val_accuracy: 0.7837 - val_loss: 1.1900\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7652 - loss: 1.2045 - val_accuracy: 0.7100 - val_loss: 1.4323\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7888 - loss: 1.1479 - val_accuracy: 0.7500 - val_loss: 1.3643\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7672 - loss: 1.1886 - val_accuracy: 0.7458 - val_loss: 1.3663\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7845 - loss: 1.1738 - val_accuracy: 0.7663 - val_loss: 1.2580\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7811 - loss: 1.1714 - val_accuracy: 0.7212 - val_loss: 1.3947\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7755 - loss: 1.1651 - val_accuracy: 0.7667 - val_loss: 1.1710\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7888 - loss: 1.1458 - val_accuracy: 0.6271 - val_loss: 1.9417\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7875 - loss: 1.1381 - val_accuracy: 0.6758 - val_loss: 1.5625\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7800 - loss: 1.1512 - val_accuracy: 0.7067 - val_loss: 1.4369\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7939 - loss: 1.1358 - val_accuracy: 0.7575 - val_loss: 1.3426\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7944 - loss: 1.1196 - val_accuracy: 0.6871 - val_loss: 1.5637\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7834 - loss: 1.1729 - val_accuracy: 0.6712 - val_loss: 1.5274\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7973 - loss: 1.0909 - val_accuracy: 0.6458 - val_loss: 1.7605\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7968 - loss: 1.1015 - val_accuracy: 0.6804 - val_loss: 1.7483\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7909 - loss: 1.1222 - val_accuracy: 0.7508 - val_loss: 1.2510\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7875 - loss: 1.1159 - val_accuracy: 0.7371 - val_loss: 1.4226\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7961 - loss: 1.0951 - val_accuracy: 0.6888 - val_loss: 1.4209\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7994 - loss: 1.1072 - val_accuracy: 0.7800 - val_loss: 1.2317\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8060 - loss: 1.0925 - val_accuracy: 0.7758 - val_loss: 1.1649\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8086 - loss: 1.0778 - val_accuracy: 0.6342 - val_loss: 2.0073\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8105 - loss: 1.0827 - val_accuracy: 0.6633 - val_loss: 1.6191\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8071 - loss: 1.0789 - val_accuracy: 0.6812 - val_loss: 1.4913\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.8156 - loss: 1.0617 - val_accuracy: 0.4325 - val_loss: 2.8520\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8139 - loss: 1.0700 - val_accuracy: 0.7479 - val_loss: 1.3482\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7984 - loss: 1.1021 - val_accuracy: 0.7833 - val_loss: 1.1705\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8112 - loss: 1.0639 - val_accuracy: 0.6917 - val_loss: 1.3929\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8145 - loss: 1.0470 - val_accuracy: 0.7358 - val_loss: 1.3346\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8108 - loss: 1.0516 - val_accuracy: 0.6083 - val_loss: 2.5340\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.8076 - loss: 1.0778 - val_accuracy: 0.8054 - val_loss: 1.1391\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8218 - loss: 1.0151 - val_accuracy: 0.7796 - val_loss: 1.2000\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8043 - loss: 1.1020 - val_accuracy: 0.7575 - val_loss: 1.2223\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8107 - loss: 1.0561 - val_accuracy: 0.7504 - val_loss: 1.2871\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8089 - loss: 1.0556 - val_accuracy: 0.7275 - val_loss: 1.3540\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8103 - loss: 1.0535 - val_accuracy: 0.7954 - val_loss: 1.1118\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8229 - loss: 1.0307 - val_accuracy: 0.7483 - val_loss: 1.2532\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8205 - loss: 1.0357 - val_accuracy: 0.7367 - val_loss: 1.3564\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8144 - loss: 1.0472 - val_accuracy: 0.8258 - val_loss: 1.0263\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8298 - loss: 1.0125 - val_accuracy: 0.7321 - val_loss: 1.3987\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8187 - loss: 1.0318 - val_accuracy: 0.7658 - val_loss: 1.1875\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8197 - loss: 1.0227 - val_accuracy: 0.8108 - val_loss: 1.0629\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8370 - loss: 0.9723 - val_accuracy: 0.7675 - val_loss: 1.2171\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8254 - loss: 1.0002 - val_accuracy: 0.5913 - val_loss: 1.9643\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8318 - loss: 1.0184 - val_accuracy: 0.7729 - val_loss: 1.1591\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8112 - loss: 1.0477 - val_accuracy: 0.7054 - val_loss: 1.3689\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8249 - loss: 0.9954 - val_accuracy: 0.7525 - val_loss: 1.3211\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8228 - loss: 0.9935 - val_accuracy: 0.7983 - val_loss: 1.0930\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8341 - loss: 0.9728 - val_accuracy: 0.7192 - val_loss: 1.3371\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8295 - loss: 0.9870 - val_accuracy: 0.7192 - val_loss: 1.5150\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8402 - loss: 0.9776 - val_accuracy: 0.8254 - val_loss: 1.0300\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8183 - loss: 1.0079 - val_accuracy: 0.7458 - val_loss: 1.2672\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8304 - loss: 0.9935 - val_accuracy: 0.5954 - val_loss: 1.8889\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8331 - loss: 0.9770 - val_accuracy: 0.6675 - val_loss: 2.0688\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8422 - loss: 0.9695 - val_accuracy: 0.7862 - val_loss: 1.2502\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8213 - loss: 0.9959 - val_accuracy: 0.6875 - val_loss: 1.9356\n",
            "Model Validation Accuracy: 0.8258\n",
            "\n",
            "Testing Model with 5 layers, 320 neurons, L2=0.005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 454ms/step - accuracy: 0.1995 - loss: 6.7980 - val_accuracy: 0.1246 - val_loss: 3.1557\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3360 - loss: 2.5055 - val_accuracy: 0.1004 - val_loss: 2.8677\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3540 - loss: 2.2410 - val_accuracy: 0.0867 - val_loss: 3.1105\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3899 - loss: 2.1100 - val_accuracy: 0.1058 - val_loss: 2.9053\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.4127 - loss: 2.1011 - val_accuracy: 0.1908 - val_loss: 2.7321\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4240 - loss: 2.0582 - val_accuracy: 0.2058 - val_loss: 2.5160\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4370 - loss: 2.0807 - val_accuracy: 0.4058 - val_loss: 2.2362\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4629 - loss: 1.9896 - val_accuracy: 0.1829 - val_loss: 3.1086\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4654 - loss: 2.0073 - val_accuracy: 0.3021 - val_loss: 2.6413\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.4892 - loss: 1.9992 - val_accuracy: 0.2288 - val_loss: 2.9588\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4901 - loss: 1.9685 - val_accuracy: 0.3975 - val_loss: 2.1794\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4770 - loss: 1.9866 - val_accuracy: 0.2146 - val_loss: 3.6488\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5192 - loss: 1.9193 - val_accuracy: 0.2925 - val_loss: 3.2494\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5128 - loss: 1.9672 - val_accuracy: 0.1708 - val_loss: 5.5632\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5462 - loss: 1.8938 - val_accuracy: 0.4412 - val_loss: 2.1454\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5363 - loss: 1.8771 - val_accuracy: 0.4100 - val_loss: 2.3150\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5597 - loss: 1.8598 - val_accuracy: 0.3642 - val_loss: 2.6246\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5494 - loss: 1.8724 - val_accuracy: 0.2488 - val_loss: 4.1239\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5768 - loss: 1.8465 - val_accuracy: 0.4704 - val_loss: 2.0521\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5548 - loss: 1.8572 - val_accuracy: 0.2113 - val_loss: 4.1646\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5724 - loss: 1.7982 - val_accuracy: 0.5004 - val_loss: 2.0144\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5837 - loss: 1.7751 - val_accuracy: 0.1821 - val_loss: 6.3423\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5914 - loss: 1.7493 - val_accuracy: 0.1742 - val_loss: 7.4366\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6027 - loss: 1.7608 - val_accuracy: 0.2571 - val_loss: 3.3121\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6027 - loss: 1.6726 - val_accuracy: 0.3929 - val_loss: 2.4700\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6101 - loss: 1.7084 - val_accuracy: 0.4517 - val_loss: 2.4411\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6177 - loss: 1.6674 - val_accuracy: 0.3700 - val_loss: 3.5475\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6178 - loss: 1.6510 - val_accuracy: 0.3758 - val_loss: 2.3013\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6308 - loss: 1.6115 - val_accuracy: 0.3800 - val_loss: 2.9541\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6251 - loss: 1.6208 - val_accuracy: 0.4117 - val_loss: 2.7795\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6487 - loss: 1.5580 - val_accuracy: 0.5271 - val_loss: 1.9201\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6371 - loss: 1.5815 - val_accuracy: 0.6229 - val_loss: 1.7526\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6507 - loss: 1.5746 - val_accuracy: 0.6037 - val_loss: 1.6978\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6739 - loss: 1.4905 - val_accuracy: 0.4704 - val_loss: 2.6330\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6391 - loss: 1.5818 - val_accuracy: 0.4250 - val_loss: 2.6905\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6625 - loss: 1.4900 - val_accuracy: 0.4517 - val_loss: 2.1098\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6359 - loss: 1.5329 - val_accuracy: 0.6483 - val_loss: 1.5569\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6700 - loss: 1.4581 - val_accuracy: 0.4954 - val_loss: 1.9849\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6675 - loss: 1.4792 - val_accuracy: 0.4092 - val_loss: 2.7203\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6675 - loss: 1.5074 - val_accuracy: 0.6150 - val_loss: 1.7391\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6745 - loss: 1.4254 - val_accuracy: 0.5333 - val_loss: 1.9982\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6826 - loss: 1.4465 - val_accuracy: 0.3233 - val_loss: 2.7068\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6889 - loss: 1.3842 - val_accuracy: 0.6317 - val_loss: 1.5862\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6793 - loss: 1.4204 - val_accuracy: 0.5817 - val_loss: 1.8084\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6820 - loss: 1.3983 - val_accuracy: 0.5096 - val_loss: 2.0673\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6884 - loss: 1.3905 - val_accuracy: 0.3421 - val_loss: 5.5139\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6883 - loss: 1.4064 - val_accuracy: 0.4858 - val_loss: 2.2615\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6870 - loss: 1.3816 - val_accuracy: 0.4467 - val_loss: 2.5226\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6954 - loss: 1.3756 - val_accuracy: 0.4796 - val_loss: 2.5816\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7013 - loss: 1.3423 - val_accuracy: 0.5142 - val_loss: 2.2481\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6982 - loss: 1.3498 - val_accuracy: 0.4721 - val_loss: 1.9740\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6897 - loss: 1.3525 - val_accuracy: 0.4204 - val_loss: 2.6947\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7138 - loss: 1.3469 - val_accuracy: 0.3063 - val_loss: 4.4092\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6934 - loss: 1.3847 - val_accuracy: 0.4550 - val_loss: 2.4838\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6992 - loss: 1.3532 - val_accuracy: 0.4117 - val_loss: 2.7942\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6946 - loss: 1.3626 - val_accuracy: 0.6233 - val_loss: 1.6346\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7105 - loss: 1.3350 - val_accuracy: 0.2942 - val_loss: 5.1180\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7023 - loss: 1.3619 - val_accuracy: 0.4896 - val_loss: 2.1333\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7201 - loss: 1.3247 - val_accuracy: 0.3746 - val_loss: 3.3629\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7138 - loss: 1.3131 - val_accuracy: 0.5079 - val_loss: 2.0314\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7037 - loss: 1.3369 - val_accuracy: 0.5100 - val_loss: 2.1253\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7088 - loss: 1.3084 - val_accuracy: 0.6533 - val_loss: 1.4936\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7113 - loss: 1.3134 - val_accuracy: 0.4883 - val_loss: 1.9758\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7238 - loss: 1.2610 - val_accuracy: 0.4879 - val_loss: 2.6654\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7155 - loss: 1.2641 - val_accuracy: 0.5650 - val_loss: 2.3039\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7185 - loss: 1.2695 - val_accuracy: 0.4246 - val_loss: 3.0151\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7170 - loss: 1.2963 - val_accuracy: 0.5671 - val_loss: 1.8202\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7232 - loss: 1.2709 - val_accuracy: 0.4675 - val_loss: 1.9213\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7428 - loss: 1.2252 - val_accuracy: 0.4433 - val_loss: 2.8066\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7313 - loss: 1.2797 - val_accuracy: 0.6400 - val_loss: 2.0098\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7144 - loss: 1.2909 - val_accuracy: 0.3933 - val_loss: 3.7143\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7319 - loss: 1.2446 - val_accuracy: 0.2279 - val_loss: 6.1239\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7268 - loss: 1.2567 - val_accuracy: 0.4579 - val_loss: 3.9757\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7330 - loss: 1.2664 - val_accuracy: 0.5129 - val_loss: 2.4215\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7281 - loss: 1.2840 - val_accuracy: 0.5587 - val_loss: 2.3368\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7283 - loss: 1.2389 - val_accuracy: 0.5450 - val_loss: 1.9056\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7437 - loss: 1.2336 - val_accuracy: 0.5913 - val_loss: 1.7944\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7283 - loss: 1.2702 - val_accuracy: 0.6379 - val_loss: 1.5119\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7439 - loss: 1.2313 - val_accuracy: 0.4829 - val_loss: 2.1749\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7289 - loss: 1.2716 - val_accuracy: 0.5408 - val_loss: 2.4720\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7409 - loss: 1.1989 - val_accuracy: 0.4967 - val_loss: 1.8747\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7354 - loss: 1.2403 - val_accuracy: 0.5562 - val_loss: 2.1049\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7426 - loss: 1.2167 - val_accuracy: 0.4750 - val_loss: 2.6088\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7457 - loss: 1.2003 - val_accuracy: 0.1746 - val_loss: 9.2081\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7469 - loss: 1.2164 - val_accuracy: 0.5283 - val_loss: 2.3252\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7458 - loss: 1.2204 - val_accuracy: 0.5450 - val_loss: 2.3271\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7346 - loss: 1.2214 - val_accuracy: 0.5679 - val_loss: 2.0457\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7565 - loss: 1.1608 - val_accuracy: 0.5704 - val_loss: 1.8108\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7515 - loss: 1.1873 - val_accuracy: 0.5375 - val_loss: 2.2716\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7512 - loss: 1.1817 - val_accuracy: 0.4221 - val_loss: 2.6268\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7424 - loss: 1.2049 - val_accuracy: 0.5938 - val_loss: 2.3618\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7526 - loss: 1.2113 - val_accuracy: 0.7296 - val_loss: 1.3151\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7516 - loss: 1.1872 - val_accuracy: 0.4667 - val_loss: 2.2284\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7474 - loss: 1.2058 - val_accuracy: 0.3704 - val_loss: 3.2226\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7512 - loss: 1.1917 - val_accuracy: 0.5342 - val_loss: 2.5478\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7492 - loss: 1.1876 - val_accuracy: 0.4200 - val_loss: 3.0371\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7367 - loss: 1.2028 - val_accuracy: 0.5158 - val_loss: 2.1053\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7414 - loss: 1.2251 - val_accuracy: 0.6117 - val_loss: 1.6381\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7414 - loss: 1.1748 - val_accuracy: 0.6575 - val_loss: 1.6203\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7515 - loss: 1.1778 - val_accuracy: 0.6354 - val_loss: 1.7162\n",
            "Model Validation Accuracy: 0.7296\n",
            "\n",
            "Testing Model with 6 layers, 320 neurons, L2=0.001\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 483ms/step - accuracy: 0.1870 - loss: 3.9508 - val_accuracy: 0.1604 - val_loss: 3.6286\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.3389 - loss: 2.9531 - val_accuracy: 0.1333 - val_loss: 3.5393\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4259 - loss: 2.2960 - val_accuracy: 0.0867 - val_loss: 3.1999\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.4946 - loss: 1.9816 - val_accuracy: 0.0908 - val_loss: 2.8565\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5537 - loss: 1.7681 - val_accuracy: 0.2392 - val_loss: 3.1379\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.5640 - loss: 1.7248 - val_accuracy: 0.3275 - val_loss: 2.4300\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5826 - loss: 1.6493 - val_accuracy: 0.3304 - val_loss: 2.6654\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6182 - loss: 1.5913 - val_accuracy: 0.5058 - val_loss: 2.3749\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.6247 - loss: 1.5400 - val_accuracy: 0.5779 - val_loss: 1.7869\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6169 - loss: 1.5433 - val_accuracy: 0.4883 - val_loss: 2.1104\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6432 - loss: 1.5122 - val_accuracy: 0.4346 - val_loss: 2.2239\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6410 - loss: 1.5367 - val_accuracy: 0.5696 - val_loss: 1.7534\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6555 - loss: 1.5119 - val_accuracy: 0.2608 - val_loss: 4.6091\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6510 - loss: 1.4984 - val_accuracy: 0.4596 - val_loss: 2.3371\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6564 - loss: 1.4787 - val_accuracy: 0.3762 - val_loss: 2.5400\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6678 - loss: 1.4608 - val_accuracy: 0.2829 - val_loss: 3.5211\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6603 - loss: 1.5099 - val_accuracy: 0.5975 - val_loss: 1.7212\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6818 - loss: 1.4416 - val_accuracy: 0.5471 - val_loss: 2.1153\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.6906 - loss: 1.4394 - val_accuracy: 0.6258 - val_loss: 1.6132\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6871 - loss: 1.4233 - val_accuracy: 0.5117 - val_loss: 1.9257\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7052 - loss: 1.4078 - val_accuracy: 0.4321 - val_loss: 3.0754\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7000 - loss: 1.4046 - val_accuracy: 0.6817 - val_loss: 1.4692\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6988 - loss: 1.3972 - val_accuracy: 0.5050 - val_loss: 2.5197\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6957 - loss: 1.4205 - val_accuracy: 0.6338 - val_loss: 1.6488\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7222 - loss: 1.3684 - val_accuracy: 0.5883 - val_loss: 1.7913\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7136 - loss: 1.3874 - val_accuracy: 0.5329 - val_loss: 2.1407\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6902 - loss: 1.4567 - val_accuracy: 0.6446 - val_loss: 1.6782\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7381 - loss: 1.3231 - val_accuracy: 0.3883 - val_loss: 3.8579\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7355 - loss: 1.3437 - val_accuracy: 0.5992 - val_loss: 1.9502\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7288 - loss: 1.3276 - val_accuracy: 0.6737 - val_loss: 1.5469\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7281 - loss: 1.3392 - val_accuracy: 0.5537 - val_loss: 1.8399\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7164 - loss: 1.3906 - val_accuracy: 0.6417 - val_loss: 1.7149\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7271 - loss: 1.3451 - val_accuracy: 0.6242 - val_loss: 1.7172\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7358 - loss: 1.3019 - val_accuracy: 0.4375 - val_loss: 2.2311\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7397 - loss: 1.3173 - val_accuracy: 0.6333 - val_loss: 1.7052\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7334 - loss: 1.3471 - val_accuracy: 0.6404 - val_loss: 1.5803\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7364 - loss: 1.3120 - val_accuracy: 0.5863 - val_loss: 2.0167\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7394 - loss: 1.3040 - val_accuracy: 0.6888 - val_loss: 1.3883\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7432 - loss: 1.3008 - val_accuracy: 0.7304 - val_loss: 1.3265\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7406 - loss: 1.2886 - val_accuracy: 0.6542 - val_loss: 1.6541\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7592 - loss: 1.2360 - val_accuracy: 0.6596 - val_loss: 1.6042\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7464 - loss: 1.2644 - val_accuracy: 0.6958 - val_loss: 1.5148\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7537 - loss: 1.2548 - val_accuracy: 0.6121 - val_loss: 1.7335\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7539 - loss: 1.2589 - val_accuracy: 0.7113 - val_loss: 1.5115\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7622 - loss: 1.2423 - val_accuracy: 0.5729 - val_loss: 2.0138\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7662 - loss: 1.2130 - val_accuracy: 0.6496 - val_loss: 1.6636\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7505 - loss: 1.2672 - val_accuracy: 0.7387 - val_loss: 1.3351\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7733 - loss: 1.2000 - val_accuracy: 0.5525 - val_loss: 1.8904\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7843 - loss: 1.1796 - val_accuracy: 0.6812 - val_loss: 1.4651\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7766 - loss: 1.2073 - val_accuracy: 0.7108 - val_loss: 1.4154\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7808 - loss: 1.2004 - val_accuracy: 0.6246 - val_loss: 1.9244\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7718 - loss: 1.2042 - val_accuracy: 0.7317 - val_loss: 1.3605\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7698 - loss: 1.2162 - val_accuracy: 0.7042 - val_loss: 1.4057\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7750 - loss: 1.2083 - val_accuracy: 0.6908 - val_loss: 1.4627\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.7742 - loss: 1.1983 - val_accuracy: 0.7683 - val_loss: 1.2936\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7877 - loss: 1.1483 - val_accuracy: 0.7421 - val_loss: 1.3681\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7768 - loss: 1.1970 - val_accuracy: 0.7812 - val_loss: 1.2163\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7899 - loss: 1.1465 - val_accuracy: 0.7254 - val_loss: 1.3327\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7998 - loss: 1.1322 - val_accuracy: 0.7254 - val_loss: 1.5028\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7723 - loss: 1.1819 - val_accuracy: 0.6446 - val_loss: 1.8593\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7977 - loss: 1.1593 - val_accuracy: 0.6562 - val_loss: 1.8577\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7977 - loss: 1.1228 - val_accuracy: 0.7333 - val_loss: 1.4111\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7898 - loss: 1.1377 - val_accuracy: 0.6246 - val_loss: 2.1254\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7967 - loss: 1.1518 - val_accuracy: 0.7617 - val_loss: 1.3619\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7848 - loss: 1.1578 - val_accuracy: 0.6629 - val_loss: 1.6149\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7911 - loss: 1.1713 - val_accuracy: 0.6867 - val_loss: 1.8471\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7999 - loss: 1.1215 - val_accuracy: 0.4437 - val_loss: 3.3835\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7866 - loss: 1.1477 - val_accuracy: 0.7175 - val_loss: 1.5389\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8090 - loss: 1.0878 - val_accuracy: 0.7287 - val_loss: 1.3339\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7981 - loss: 1.1058 - val_accuracy: 0.6750 - val_loss: 1.6993\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.7995 - loss: 1.1144 - val_accuracy: 0.8129 - val_loss: 1.0850\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8105 - loss: 1.0870 - val_accuracy: 0.7117 - val_loss: 1.4669\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8077 - loss: 1.1233 - val_accuracy: 0.7429 - val_loss: 1.4068\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8136 - loss: 1.0829 - val_accuracy: 0.6383 - val_loss: 1.7564\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8067 - loss: 1.0995 - val_accuracy: 0.7487 - val_loss: 1.2393\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8169 - loss: 1.0437 - val_accuracy: 0.7479 - val_loss: 1.3206\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8089 - loss: 1.0465 - val_accuracy: 0.6883 - val_loss: 1.5556\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.7975 - loss: 1.1371 - val_accuracy: 0.8308 - val_loss: 1.0539\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8074 - loss: 1.0761 - val_accuracy: 0.8317 - val_loss: 1.0151\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8141 - loss: 1.0701 - val_accuracy: 0.5042 - val_loss: 3.7590\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8100 - loss: 1.0605 - val_accuracy: 0.7917 - val_loss: 1.1451\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8180 - loss: 1.0461 - val_accuracy: 0.7529 - val_loss: 1.4504\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8106 - loss: 1.0697 - val_accuracy: 0.6075 - val_loss: 1.8915\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8245 - loss: 1.0588 - val_accuracy: 0.6712 - val_loss: 1.7842\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8126 - loss: 1.0686 - val_accuracy: 0.7500 - val_loss: 1.4385\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8139 - loss: 1.0605 - val_accuracy: 0.7100 - val_loss: 1.6025\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8125 - loss: 1.0815 - val_accuracy: 0.7996 - val_loss: 1.1412\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8123 - loss: 1.0398 - val_accuracy: 0.7375 - val_loss: 1.3538\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8205 - loss: 1.0205 - val_accuracy: 0.6737 - val_loss: 1.6507\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8158 - loss: 1.0665 - val_accuracy: 0.6608 - val_loss: 1.6953\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8250 - loss: 1.0436 - val_accuracy: 0.8242 - val_loss: 1.0724\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8199 - loss: 1.0428 - val_accuracy: 0.7708 - val_loss: 1.1530\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8147 - loss: 1.0407 - val_accuracy: 0.7317 - val_loss: 1.4238\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8310 - loss: 1.0135 - val_accuracy: 0.7504 - val_loss: 1.2009\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8393 - loss: 0.9803 - val_accuracy: 0.6596 - val_loss: 1.8693\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8236 - loss: 1.0184 - val_accuracy: 0.5054 - val_loss: 3.3891\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8239 - loss: 1.0486 - val_accuracy: 0.8029 - val_loss: 1.1317\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8083 - loss: 1.0602 - val_accuracy: 0.7458 - val_loss: 1.3553\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8254 - loss: 1.0054 - val_accuracy: 0.7312 - val_loss: 1.4014\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8289 - loss: 1.0058 - val_accuracy: 0.7654 - val_loss: 1.1983\n",
            "Model Validation Accuracy: 0.8317\n",
            "\n",
            "Testing Model with 1 layers, 16 neurons, L2=0.0025\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 115ms/step - accuracy: 0.1175 - loss: 2.3428 - val_accuracy: 0.1371 - val_loss: 2.3044\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.1293 - loss: 2.2922 - val_accuracy: 0.1454 - val_loss: 2.3006\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.1489 - loss: 2.2726 - val_accuracy: 0.1367 - val_loss: 2.2947\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.1661 - loss: 2.2593 - val_accuracy: 0.1988 - val_loss: 2.2582\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.1604 - loss: 2.2581 - val_accuracy: 0.1704 - val_loss: 2.2505\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.1674 - loss: 2.2526 - val_accuracy: 0.1725 - val_loss: 2.2502\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.1857 - loss: 2.2323 - val_accuracy: 0.1713 - val_loss: 2.2310\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.1510 - loss: 2.2419 - val_accuracy: 0.2192 - val_loss: 2.2104\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.1770 - loss: 2.2301 - val_accuracy: 0.2138 - val_loss: 2.1871\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.1810 - loss: 2.2255 - val_accuracy: 0.2196 - val_loss: 2.2039\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.1856 - loss: 2.2169 - val_accuracy: 0.1813 - val_loss: 2.1931\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.1816 - loss: 2.2213 - val_accuracy: 0.2242 - val_loss: 2.1826\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.1884 - loss: 2.2215 - val_accuracy: 0.1667 - val_loss: 2.2152\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.1989 - loss: 2.1940 - val_accuracy: 0.2188 - val_loss: 2.1668\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.1970 - loss: 2.2039 - val_accuracy: 0.2142 - val_loss: 2.1680\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2056 - loss: 2.1975 - val_accuracy: 0.2400 - val_loss: 2.1598\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.2037 - loss: 2.1879 - val_accuracy: 0.1742 - val_loss: 2.2190\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.1906 - loss: 2.1908 - val_accuracy: 0.2113 - val_loss: 2.1889\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.1944 - loss: 2.1979 - val_accuracy: 0.2250 - val_loss: 2.1516\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2079 - loss: 2.1765 - val_accuracy: 0.1904 - val_loss: 2.1796\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2088 - loss: 2.1854 - val_accuracy: 0.2108 - val_loss: 2.1804\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2001 - loss: 2.1860 - val_accuracy: 0.2163 - val_loss: 2.1412\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2125 - loss: 2.1728 - val_accuracy: 0.2037 - val_loss: 2.1831\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2200 - loss: 2.1622 - val_accuracy: 0.2517 - val_loss: 2.1353\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2144 - loss: 2.1595 - val_accuracy: 0.2433 - val_loss: 2.1161\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2027 - loss: 2.1826 - val_accuracy: 0.2296 - val_loss: 2.1309\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2127 - loss: 2.1680 - val_accuracy: 0.2100 - val_loss: 2.1601\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2162 - loss: 2.1690 - val_accuracy: 0.2292 - val_loss: 2.1456\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2143 - loss: 2.1573 - val_accuracy: 0.2142 - val_loss: 2.1643\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2167 - loss: 2.1536 - val_accuracy: 0.2000 - val_loss: 2.1657\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.2168 - loss: 2.1655 - val_accuracy: 0.1742 - val_loss: 2.2263\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2217 - loss: 2.1596 - val_accuracy: 0.2429 - val_loss: 2.0958\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.2243 - loss: 2.1383 - val_accuracy: 0.2237 - val_loss: 2.1101\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2091 - loss: 2.1616 - val_accuracy: 0.2458 - val_loss: 2.0935\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2209 - loss: 2.1446 - val_accuracy: 0.2138 - val_loss: 2.1329\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2276 - loss: 2.1572 - val_accuracy: 0.1887 - val_loss: 2.2346\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2306 - loss: 2.1278 - val_accuracy: 0.2246 - val_loss: 2.1555\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2199 - loss: 2.1397 - val_accuracy: 0.1950 - val_loss: 2.1843\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2159 - loss: 2.1382 - val_accuracy: 0.2587 - val_loss: 2.1014\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2318 - loss: 2.1349 - val_accuracy: 0.2200 - val_loss: 2.1705\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2245 - loss: 2.1386 - val_accuracy: 0.2604 - val_loss: 2.1018\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2305 - loss: 2.1361 - val_accuracy: 0.2704 - val_loss: 2.1007\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2288 - loss: 2.1268 - val_accuracy: 0.1992 - val_loss: 2.1674\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2372 - loss: 2.1207 - val_accuracy: 0.2783 - val_loss: 2.0574\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2423 - loss: 2.1338 - val_accuracy: 0.2267 - val_loss: 2.1132\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2281 - loss: 2.1267 - val_accuracy: 0.2754 - val_loss: 2.0840\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.2207 - loss: 2.1397 - val_accuracy: 0.1792 - val_loss: 2.1755\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2261 - loss: 2.1206 - val_accuracy: 0.2329 - val_loss: 2.1238\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2328 - loss: 2.1327 - val_accuracy: 0.2600 - val_loss: 2.0685\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2242 - loss: 2.1136 - val_accuracy: 0.2237 - val_loss: 2.1986\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2364 - loss: 2.1301 - val_accuracy: 0.1988 - val_loss: 2.1435\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.2423 - loss: 2.1061 - val_accuracy: 0.2512 - val_loss: 2.0921\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2374 - loss: 2.1230 - val_accuracy: 0.2804 - val_loss: 2.0612\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2346 - loss: 2.1168 - val_accuracy: 0.2646 - val_loss: 2.0522\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2428 - loss: 2.1062 - val_accuracy: 0.2133 - val_loss: 2.1174\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2446 - loss: 2.1130 - val_accuracy: 0.2546 - val_loss: 2.1011\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2381 - loss: 2.1181 - val_accuracy: 0.2400 - val_loss: 2.0897\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2411 - loss: 2.1319 - val_accuracy: 0.1954 - val_loss: 2.2017\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2329 - loss: 2.1217 - val_accuracy: 0.1592 - val_loss: 2.4257\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2486 - loss: 2.1158 - val_accuracy: 0.2463 - val_loss: 2.0910\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2447 - loss: 2.1087 - val_accuracy: 0.2942 - val_loss: 2.0381\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2368 - loss: 2.1216 - val_accuracy: 0.2567 - val_loss: 2.0501\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2461 - loss: 2.1164 - val_accuracy: 0.2679 - val_loss: 2.0605\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2361 - loss: 2.1031 - val_accuracy: 0.2729 - val_loss: 2.0488\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2307 - loss: 2.0984 - val_accuracy: 0.2138 - val_loss: 2.1323\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2441 - loss: 2.1152 - val_accuracy: 0.1825 - val_loss: 2.1824\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2435 - loss: 2.1055 - val_accuracy: 0.2812 - val_loss: 2.0355\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2545 - loss: 2.0926 - val_accuracy: 0.2729 - val_loss: 2.0362\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2475 - loss: 2.0971 - val_accuracy: 0.2571 - val_loss: 2.0591\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2456 - loss: 2.0934 - val_accuracy: 0.2317 - val_loss: 2.1479\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2600 - loss: 2.0870 - val_accuracy: 0.1717 - val_loss: 2.2726\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2516 - loss: 2.0874 - val_accuracy: 0.1492 - val_loss: 2.3329\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2352 - loss: 2.1106 - val_accuracy: 0.3100 - val_loss: 2.0224\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2490 - loss: 2.0969 - val_accuracy: 0.1804 - val_loss: 2.3058\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2370 - loss: 2.1016 - val_accuracy: 0.2758 - val_loss: 2.0509\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2484 - loss: 2.0837 - val_accuracy: 0.2746 - val_loss: 2.0580\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2489 - loss: 2.0932 - val_accuracy: 0.2583 - val_loss: 2.0940\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2473 - loss: 2.0931 - val_accuracy: 0.2338 - val_loss: 2.1529\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2526 - loss: 2.0951 - val_accuracy: 0.2925 - val_loss: 2.0797\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2474 - loss: 2.1033 - val_accuracy: 0.2988 - val_loss: 2.0135\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2536 - loss: 2.0786 - val_accuracy: 0.3021 - val_loss: 2.0295\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2506 - loss: 2.0801 - val_accuracy: 0.2600 - val_loss: 2.0697\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2652 - loss: 2.0817 - val_accuracy: 0.2775 - val_loss: 2.0838\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2386 - loss: 2.1025 - val_accuracy: 0.2992 - val_loss: 2.0248\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2672 - loss: 2.0888 - val_accuracy: 0.3400 - val_loss: 1.9916\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2801 - loss: 2.0683 - val_accuracy: 0.2271 - val_loss: 2.1188\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2599 - loss: 2.0688 - val_accuracy: 0.2946 - val_loss: 2.0257\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2522 - loss: 2.0821 - val_accuracy: 0.2717 - val_loss: 2.0956\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2556 - loss: 2.0890 - val_accuracy: 0.3042 - val_loss: 2.0325\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2534 - loss: 2.0947 - val_accuracy: 0.2562 - val_loss: 2.0851\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2524 - loss: 2.0911 - val_accuracy: 0.2321 - val_loss: 2.1197\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2594 - loss: 2.0810 - val_accuracy: 0.3192 - val_loss: 2.0126\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2594 - loss: 2.0784 - val_accuracy: 0.3067 - val_loss: 2.0531\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2780 - loss: 2.0631 - val_accuracy: 0.3008 - val_loss: 2.0033\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2663 - loss: 2.0637 - val_accuracy: 0.3242 - val_loss: 1.9914\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2566 - loss: 2.0750 - val_accuracy: 0.1342 - val_loss: 2.4512\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2575 - loss: 2.0719 - val_accuracy: 0.2488 - val_loss: 2.1206\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2603 - loss: 2.0775 - val_accuracy: 0.2321 - val_loss: 2.0996\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2705 - loss: 2.0553 - val_accuracy: 0.2988 - val_loss: 2.0090\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2567 - loss: 2.0675 - val_accuracy: 0.3200 - val_loss: 1.9827\n",
            "Model Validation Accuracy: 0.3400\n",
            "\n",
            "Testing Model with 5 layers, 272 neurons, L2=0.0005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 388ms/step - accuracy: 0.2048 - loss: 2.7962 - val_accuracy: 0.1025 - val_loss: 4.4359\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3465 - loss: 2.3390 - val_accuracy: 0.1742 - val_loss: 3.1163\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4341 - loss: 2.0391 - val_accuracy: 0.2029 - val_loss: 3.0342\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.4904 - loss: 1.7975 - val_accuracy: 0.2350 - val_loss: 2.7650\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5334 - loss: 1.6866 - val_accuracy: 0.3692 - val_loss: 2.3362\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5766 - loss: 1.5697 - val_accuracy: 0.4842 - val_loss: 1.8936\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6133 - loss: 1.4284 - val_accuracy: 0.5508 - val_loss: 1.6628\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6198 - loss: 1.4056 - val_accuracy: 0.3446 - val_loss: 2.3725\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6537 - loss: 1.3161 - val_accuracy: 0.3413 - val_loss: 3.7381\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6598 - loss: 1.3103 - val_accuracy: 0.5783 - val_loss: 1.6195\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6763 - loss: 1.2763 - val_accuracy: 0.5654 - val_loss: 1.6556\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6844 - loss: 1.2344 - val_accuracy: 0.5296 - val_loss: 1.7328\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7072 - loss: 1.1948 - val_accuracy: 0.5263 - val_loss: 1.6791\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6981 - loss: 1.2075 - val_accuracy: 0.5446 - val_loss: 2.3682\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7120 - loss: 1.1893 - val_accuracy: 0.6096 - val_loss: 1.5955\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7074 - loss: 1.1986 - val_accuracy: 0.6350 - val_loss: 1.5497\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7178 - loss: 1.1681 - val_accuracy: 0.6104 - val_loss: 1.4471\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7158 - loss: 1.1600 - val_accuracy: 0.6279 - val_loss: 1.4762\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7281 - loss: 1.1194 - val_accuracy: 0.6367 - val_loss: 1.4300\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7302 - loss: 1.1225 - val_accuracy: 0.5704 - val_loss: 1.7678\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7553 - loss: 1.0863 - val_accuracy: 0.5846 - val_loss: 1.6879\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7384 - loss: 1.0920 - val_accuracy: 0.6787 - val_loss: 1.3612\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7385 - loss: 1.1253 - val_accuracy: 0.6450 - val_loss: 1.3514\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7505 - loss: 1.0604 - val_accuracy: 0.5033 - val_loss: 2.3023\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7673 - loss: 1.0681 - val_accuracy: 0.6592 - val_loss: 1.3651\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7577 - loss: 1.0875 - val_accuracy: 0.6192 - val_loss: 1.6481\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7722 - loss: 1.0572 - val_accuracy: 0.6237 - val_loss: 1.7735\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7543 - loss: 1.0827 - val_accuracy: 0.6446 - val_loss: 1.6456\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7611 - loss: 1.0783 - val_accuracy: 0.6567 - val_loss: 1.3555\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7545 - loss: 1.0842 - val_accuracy: 0.6538 - val_loss: 1.5052\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7689 - loss: 1.0674 - val_accuracy: 0.5904 - val_loss: 1.5305\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7614 - loss: 1.0681 - val_accuracy: 0.7392 - val_loss: 1.1303\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7838 - loss: 1.0105 - val_accuracy: 0.6271 - val_loss: 1.8186\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7698 - loss: 1.0625 - val_accuracy: 0.6717 - val_loss: 1.4105\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7830 - loss: 1.0152 - val_accuracy: 0.6304 - val_loss: 1.5835\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7773 - loss: 1.0177 - val_accuracy: 0.6696 - val_loss: 1.4954\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7830 - loss: 1.0456 - val_accuracy: 0.5283 - val_loss: 3.0157\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7756 - loss: 1.0577 - val_accuracy: 0.6229 - val_loss: 1.6944\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.8077 - loss: 0.9595 - val_accuracy: 0.7483 - val_loss: 1.2213\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7859 - loss: 1.0140 - val_accuracy: 0.7237 - val_loss: 1.2302\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8015 - loss: 0.9842 - val_accuracy: 0.7867 - val_loss: 1.0359\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7998 - loss: 0.9741 - val_accuracy: 0.7038 - val_loss: 1.2391\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7974 - loss: 0.9897 - val_accuracy: 0.6525 - val_loss: 1.4401\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7974 - loss: 0.9835 - val_accuracy: 0.6538 - val_loss: 1.7141\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8233 - loss: 0.9162 - val_accuracy: 0.5375 - val_loss: 1.8669\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8210 - loss: 0.9483 - val_accuracy: 0.7567 - val_loss: 1.0925\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8045 - loss: 0.9569 - val_accuracy: 0.5804 - val_loss: 2.0384\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8122 - loss: 0.9723 - val_accuracy: 0.6954 - val_loss: 1.4353\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8125 - loss: 0.9600 - val_accuracy: 0.5796 - val_loss: 2.2082\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8118 - loss: 0.9808 - val_accuracy: 0.8025 - val_loss: 0.9814\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8084 - loss: 0.9500 - val_accuracy: 0.7417 - val_loss: 1.2728\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8026 - loss: 0.9814 - val_accuracy: 0.7688 - val_loss: 1.1384\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8192 - loss: 0.9356 - val_accuracy: 0.6229 - val_loss: 2.0815\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8053 - loss: 0.9819 - val_accuracy: 0.7292 - val_loss: 1.2834\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8102 - loss: 0.9602 - val_accuracy: 0.7212 - val_loss: 1.2388\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8139 - loss: 0.9513 - val_accuracy: 0.6933 - val_loss: 1.3649\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8107 - loss: 0.9399 - val_accuracy: 0.7092 - val_loss: 1.5997\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8170 - loss: 0.9387 - val_accuracy: 0.7008 - val_loss: 1.5215\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8286 - loss: 0.9199 - val_accuracy: 0.6275 - val_loss: 2.4037\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8184 - loss: 0.9243 - val_accuracy: 0.7642 - val_loss: 1.1571\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8290 - loss: 0.9228 - val_accuracy: 0.7729 - val_loss: 1.0425\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8213 - loss: 0.9118 - val_accuracy: 0.7896 - val_loss: 1.0612\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8223 - loss: 0.9299 - val_accuracy: 0.7633 - val_loss: 1.1722\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8343 - loss: 0.9054 - val_accuracy: 0.7392 - val_loss: 1.2302\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8322 - loss: 0.9049 - val_accuracy: 0.7733 - val_loss: 1.0619\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8258 - loss: 0.8964 - val_accuracy: 0.7217 - val_loss: 1.2471\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8320 - loss: 0.9040 - val_accuracy: 0.7508 - val_loss: 1.2664\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8274 - loss: 0.8998 - val_accuracy: 0.7567 - val_loss: 1.1201\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8437 - loss: 0.8879 - val_accuracy: 0.6896 - val_loss: 1.4767\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8291 - loss: 0.9130 - val_accuracy: 0.7329 - val_loss: 1.4406\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8292 - loss: 0.9075 - val_accuracy: 0.7900 - val_loss: 1.1523\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8225 - loss: 0.9069 - val_accuracy: 0.7550 - val_loss: 1.1427\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8380 - loss: 0.8652 - val_accuracy: 0.7729 - val_loss: 1.0749\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.8346 - loss: 0.8761 - val_accuracy: 0.8075 - val_loss: 0.9601\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8418 - loss: 0.8604 - val_accuracy: 0.6854 - val_loss: 1.4919\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8263 - loss: 0.9124 - val_accuracy: 0.7429 - val_loss: 1.2816\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8412 - loss: 0.8791 - val_accuracy: 0.5138 - val_loss: 2.0276\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8356 - loss: 0.9008 - val_accuracy: 0.7483 - val_loss: 1.3185\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8131 - loss: 0.9466 - val_accuracy: 0.7437 - val_loss: 1.2683\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8442 - loss: 0.8597 - val_accuracy: 0.7679 - val_loss: 1.2906\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8415 - loss: 0.8505 - val_accuracy: 0.5533 - val_loss: 2.5383\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8485 - loss: 0.8489 - val_accuracy: 0.7446 - val_loss: 1.2007\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8306 - loss: 0.8795 - val_accuracy: 0.7904 - val_loss: 1.0337\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8386 - loss: 0.8676 - val_accuracy: 0.7200 - val_loss: 1.3110\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8486 - loss: 0.8429 - val_accuracy: 0.7171 - val_loss: 1.3486\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8424 - loss: 0.8673 - val_accuracy: 0.6621 - val_loss: 1.9507\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8339 - loss: 0.8771 - val_accuracy: 0.7425 - val_loss: 1.2473\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8521 - loss: 0.8423 - val_accuracy: 0.6400 - val_loss: 1.9248\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8433 - loss: 0.8639 - val_accuracy: 0.7792 - val_loss: 1.1365\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8464 - loss: 0.8373 - val_accuracy: 0.7412 - val_loss: 1.2229\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8416 - loss: 0.8527 - val_accuracy: 0.7404 - val_loss: 1.2276\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8453 - loss: 0.8259 - val_accuracy: 0.6817 - val_loss: 1.5848\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8426 - loss: 0.8667 - val_accuracy: 0.8263 - val_loss: 0.9983\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8318 - loss: 0.8861 - val_accuracy: 0.7287 - val_loss: 1.3667\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8562 - loss: 0.8165 - val_accuracy: 0.7821 - val_loss: 1.1478\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8418 - loss: 0.8453 - val_accuracy: 0.7521 - val_loss: 1.1976\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8505 - loss: 0.8222 - val_accuracy: 0.7504 - val_loss: 1.4464\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8454 - loss: 0.8486 - val_accuracy: 0.7775 - val_loss: 1.1960\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8446 - loss: 0.8448 - val_accuracy: 0.7862 - val_loss: 1.0201\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8678 - loss: 0.7868 - val_accuracy: 0.7675 - val_loss: 1.2466\n",
            "Model Validation Accuracy: 0.8263\n",
            "\n",
            "Testing Model with 2 layers, 80 neurons, L2=0.005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 180ms/step - accuracy: 0.1409 - loss: 2.8164 - val_accuracy: 0.1133 - val_loss: 2.5643\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2135 - loss: 2.4273 - val_accuracy: 0.1117 - val_loss: 2.4639\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2359 - loss: 2.2881 - val_accuracy: 0.1229 - val_loss: 2.4717\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.2495 - loss: 2.2028 - val_accuracy: 0.1146 - val_loss: 2.3770\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2694 - loss: 2.1355 - val_accuracy: 0.1142 - val_loss: 2.5792\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2689 - loss: 2.1260 - val_accuracy: 0.1688 - val_loss: 2.2448\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2876 - loss: 2.0782 - val_accuracy: 0.2746 - val_loss: 2.0898\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3072 - loss: 2.0693 - val_accuracy: 0.2946 - val_loss: 2.0715\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3153 - loss: 2.0180 - val_accuracy: 0.2721 - val_loss: 2.0940\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3144 - loss: 2.0281 - val_accuracy: 0.2271 - val_loss: 2.2183\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3358 - loss: 1.9979 - val_accuracy: 0.2358 - val_loss: 2.0892\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3323 - loss: 1.9854 - val_accuracy: 0.2725 - val_loss: 2.0831\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3331 - loss: 1.9867 - val_accuracy: 0.2708 - val_loss: 2.2039\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3619 - loss: 1.9524 - val_accuracy: 0.3212 - val_loss: 2.0313\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.3782 - loss: 1.9184 - val_accuracy: 0.2629 - val_loss: 2.0770\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3614 - loss: 1.9365 - val_accuracy: 0.3183 - val_loss: 1.9908\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3673 - loss: 1.9215 - val_accuracy: 0.3792 - val_loss: 1.9342\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3748 - loss: 1.8857 - val_accuracy: 0.3500 - val_loss: 1.9463\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3728 - loss: 1.8963 - val_accuracy: 0.2154 - val_loss: 2.3547\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3714 - loss: 1.9068 - val_accuracy: 0.3267 - val_loss: 1.9629\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3830 - loss: 1.8556 - val_accuracy: 0.3292 - val_loss: 1.9926\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3934 - loss: 1.8888 - val_accuracy: 0.3079 - val_loss: 2.3317\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3969 - loss: 1.8499 - val_accuracy: 0.3246 - val_loss: 2.1476\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4060 - loss: 1.8385 - val_accuracy: 0.3454 - val_loss: 1.9394\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4113 - loss: 1.8247 - val_accuracy: 0.1750 - val_loss: 2.5902\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4092 - loss: 1.8094 - val_accuracy: 0.3504 - val_loss: 1.9842\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4023 - loss: 1.8242 - val_accuracy: 0.1988 - val_loss: 2.8706\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4339 - loss: 1.7881 - val_accuracy: 0.3579 - val_loss: 2.0191\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4239 - loss: 1.7872 - val_accuracy: 0.2442 - val_loss: 2.3916\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4199 - loss: 1.8068 - val_accuracy: 0.3325 - val_loss: 2.2860\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.4247 - loss: 1.7699 - val_accuracy: 0.3250 - val_loss: 2.0592\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4327 - loss: 1.7625 - val_accuracy: 0.3313 - val_loss: 2.2081\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4337 - loss: 1.7314 - val_accuracy: 0.2125 - val_loss: 2.9052\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4367 - loss: 1.7620 - val_accuracy: 0.3746 - val_loss: 1.8561\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4427 - loss: 1.7330 - val_accuracy: 0.2637 - val_loss: 2.1979\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4457 - loss: 1.7163 - val_accuracy: 0.3496 - val_loss: 2.3067\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4665 - loss: 1.6998 - val_accuracy: 0.4679 - val_loss: 1.6885\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4560 - loss: 1.7389 - val_accuracy: 0.2500 - val_loss: 3.0285\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4527 - loss: 1.7198 - val_accuracy: 0.4225 - val_loss: 1.7566\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4435 - loss: 1.7078 - val_accuracy: 0.3638 - val_loss: 1.9772\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4565 - loss: 1.7186 - val_accuracy: 0.3217 - val_loss: 2.0378\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4622 - loss: 1.6996 - val_accuracy: 0.3388 - val_loss: 2.1518\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4561 - loss: 1.7359 - val_accuracy: 0.4433 - val_loss: 1.7007\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4399 - loss: 1.6893 - val_accuracy: 0.3938 - val_loss: 1.8947\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4582 - loss: 1.6874 - val_accuracy: 0.4013 - val_loss: 1.9315\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4639 - loss: 1.6546 - val_accuracy: 0.2512 - val_loss: 2.5733\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4785 - loss: 1.6443 - val_accuracy: 0.3821 - val_loss: 1.9104\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4840 - loss: 1.6480 - val_accuracy: 0.4200 - val_loss: 1.8583\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4844 - loss: 1.6363 - val_accuracy: 0.3629 - val_loss: 2.7681\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4717 - loss: 1.6679 - val_accuracy: 0.5013 - val_loss: 1.5900\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4637 - loss: 1.6555 - val_accuracy: 0.3321 - val_loss: 2.3026\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4953 - loss: 1.6269 - val_accuracy: 0.3663 - val_loss: 1.9299\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4856 - loss: 1.6330 - val_accuracy: 0.4471 - val_loss: 1.7406\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4739 - loss: 1.6402 - val_accuracy: 0.3288 - val_loss: 2.4731\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4618 - loss: 1.6903 - val_accuracy: 0.4325 - val_loss: 1.8457\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4936 - loss: 1.6166 - val_accuracy: 0.2879 - val_loss: 2.8025\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.4906 - loss: 1.6358 - val_accuracy: 0.3600 - val_loss: 2.1738\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4985 - loss: 1.5960 - val_accuracy: 0.4563 - val_loss: 1.6985\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4871 - loss: 1.6315 - val_accuracy: 0.2587 - val_loss: 3.3854\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4852 - loss: 1.6166 - val_accuracy: 0.3858 - val_loss: 2.1408\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4904 - loss: 1.5865 - val_accuracy: 0.2650 - val_loss: 2.8349\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4949 - loss: 1.6094 - val_accuracy: 0.3708 - val_loss: 2.1663\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4965 - loss: 1.6002 - val_accuracy: 0.4979 - val_loss: 1.6119\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5159 - loss: 1.5493 - val_accuracy: 0.3871 - val_loss: 1.9007\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4855 - loss: 1.6261 - val_accuracy: 0.3629 - val_loss: 2.1490\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5004 - loss: 1.5761 - val_accuracy: 0.4467 - val_loss: 1.7353\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4913 - loss: 1.6212 - val_accuracy: 0.4321 - val_loss: 1.7960\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5021 - loss: 1.5870 - val_accuracy: 0.2000 - val_loss: 3.3661\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5069 - loss: 1.5785 - val_accuracy: 0.4867 - val_loss: 1.6795\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5020 - loss: 1.5908 - val_accuracy: 0.4267 - val_loss: 1.9862\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5034 - loss: 1.5808 - val_accuracy: 0.3979 - val_loss: 1.9332\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5007 - loss: 1.5797 - val_accuracy: 0.3762 - val_loss: 2.1011\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5014 - loss: 1.5917 - val_accuracy: 0.4350 - val_loss: 1.9504\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4982 - loss: 1.6065 - val_accuracy: 0.3333 - val_loss: 2.3504\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4914 - loss: 1.6113 - val_accuracy: 0.4608 - val_loss: 1.9402\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5167 - loss: 1.5679 - val_accuracy: 0.4571 - val_loss: 1.7030\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5133 - loss: 1.5639 - val_accuracy: 0.4096 - val_loss: 1.8773\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5094 - loss: 1.5699 - val_accuracy: 0.2467 - val_loss: 3.1972\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5168 - loss: 1.5686 - val_accuracy: 0.3575 - val_loss: 2.6098\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5051 - loss: 1.5612 - val_accuracy: 0.4196 - val_loss: 1.8481\n",
            "Model Validation Accuracy: 0.5013\n",
            "\n",
            "Testing Model with 4 layers, 160 neurons, L2=0.01\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 395ms/step - accuracy: 0.1706 - loss: 5.9807 - val_accuracy: 0.1025 - val_loss: 3.2095\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2795 - loss: 2.6130 - val_accuracy: 0.1042 - val_loss: 2.6615\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3270 - loss: 2.1889 - val_accuracy: 0.0887 - val_loss: 2.7295\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.3691 - loss: 2.1040 - val_accuracy: 0.1004 - val_loss: 2.7055\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.3841 - loss: 2.0143 - val_accuracy: 0.2612 - val_loss: 2.4819\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4279 - loss: 1.9370 - val_accuracy: 0.2600 - val_loss: 2.4094\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4199 - loss: 1.9629 - val_accuracy: 0.2742 - val_loss: 2.3061\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4569 - loss: 1.8840 - val_accuracy: 0.1983 - val_loss: 4.0310\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4554 - loss: 1.9233 - val_accuracy: 0.2442 - val_loss: 2.6243\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4605 - loss: 1.8816 - val_accuracy: 0.2171 - val_loss: 2.9496\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.4785 - loss: 1.8466 - val_accuracy: 0.1421 - val_loss: 5.3205\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4806 - loss: 1.8551 - val_accuracy: 0.3550 - val_loss: 2.2679\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4942 - loss: 1.7950 - val_accuracy: 0.4812 - val_loss: 1.8181\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5113 - loss: 1.7853 - val_accuracy: 0.3679 - val_loss: 2.1366\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5465 - loss: 1.7272 - val_accuracy: 0.2417 - val_loss: 3.1925\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5303 - loss: 1.7380 - val_accuracy: 0.2775 - val_loss: 4.4553\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5283 - loss: 1.7522 - val_accuracy: 0.4038 - val_loss: 2.0986\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5439 - loss: 1.7212 - val_accuracy: 0.4100 - val_loss: 2.2062\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5616 - loss: 1.6618 - val_accuracy: 0.1996 - val_loss: 4.0396\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5711 - loss: 1.6463 - val_accuracy: 0.3258 - val_loss: 2.9531\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5751 - loss: 1.6609 - val_accuracy: 0.4258 - val_loss: 2.7619\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5717 - loss: 1.6507 - val_accuracy: 0.1942 - val_loss: 3.2602\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5877 - loss: 1.5904 - val_accuracy: 0.4446 - val_loss: 1.9814\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5918 - loss: 1.5753 - val_accuracy: 0.5275 - val_loss: 2.0156\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5954 - loss: 1.5889 - val_accuracy: 0.2983 - val_loss: 4.4999\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5843 - loss: 1.6107 - val_accuracy: 0.4137 - val_loss: 2.2764\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6054 - loss: 1.5733 - val_accuracy: 0.2629 - val_loss: 3.9585\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6054 - loss: 1.5743 - val_accuracy: 0.5562 - val_loss: 1.7438\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6079 - loss: 1.5610 - val_accuracy: 0.4454 - val_loss: 2.1867\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6359 - loss: 1.5004 - val_accuracy: 0.2904 - val_loss: 3.6068\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6166 - loss: 1.5580 - val_accuracy: 0.3679 - val_loss: 3.9115\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6063 - loss: 1.5389 - val_accuracy: 0.3829 - val_loss: 2.9769\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6153 - loss: 1.5039 - val_accuracy: 0.3796 - val_loss: 3.5597\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6227 - loss: 1.5069 - val_accuracy: 0.5958 - val_loss: 1.5954\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6265 - loss: 1.4907 - val_accuracy: 0.4688 - val_loss: 2.4390\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6078 - loss: 1.5340 - val_accuracy: 0.5604 - val_loss: 1.6587\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6464 - loss: 1.4320 - val_accuracy: 0.5854 - val_loss: 1.5869\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6406 - loss: 1.4221 - val_accuracy: 0.5163 - val_loss: 1.8454\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6340 - loss: 1.4631 - val_accuracy: 0.4500 - val_loss: 2.3951\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6431 - loss: 1.4307 - val_accuracy: 0.3071 - val_loss: 4.4387\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6454 - loss: 1.4170 - val_accuracy: 0.4658 - val_loss: 2.4272\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6525 - loss: 1.4106 - val_accuracy: 0.5738 - val_loss: 1.6665\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6600 - loss: 1.4003 - val_accuracy: 0.3829 - val_loss: 2.7608\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6387 - loss: 1.4333 - val_accuracy: 0.4642 - val_loss: 2.0386\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6566 - loss: 1.4022 - val_accuracy: 0.5871 - val_loss: 1.8281\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6468 - loss: 1.3821 - val_accuracy: 0.5029 - val_loss: 2.0845\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6501 - loss: 1.3662 - val_accuracy: 0.5529 - val_loss: 1.7776\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6507 - loss: 1.3799 - val_accuracy: 0.6087 - val_loss: 1.5818\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6540 - loss: 1.3858 - val_accuracy: 0.3008 - val_loss: 5.1441\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6437 - loss: 1.3985 - val_accuracy: 0.4050 - val_loss: 3.2271\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6686 - loss: 1.3280 - val_accuracy: 0.5133 - val_loss: 1.7981\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6686 - loss: 1.3485 - val_accuracy: 0.3508 - val_loss: 2.3068\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6685 - loss: 1.3318 - val_accuracy: 0.4800 - val_loss: 2.1214\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6566 - loss: 1.3468 - val_accuracy: 0.2992 - val_loss: 4.7633\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6644 - loss: 1.3557 - val_accuracy: 0.4121 - val_loss: 2.5861\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6635 - loss: 1.3392 - val_accuracy: 0.4758 - val_loss: 1.8763\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6607 - loss: 1.3704 - val_accuracy: 0.4729 - val_loss: 2.3600\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6697 - loss: 1.3243 - val_accuracy: 0.4379 - val_loss: 2.8622\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6746 - loss: 1.3162 - val_accuracy: 0.4663 - val_loss: 2.5738\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6817 - loss: 1.3070 - val_accuracy: 0.6133 - val_loss: 1.5423\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6721 - loss: 1.3421 - val_accuracy: 0.5821 - val_loss: 1.7174\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6740 - loss: 1.2864 - val_accuracy: 0.3258 - val_loss: 2.8924\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6704 - loss: 1.2973 - val_accuracy: 0.5638 - val_loss: 1.6903\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6950 - loss: 1.2584 - val_accuracy: 0.5496 - val_loss: 1.9679\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6719 - loss: 1.3253 - val_accuracy: 0.3908 - val_loss: 3.3573\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6673 - loss: 1.3471 - val_accuracy: 0.6529 - val_loss: 1.4269\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6879 - loss: 1.2678 - val_accuracy: 0.1817 - val_loss: 9.9705\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6663 - loss: 1.3501 - val_accuracy: 0.5362 - val_loss: 2.1003\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6886 - loss: 1.2689 - val_accuracy: 0.4025 - val_loss: 3.6899\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6663 - loss: 1.3075 - val_accuracy: 0.5608 - val_loss: 1.5641\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6981 - loss: 1.2485 - val_accuracy: 0.5704 - val_loss: 1.5487\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6839 - loss: 1.2710 - val_accuracy: 0.6233 - val_loss: 1.5205\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6930 - loss: 1.2448 - val_accuracy: 0.5975 - val_loss: 1.6100\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6796 - loss: 1.2407 - val_accuracy: 0.5867 - val_loss: 1.6772\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6716 - loss: 1.2795 - val_accuracy: 0.4850 - val_loss: 2.9805\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6842 - loss: 1.2747 - val_accuracy: 0.6625 - val_loss: 1.3203\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6980 - loss: 1.2403 - val_accuracy: 0.5183 - val_loss: 2.0805\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7018 - loss: 1.2307 - val_accuracy: 0.4454 - val_loss: 3.4715\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6887 - loss: 1.2695 - val_accuracy: 0.4304 - val_loss: 2.5436\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6857 - loss: 1.2675 - val_accuracy: 0.5600 - val_loss: 1.8980\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6873 - loss: 1.2622 - val_accuracy: 0.4654 - val_loss: 2.5837\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7013 - loss: 1.2227 - val_accuracy: 0.6263 - val_loss: 1.5413\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6964 - loss: 1.2176 - val_accuracy: 0.3754 - val_loss: 4.9777\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6934 - loss: 1.2385 - val_accuracy: 0.4608 - val_loss: 3.1548\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6853 - loss: 1.2378 - val_accuracy: 0.3971 - val_loss: 2.5980\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6994 - loss: 1.2398 - val_accuracy: 0.4142 - val_loss: 2.2024\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7024 - loss: 1.2162 - val_accuracy: 0.3596 - val_loss: 2.8410\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7033 - loss: 1.2282 - val_accuracy: 0.5367 - val_loss: 1.7897\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7062 - loss: 1.2125 - val_accuracy: 0.6146 - val_loss: 1.5595\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6930 - loss: 1.2249 - val_accuracy: 0.3183 - val_loss: 7.2190\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7106 - loss: 1.2050 - val_accuracy: 0.6633 - val_loss: 1.3563\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6946 - loss: 1.2101 - val_accuracy: 0.5500 - val_loss: 2.1607\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7078 - loss: 1.1944 - val_accuracy: 0.6033 - val_loss: 1.5301\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6979 - loss: 1.2164 - val_accuracy: 0.3862 - val_loss: 2.1762\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7097 - loss: 1.1975 - val_accuracy: 0.4883 - val_loss: 3.0461\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7076 - loss: 1.1957 - val_accuracy: 0.5971 - val_loss: 1.9267\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7094 - loss: 1.1985 - val_accuracy: 0.5867 - val_loss: 1.6731\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7102 - loss: 1.2094 - val_accuracy: 0.5771 - val_loss: 1.9223\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7183 - loss: 1.1723 - val_accuracy: 0.4258 - val_loss: 3.4638\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7046 - loss: 1.1962 - val_accuracy: 0.5421 - val_loss: 1.8212\n",
            "Model Validation Accuracy: 0.6633\n",
            "\n",
            "Testing Model with 5 layers, 496 neurons, L2=0.005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 456ms/step - accuracy: 0.1861 - loss: 8.8797 - val_accuracy: 0.1029 - val_loss: 3.2578\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.2852 - loss: 2.5223 - val_accuracy: 0.0867 - val_loss: 4.0296\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3435 - loss: 2.3120 - val_accuracy: 0.0867 - val_loss: 3.6829\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.3702 - loss: 2.2858 - val_accuracy: 0.0867 - val_loss: 3.6895\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.3832 - loss: 2.2817 - val_accuracy: 0.0904 - val_loss: 3.4554\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3959 - loss: 2.2481 - val_accuracy: 0.1333 - val_loss: 2.9697\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3964 - loss: 2.2141 - val_accuracy: 0.1846 - val_loss: 2.7412\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.4059 - loss: 2.2523 - val_accuracy: 0.2442 - val_loss: 2.9268\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4100 - loss: 2.2231 - val_accuracy: 0.1767 - val_loss: 2.9071\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.3969 - loss: 2.2543 - val_accuracy: 0.2571 - val_loss: 2.6807\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4496 - loss: 2.1376 - val_accuracy: 0.1675 - val_loss: 4.4770\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4193 - loss: 2.1848 - val_accuracy: 0.2375 - val_loss: 3.4493\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.4279 - loss: 2.1934 - val_accuracy: 0.3071 - val_loss: 2.5167\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4338 - loss: 2.1511 - val_accuracy: 0.2883 - val_loss: 3.1662\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4571 - loss: 2.1122 - val_accuracy: 0.1629 - val_loss: 4.3321\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4530 - loss: 2.0954 - val_accuracy: 0.3033 - val_loss: 2.4478\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4676 - loss: 2.0520 - val_accuracy: 0.0988 - val_loss: 6.3289\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.4443 - loss: 2.1802 - val_accuracy: 0.2208 - val_loss: 3.3102\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.4564 - loss: 2.0800 - val_accuracy: 0.4392 - val_loss: 2.1666\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4891 - loss: 2.0171 - val_accuracy: 0.2329 - val_loss: 3.3912\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4787 - loss: 2.0799 - val_accuracy: 0.1954 - val_loss: 3.8856\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.4931 - loss: 2.0171 - val_accuracy: 0.1071 - val_loss: 12.2498\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.4697 - loss: 2.0791 - val_accuracy: 0.3150 - val_loss: 2.5811\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5039 - loss: 1.9845 - val_accuracy: 0.1412 - val_loss: 5.3447\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5279 - loss: 1.9679 - val_accuracy: 0.2087 - val_loss: 4.2886\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5171 - loss: 1.9923 - val_accuracy: 0.3725 - val_loss: 2.3765\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5186 - loss: 1.9542 - val_accuracy: 0.3533 - val_loss: 2.9539\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5325 - loss: 1.9659 - val_accuracy: 0.3279 - val_loss: 2.9399\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5462 - loss: 1.8873 - val_accuracy: 0.3771 - val_loss: 2.5325\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.5433 - loss: 1.9050 - val_accuracy: 0.3175 - val_loss: 2.5303\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5402 - loss: 1.9106 - val_accuracy: 0.3729 - val_loss: 2.4998\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5638 - loss: 1.8745 - val_accuracy: 0.4100 - val_loss: 2.4008\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5780 - loss: 1.8207 - val_accuracy: 0.3683 - val_loss: 3.0622\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.5741 - loss: 1.8002 - val_accuracy: 0.3379 - val_loss: 2.8842\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.5739 - loss: 1.8108 - val_accuracy: 0.4437 - val_loss: 2.1511\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6034 - loss: 1.7358 - val_accuracy: 0.3079 - val_loss: 3.2634\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5944 - loss: 1.7544 - val_accuracy: 0.4288 - val_loss: 2.3727\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.6056 - loss: 1.6922 - val_accuracy: 0.4658 - val_loss: 2.3600\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6098 - loss: 1.6805 - val_accuracy: 0.4783 - val_loss: 2.1521\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.6212 - loss: 1.6757 - val_accuracy: 0.5904 - val_loss: 1.8287\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6332 - loss: 1.6690 - val_accuracy: 0.3758 - val_loss: 2.7869\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6112 - loss: 1.6977 - val_accuracy: 0.4642 - val_loss: 2.0570\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6479 - loss: 1.5523 - val_accuracy: 0.5046 - val_loss: 2.2646\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6134 - loss: 1.6688 - val_accuracy: 0.4050 - val_loss: 2.7604\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6446 - loss: 1.5751 - val_accuracy: 0.2646 - val_loss: 6.2744\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6397 - loss: 1.6117 - val_accuracy: 0.2862 - val_loss: 3.6878\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6392 - loss: 1.5701 - val_accuracy: 0.3554 - val_loss: 3.0351\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6591 - loss: 1.5557 - val_accuracy: 0.3663 - val_loss: 2.7749\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6618 - loss: 1.5333 - val_accuracy: 0.2837 - val_loss: 3.3766\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6656 - loss: 1.5131 - val_accuracy: 0.2412 - val_loss: 6.8653\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6848 - loss: 1.5028 - val_accuracy: 0.3633 - val_loss: 3.1241\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6649 - loss: 1.4945 - val_accuracy: 0.3329 - val_loss: 3.8001\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6626 - loss: 1.5502 - val_accuracy: 0.5529 - val_loss: 2.0293\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.6709 - loss: 1.5217 - val_accuracy: 0.4525 - val_loss: 2.8098\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6792 - loss: 1.4723 - val_accuracy: 0.2058 - val_loss: 7.6754\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6982 - loss: 1.4404 - val_accuracy: 0.4154 - val_loss: 2.6834\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6663 - loss: 1.5053 - val_accuracy: 0.4471 - val_loss: 2.9813\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6813 - loss: 1.4668 - val_accuracy: 0.3338 - val_loss: 3.0888\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6703 - loss: 1.4760 - val_accuracy: 0.4946 - val_loss: 3.0309\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6854 - loss: 1.4213 - val_accuracy: 0.2683 - val_loss: 3.8638\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6726 - loss: 1.4629 - val_accuracy: 0.2671 - val_loss: 6.3364\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6777 - loss: 1.4403 - val_accuracy: 0.4425 - val_loss: 3.7021\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6925 - loss: 1.4598 - val_accuracy: 0.5192 - val_loss: 2.1104\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6997 - loss: 1.3899 - val_accuracy: 0.5242 - val_loss: 2.2042\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6932 - loss: 1.4460 - val_accuracy: 0.4221 - val_loss: 4.4614\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7067 - loss: 1.3936 - val_accuracy: 0.1796 - val_loss: 5.9640\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6911 - loss: 1.4320 - val_accuracy: 0.4171 - val_loss: 3.1038\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6977 - loss: 1.4306 - val_accuracy: 0.4154 - val_loss: 3.2633\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7012 - loss: 1.3934 - val_accuracy: 0.4563 - val_loss: 2.2154\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7072 - loss: 1.3912 - val_accuracy: 0.3642 - val_loss: 3.7677\n",
            "Model Validation Accuracy: 0.5904\n",
            "\n",
            "Testing Model with 3 layers, 384 neurons, L2=0.005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 478ms/step - accuracy: 0.1785 - loss: 4.9453 - val_accuracy: 0.1025 - val_loss: 2.8653\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.2777 - loss: 2.3911 - val_accuracy: 0.0867 - val_loss: 2.9367\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.3004 - loss: 2.2319 - val_accuracy: 0.0908 - val_loss: 3.0133\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3076 - loss: 2.2090 - val_accuracy: 0.1800 - val_loss: 2.7651\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3220 - loss: 2.1792 - val_accuracy: 0.1846 - val_loss: 2.6827\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.3375 - loss: 2.1391 - val_accuracy: 0.1817 - val_loss: 2.5582\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3522 - loss: 2.1144 - val_accuracy: 0.2362 - val_loss: 2.5064\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.3768 - loss: 2.0896 - val_accuracy: 0.2246 - val_loss: 2.6717\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.3744 - loss: 2.0718 - val_accuracy: 0.3433 - val_loss: 2.1392\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.3719 - loss: 2.0692 - val_accuracy: 0.1717 - val_loss: 3.3491\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.3968 - loss: 2.0150 - val_accuracy: 0.2733 - val_loss: 2.6509\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4146 - loss: 2.0027 - val_accuracy: 0.3533 - val_loss: 2.1652\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4008 - loss: 2.0060 - val_accuracy: 0.1813 - val_loss: 4.1303\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4264 - loss: 1.9492 - val_accuracy: 0.3288 - val_loss: 2.3715\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4394 - loss: 1.9318 - val_accuracy: 0.4279 - val_loss: 1.9355\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4456 - loss: 1.9081 - val_accuracy: 0.2150 - val_loss: 3.2549\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4633 - loss: 1.8539 - val_accuracy: 0.4192 - val_loss: 2.0266\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4635 - loss: 1.8895 - val_accuracy: 0.3750 - val_loss: 2.1795\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4821 - loss: 1.8596 - val_accuracy: 0.3800 - val_loss: 2.1654\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4674 - loss: 1.8666 - val_accuracy: 0.3200 - val_loss: 2.4944\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4768 - loss: 1.8091 - val_accuracy: 0.2200 - val_loss: 3.2406\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4907 - loss: 1.7989 - val_accuracy: 0.2308 - val_loss: 3.3373\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5086 - loss: 1.7567 - val_accuracy: 0.3679 - val_loss: 2.3759\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5147 - loss: 1.7368 - val_accuracy: 0.4812 - val_loss: 1.8202\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5136 - loss: 1.7052 - val_accuracy: 0.3767 - val_loss: 2.2286\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5075 - loss: 1.7450 - val_accuracy: 0.3846 - val_loss: 2.2012\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5269 - loss: 1.6853 - val_accuracy: 0.2746 - val_loss: 3.0262\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5422 - loss: 1.6648 - val_accuracy: 0.3633 - val_loss: 2.6508\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5226 - loss: 1.6816 - val_accuracy: 0.3446 - val_loss: 2.4674\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5649 - loss: 1.6088 - val_accuracy: 0.2237 - val_loss: 2.8379\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5396 - loss: 1.6882 - val_accuracy: 0.3696 - val_loss: 2.8502\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5450 - loss: 1.6647 - val_accuracy: 0.2392 - val_loss: 3.2230\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5574 - loss: 1.6269 - val_accuracy: 0.2446 - val_loss: 3.9128\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5804 - loss: 1.5323 - val_accuracy: 0.2788 - val_loss: 2.9373\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5701 - loss: 1.6066 - val_accuracy: 0.2579 - val_loss: 3.5989\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5809 - loss: 1.5334 - val_accuracy: 0.4004 - val_loss: 2.4222\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5961 - loss: 1.5358 - val_accuracy: 0.5350 - val_loss: 1.6972\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5871 - loss: 1.5170 - val_accuracy: 0.2525 - val_loss: 3.9665\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5818 - loss: 1.5247 - val_accuracy: 0.2800 - val_loss: 3.2649\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5855 - loss: 1.5291 - val_accuracy: 0.2304 - val_loss: 4.1743\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5975 - loss: 1.4804 - val_accuracy: 0.2471 - val_loss: 3.4346\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5893 - loss: 1.5198 - val_accuracy: 0.1450 - val_loss: 9.2665\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5938 - loss: 1.4810 - val_accuracy: 0.3137 - val_loss: 4.0005\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6066 - loss: 1.4657 - val_accuracy: 0.2121 - val_loss: 6.9613\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6100 - loss: 1.4823 - val_accuracy: 0.4892 - val_loss: 1.8677\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6269 - loss: 1.4320 - val_accuracy: 0.3663 - val_loss: 2.2508\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6141 - loss: 1.4525 - val_accuracy: 0.5233 - val_loss: 1.8990\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6063 - loss: 1.4406 - val_accuracy: 0.2788 - val_loss: 3.1464\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6181 - loss: 1.4314 - val_accuracy: 0.3833 - val_loss: 3.0085\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6344 - loss: 1.3943 - val_accuracy: 0.3479 - val_loss: 2.6836\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6311 - loss: 1.4098 - val_accuracy: 0.3667 - val_loss: 3.2456\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6341 - loss: 1.3868 - val_accuracy: 0.3321 - val_loss: 3.8942\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6271 - loss: 1.4268 - val_accuracy: 0.4421 - val_loss: 2.2812\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6371 - loss: 1.3661 - val_accuracy: 0.4221 - val_loss: 2.1823\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6212 - loss: 1.4391 - val_accuracy: 0.4375 - val_loss: 2.3170\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6524 - loss: 1.3314 - val_accuracy: 0.2850 - val_loss: 3.9580\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6292 - loss: 1.3950 - val_accuracy: 0.2562 - val_loss: 6.3251\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6293 - loss: 1.3885 - val_accuracy: 0.3946 - val_loss: 2.6388\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6514 - loss: 1.3365 - val_accuracy: 0.3621 - val_loss: 2.7171\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6353 - loss: 1.3835 - val_accuracy: 0.4588 - val_loss: 2.2675\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6377 - loss: 1.3511 - val_accuracy: 0.2533 - val_loss: 4.9264\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6545 - loss: 1.3124 - val_accuracy: 0.4475 - val_loss: 2.1180\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6553 - loss: 1.3000 - val_accuracy: 0.2912 - val_loss: 2.9098\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6525 - loss: 1.3259 - val_accuracy: 0.2275 - val_loss: 4.7472\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6399 - loss: 1.3767 - val_accuracy: 0.4313 - val_loss: 2.2028\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6678 - loss: 1.2736 - val_accuracy: 0.3729 - val_loss: 2.8817\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6549 - loss: 1.3131 - val_accuracy: 0.3554 - val_loss: 4.1457\n",
            "Model Validation Accuracy: 0.5350\n",
            "\n",
            "Testing Model with 3 layers, 32 neurons, L2=0.005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 227ms/step - accuracy: 0.1308 - loss: 2.6240 - val_accuracy: 0.1037 - val_loss: 2.5249\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.1910 - loss: 2.4134 - val_accuracy: 0.1108 - val_loss: 2.4616\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.2252 - loss: 2.3011 - val_accuracy: 0.1679 - val_loss: 2.3946\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2457 - loss: 2.2395 - val_accuracy: 0.2133 - val_loss: 2.3197\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.2850 - loss: 2.1449 - val_accuracy: 0.2396 - val_loss: 2.2737\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3068 - loss: 2.0896 - val_accuracy: 0.2283 - val_loss: 2.1709\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3089 - loss: 2.0551 - val_accuracy: 0.1783 - val_loss: 2.2937\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3185 - loss: 2.0078 - val_accuracy: 0.3187 - val_loss: 1.9837\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3328 - loss: 1.9681 - val_accuracy: 0.2725 - val_loss: 2.1629\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3713 - loss: 1.9068 - val_accuracy: 0.3400 - val_loss: 1.9660\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3937 - loss: 1.8591 - val_accuracy: 0.4200 - val_loss: 1.7891\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3849 - loss: 1.8649 - val_accuracy: 0.3512 - val_loss: 1.8697\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4177 - loss: 1.8020 - val_accuracy: 0.2800 - val_loss: 2.0651\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4080 - loss: 1.8046 - val_accuracy: 0.3575 - val_loss: 1.9046\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3931 - loss: 1.7989 - val_accuracy: 0.3767 - val_loss: 1.8857\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4263 - loss: 1.7704 - val_accuracy: 0.2504 - val_loss: 2.4254\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4216 - loss: 1.7782 - val_accuracy: 0.4038 - val_loss: 1.8409\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4301 - loss: 1.7246 - val_accuracy: 0.4046 - val_loss: 1.7346\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4299 - loss: 1.7161 - val_accuracy: 0.4354 - val_loss: 1.7152\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4552 - loss: 1.6928 - val_accuracy: 0.4396 - val_loss: 1.6649\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4578 - loss: 1.6829 - val_accuracy: 0.4679 - val_loss: 1.5904\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4604 - loss: 1.6835 - val_accuracy: 0.5462 - val_loss: 1.5064\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4517 - loss: 1.6977 - val_accuracy: 0.3304 - val_loss: 2.0736\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4641 - loss: 1.6558 - val_accuracy: 0.3946 - val_loss: 2.1963\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4629 - loss: 1.6500 - val_accuracy: 0.4808 - val_loss: 1.6236\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4536 - loss: 1.6426 - val_accuracy: 0.3217 - val_loss: 2.3670\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4810 - loss: 1.6234 - val_accuracy: 0.3963 - val_loss: 1.9693\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4969 - loss: 1.5699 - val_accuracy: 0.4329 - val_loss: 1.8692\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4797 - loss: 1.6181 - val_accuracy: 0.5521 - val_loss: 1.4332\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4764 - loss: 1.5993 - val_accuracy: 0.3504 - val_loss: 1.8565\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4695 - loss: 1.6099 - val_accuracy: 0.5196 - val_loss: 1.5074\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4862 - loss: 1.5952 - val_accuracy: 0.4767 - val_loss: 1.6228\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4802 - loss: 1.5833 - val_accuracy: 0.4758 - val_loss: 1.5967\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4869 - loss: 1.5781 - val_accuracy: 0.5058 - val_loss: 1.5545\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4882 - loss: 1.5897 - val_accuracy: 0.5546 - val_loss: 1.4522\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4923 - loss: 1.5689 - val_accuracy: 0.4967 - val_loss: 1.6450\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5056 - loss: 1.5514 - val_accuracy: 0.5383 - val_loss: 1.4716\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4815 - loss: 1.5644 - val_accuracy: 0.3900 - val_loss: 2.1642\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4954 - loss: 1.5527 - val_accuracy: 0.5158 - val_loss: 1.5450\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4871 - loss: 1.5819 - val_accuracy: 0.5154 - val_loss: 1.5743\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5089 - loss: 1.5332 - val_accuracy: 0.5417 - val_loss: 1.4019\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5175 - loss: 1.4992 - val_accuracy: 0.4675 - val_loss: 1.6543\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5099 - loss: 1.5123 - val_accuracy: 0.5058 - val_loss: 1.5790\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5086 - loss: 1.5411 - val_accuracy: 0.4146 - val_loss: 2.0082\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5233 - loss: 1.4926 - val_accuracy: 0.4563 - val_loss: 1.7884\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5270 - loss: 1.4912 - val_accuracy: 0.4271 - val_loss: 1.7688\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5237 - loss: 1.4989 - val_accuracy: 0.5092 - val_loss: 1.4845\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5290 - loss: 1.4927 - val_accuracy: 0.4908 - val_loss: 1.5543\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5121 - loss: 1.5075 - val_accuracy: 0.4575 - val_loss: 1.6995\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5127 - loss: 1.5571 - val_accuracy: 0.6037 - val_loss: 1.3523\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5276 - loss: 1.4792 - val_accuracy: 0.5958 - val_loss: 1.3284\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5245 - loss: 1.4757 - val_accuracy: 0.4608 - val_loss: 2.0366\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5262 - loss: 1.5085 - val_accuracy: 0.5642 - val_loss: 1.3716\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5330 - loss: 1.4820 - val_accuracy: 0.5300 - val_loss: 1.4776\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5111 - loss: 1.4856 - val_accuracy: 0.5138 - val_loss: 1.6976\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5169 - loss: 1.5011 - val_accuracy: 0.2933 - val_loss: 2.7169\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5294 - loss: 1.4932 - val_accuracy: 0.5987 - val_loss: 1.3310\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5323 - loss: 1.4667 - val_accuracy: 0.5250 - val_loss: 1.4680\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5234 - loss: 1.5003 - val_accuracy: 0.3421 - val_loss: 2.6777\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5450 - loss: 1.4446 - val_accuracy: 0.4983 - val_loss: 1.5812\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5442 - loss: 1.4682 - val_accuracy: 0.5654 - val_loss: 1.4117\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5302 - loss: 1.4938 - val_accuracy: 0.4767 - val_loss: 1.6398\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5276 - loss: 1.4847 - val_accuracy: 0.5567 - val_loss: 1.4382\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5419 - loss: 1.4827 - val_accuracy: 0.5258 - val_loss: 1.5543\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5178 - loss: 1.4974 - val_accuracy: 0.6329 - val_loss: 1.2382\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5334 - loss: 1.4574 - val_accuracy: 0.5113 - val_loss: 1.6529\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5392 - loss: 1.4617 - val_accuracy: 0.5471 - val_loss: 1.4327\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5603 - loss: 1.4328 - val_accuracy: 0.3887 - val_loss: 1.9401\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5427 - loss: 1.4504 - val_accuracy: 0.5758 - val_loss: 1.3661\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5354 - loss: 1.4589 - val_accuracy: 0.5558 - val_loss: 1.5983\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5389 - loss: 1.4610 - val_accuracy: 0.4883 - val_loss: 1.5500\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5477 - loss: 1.4627 - val_accuracy: 0.4921 - val_loss: 1.6454\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5540 - loss: 1.4231 - val_accuracy: 0.4396 - val_loss: 1.7355\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5588 - loss: 1.4177 - val_accuracy: 0.5917 - val_loss: 1.3612\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5304 - loss: 1.4815 - val_accuracy: 0.5608 - val_loss: 1.3853\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5607 - loss: 1.4232 - val_accuracy: 0.5975 - val_loss: 1.3807\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5414 - loss: 1.4387 - val_accuracy: 0.5638 - val_loss: 1.4522\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5491 - loss: 1.4296 - val_accuracy: 0.4567 - val_loss: 2.1263\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5322 - loss: 1.4681 - val_accuracy: 0.5946 - val_loss: 1.3432\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5490 - loss: 1.4489 - val_accuracy: 0.5725 - val_loss: 1.5157\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5584 - loss: 1.4279 - val_accuracy: 0.4512 - val_loss: 1.6774\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5603 - loss: 1.4079 - val_accuracy: 0.5571 - val_loss: 1.4610\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5466 - loss: 1.4270 - val_accuracy: 0.5238 - val_loss: 1.6491\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5448 - loss: 1.4329 - val_accuracy: 0.5829 - val_loss: 1.3448\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5570 - loss: 1.4116 - val_accuracy: 0.5221 - val_loss: 1.4957\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5453 - loss: 1.4465 - val_accuracy: 0.3867 - val_loss: 1.8840\n",
            "Epoch 87/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5539 - loss: 1.4099"
          ]
        }
      ],
      "source": [
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "# TPU initialization\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Leave tpu= argument empty for Colab TPU\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# Define the hyperparameter space\n",
        "num_layers_options = list(range(1, 7))  # Reduce maximum number of layers to avoid excessive downsampling\n",
        "neurons_options = list(range(16, 513, 16))\n",
        "regs = [0.0005, 0.001, 0.0015, 0.0025, 0.005, 0.01]\n",
        "\n",
        "# Set the number of random searches\n",
        "n_random_searches = 10\n",
        "\n",
        "best_model = None\n",
        "best_val_acc = 0\n",
        "best_hyperparams = None\n",
        "\n",
        "results_G = []\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Initial Random Search\n",
        "for _ in range(n_random_searches):\n",
        "    # Randomly select a combination of hyperparameters\n",
        "    num_layers = random.choice(num_layers_options)\n",
        "    neurons = random.choice(neurons_options)\n",
        "    reg = random.choice(regs)\n",
        "\n",
        "    print(f\"\\nTesting Model with {num_layers} layers, {neurons} neurons, L2={reg}\")\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(28, 28, 3)))\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            model.add(Conv2D(filters=neurons, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(LeakyReLU())\n",
        "            # Apply MaxPooling2D every other layer to avoid excessive downsampling\n",
        "            if i % 2 == 1:\n",
        "                model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(neurons, activation=\"relu\"))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "        optimizer = AdamW(learning_rate=0.001)\n",
        "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, verbose=0)\n",
        "        checkpoint = ModelCheckpoint('model_checkpoint.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "        csv_logger = CSVLogger('training_log.csv', append=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            datagen.flow(X_train, y_train, batch_size=64),\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            epochs=100,  # Reduced number of epochs\n",
        "            callbacks=[early_stopping, checkpoint, csv_logger],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    best_epoch = np.argmax(history.history['val_accuracy'])\n",
        "    train_acc = history.history['accuracy'][best_epoch]\n",
        "    val_acc = history.history['val_accuracy'][best_epoch]\n",
        "    print(f\"Model Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    results_G.append({\n",
        "        \"num_layers\": num_layers,\n",
        "        \"neurons\": neurons,\n",
        "        \"l2_reg\": reg,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_acc\": val_acc\n",
        "    })\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = model\n",
        "        best_hyperparams = (num_layers, neurons, reg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6zfXryesFXH",
        "outputId": "abe0737d-eee6-4259-e629-3554450338fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Model with 5 layers, 304 neurons, L2=0.0005\n",
            "Epoch 1/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.1976 - loss: 2.9020"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 427ms/step - accuracy: 0.1988 - loss: 2.8985 - val_accuracy: 0.1050 - val_loss: 3.4051\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3736 - loss: 2.3408"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3737 - loss: 2.3403 - val_accuracy: 0.2267 - val_loss: 2.9203\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4281 - loss: 2.0692 - val_accuracy: 0.1258 - val_loss: 2.8017\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4842 - loss: 1.8496 - val_accuracy: 0.1408 - val_loss: 2.6816\n",
            "Epoch 5/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5448 - loss: 1.6609"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5447 - loss: 1.6609 - val_accuracy: 0.2771 - val_loss: 2.3814\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5631 - loss: 1.5866"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5630 - loss: 1.5868 - val_accuracy: 0.3000 - val_loss: 2.3567\n",
            "Epoch 7/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6048 - loss: 1.4778"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6048 - loss: 1.4780 - val_accuracy: 0.4746 - val_loss: 1.7866\n",
            "Epoch 8/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6267 - loss: 1.3916"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6266 - loss: 1.3918 - val_accuracy: 0.5683 - val_loss: 1.6198\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6109 - loss: 1.4754 - val_accuracy: 0.5546 - val_loss: 1.7227\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6508 - loss: 1.3291 - val_accuracy: 0.4892 - val_loss: 2.1219\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6642 - loss: 1.3370 - val_accuracy: 0.4217 - val_loss: 2.6262\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6784 - loss: 1.2855 - val_accuracy: 0.2992 - val_loss: 4.4817\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6892 - loss: 1.2558 - val_accuracy: 0.5375 - val_loss: 1.8998\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6807 - loss: 1.2859 - val_accuracy: 0.3908 - val_loss: 3.3479\n",
            "Epoch 15/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6913 - loss: 1.2533"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6915 - loss: 1.2530 - val_accuracy: 0.6083 - val_loss: 1.4930\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7102 - loss: 1.2333 - val_accuracy: 0.5233 - val_loss: 1.9017\n",
            "Epoch 17/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7020 - loss: 1.2282"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7017 - loss: 1.2292 - val_accuracy: 0.6300 - val_loss: 1.6141\n",
            "Epoch 18/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7143 - loss: 1.2230"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7142 - loss: 1.2230 - val_accuracy: 0.6421 - val_loss: 1.4830\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7204 - loss: 1.1943 - val_accuracy: 0.5763 - val_loss: 1.6245\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7375 - loss: 1.1466 - val_accuracy: 0.5154 - val_loss: 2.0581\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7366 - loss: 1.1571 - val_accuracy: 0.6025 - val_loss: 1.6121\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7324 - loss: 1.1661 - val_accuracy: 0.5321 - val_loss: 2.1247\n",
            "Epoch 23/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7234 - loss: 1.1726"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7235 - loss: 1.1730 - val_accuracy: 0.6696 - val_loss: 1.3193\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7458 - loss: 1.1246 - val_accuracy: 0.5033 - val_loss: 2.2792\n",
            "Epoch 25/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7553 - loss: 1.1340"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7551 - loss: 1.1340 - val_accuracy: 0.6975 - val_loss: 1.2634\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7570 - loss: 1.1014"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7570 - loss: 1.1016 - val_accuracy: 0.7071 - val_loss: 1.2966\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7684 - loss: 1.0644 - val_accuracy: 0.5679 - val_loss: 2.0460\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7539 - loss: 1.1218 - val_accuracy: 0.6325 - val_loss: 1.6853\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7672 - loss: 1.0869 - val_accuracy: 0.6721 - val_loss: 1.3971\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7573 - loss: 1.0982 - val_accuracy: 0.6379 - val_loss: 1.5781\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7578 - loss: 1.0784 - val_accuracy: 0.3767 - val_loss: 2.8376\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7660 - loss: 1.0755 - val_accuracy: 0.6242 - val_loss: 1.6329\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7823 - loss: 1.0467 - val_accuracy: 0.5683 - val_loss: 1.7232\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7711 - loss: 1.0580 - val_accuracy: 0.6683 - val_loss: 1.5342\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7691 - loss: 1.0968 - val_accuracy: 0.6921 - val_loss: 1.3679\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7763 - loss: 1.0775 - val_accuracy: 0.6062 - val_loss: 1.6472\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7780 - loss: 1.1019 - val_accuracy: 0.5075 - val_loss: 1.8717\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7788 - loss: 1.0621 - val_accuracy: 0.6146 - val_loss: 1.7142\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7964 - loss: 1.0201 - val_accuracy: 0.4313 - val_loss: 4.2297\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7891 - loss: 1.0541 - val_accuracy: 0.6633 - val_loss: 1.3924\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7884 - loss: 1.0087 - val_accuracy: 0.5883 - val_loss: 1.7116\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8046 - loss: 0.9869 - val_accuracy: 0.5938 - val_loss: 2.0947\n",
            "Epoch 43/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7867 - loss: 1.0586"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7867 - loss: 1.0582 - val_accuracy: 0.7325 - val_loss: 1.2237\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7953 - loss: 1.0203 - val_accuracy: 0.2862 - val_loss: 8.3716\n",
            "Epoch 45/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7755 - loss: 1.0768"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7758 - loss: 1.0761 - val_accuracy: 0.7425 - val_loss: 1.2094\n",
            "Epoch 46/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7929 - loss: 1.0029"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7929 - loss: 1.0030 - val_accuracy: 0.7958 - val_loss: 1.0783\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8014 - loss: 0.9872 - val_accuracy: 0.6521 - val_loss: 1.7205\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8047 - loss: 1.0192 - val_accuracy: 0.7454 - val_loss: 1.2509\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8011 - loss: 0.9878 - val_accuracy: 0.7063 - val_loss: 1.4764\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8086 - loss: 0.9694 - val_accuracy: 0.7054 - val_loss: 1.2754\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8075 - loss: 0.9733 - val_accuracy: 0.6750 - val_loss: 1.4675\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8132 - loss: 0.9568 - val_accuracy: 0.6825 - val_loss: 1.7575\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8028 - loss: 0.9952 - val_accuracy: 0.7688 - val_loss: 1.1051\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8175 - loss: 0.9513 - val_accuracy: 0.6904 - val_loss: 1.4294\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7991 - loss: 0.9837 - val_accuracy: 0.5071 - val_loss: 2.0272\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8052 - loss: 1.0081 - val_accuracy: 0.6837 - val_loss: 1.5868\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8195 - loss: 0.9677 - val_accuracy: 0.6158 - val_loss: 1.8388\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8280 - loss: 0.9139 - val_accuracy: 0.7246 - val_loss: 1.4647\n",
            "Epoch 59/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8229 - loss: 0.9216"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8225 - loss: 0.9225 - val_accuracy: 0.8188 - val_loss: 0.9588\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8167 - loss: 0.9439 - val_accuracy: 0.7104 - val_loss: 1.3079\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8097 - loss: 0.9633 - val_accuracy: 0.6942 - val_loss: 1.3320\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8125 - loss: 0.9488 - val_accuracy: 0.7083 - val_loss: 1.3256\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8182 - loss: 0.9524 - val_accuracy: 0.7404 - val_loss: 1.2641\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8240 - loss: 0.9331 - val_accuracy: 0.8021 - val_loss: 1.0299\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8203 - loss: 0.9272 - val_accuracy: 0.7821 - val_loss: 1.1041\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8346 - loss: 0.8920 - val_accuracy: 0.6992 - val_loss: 1.4798\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8186 - loss: 0.9406 - val_accuracy: 0.6329 - val_loss: 1.8397\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8150 - loss: 0.9474 - val_accuracy: 0.6504 - val_loss: 1.5236\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8149 - loss: 0.9568 - val_accuracy: 0.6708 - val_loss: 1.7129\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8336 - loss: 0.9069 - val_accuracy: 0.7408 - val_loss: 1.2133\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8338 - loss: 0.8956 - val_accuracy: 0.8071 - val_loss: 1.0135\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8338 - loss: 0.9049 - val_accuracy: 0.5854 - val_loss: 1.9635\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8307 - loss: 0.8907 - val_accuracy: 0.7638 - val_loss: 1.3265\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8287 - loss: 0.9189 - val_accuracy: 0.7679 - val_loss: 1.1355\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8248 - loss: 0.9242 - val_accuracy: 0.7433 - val_loss: 1.1931\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8362 - loss: 0.8887 - val_accuracy: 0.7721 - val_loss: 1.1174\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8349 - loss: 0.8816 - val_accuracy: 0.7321 - val_loss: 1.2805\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8193 - loss: 0.9252 - val_accuracy: 0.6938 - val_loss: 1.3589\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8391 - loss: 0.8768 - val_accuracy: 0.7563 - val_loss: 1.1767\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8377 - loss: 0.8962 - val_accuracy: 0.7708 - val_loss: 1.1637\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8358 - loss: 0.8793 - val_accuracy: 0.7725 - val_loss: 1.1409\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8301 - loss: 0.8978 - val_accuracy: 0.7600 - val_loss: 1.1424\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8375 - loss: 0.8901 - val_accuracy: 0.7517 - val_loss: 1.2263\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8473 - loss: 0.8597 - val_accuracy: 0.6425 - val_loss: 1.7638\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8378 - loss: 0.8979 - val_accuracy: 0.7933 - val_loss: 1.0931\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8274 - loss: 0.9188 - val_accuracy: 0.7183 - val_loss: 1.4463\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8466 - loss: 0.8536 - val_accuracy: 0.8050 - val_loss: 1.0321\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8362 - loss: 0.8995 - val_accuracy: 0.7642 - val_loss: 1.1032\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8393 - loss: 0.8638 - val_accuracy: 0.7833 - val_loss: 1.1180\n",
            "Model Validation Accuracy: 0.8188\n",
            "\n",
            "Testing Model with 5 layers, 304 neurons, L2=0.001\n",
            "Epoch 1/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.1858 - loss: 3.4431"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 429ms/step - accuracy: 0.1878 - loss: 3.4349 - val_accuracy: 0.1829 - val_loss: 3.1996\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3511 - loss: 2.5787 - val_accuracy: 0.1025 - val_loss: 3.1578\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4314 - loss: 2.1170 - val_accuracy: 0.0867 - val_loss: 3.1093\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.4731 - loss: 1.9205 - val_accuracy: 0.1617 - val_loss: 2.9436\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5094 - loss: 1.7872 - val_accuracy: 0.1496 - val_loss: 2.9472\n",
            "Epoch 6/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5527 - loss: 1.6831"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5527 - loss: 1.6825 - val_accuracy: 0.2962 - val_loss: 2.5698\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5720 - loss: 1.5973 - val_accuracy: 0.2308 - val_loss: 4.3436\n",
            "Epoch 8/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5817 - loss: 1.5917"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5820 - loss: 1.5906 - val_accuracy: 0.4929 - val_loss: 1.8148\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5965 - loss: 1.5026 - val_accuracy: 0.4225 - val_loss: 2.3720\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6205 - loss: 1.4949 - val_accuracy: 0.4550 - val_loss: 2.4367\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6234 - loss: 1.4769 - val_accuracy: 0.4737 - val_loss: 2.0080\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6457 - loss: 1.4776 - val_accuracy: 0.2083 - val_loss: 8.6611\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6479 - loss: 1.4295 - val_accuracy: 0.3596 - val_loss: 4.3943\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6637 - loss: 1.4062 - val_accuracy: 0.4613 - val_loss: 2.2949\n",
            "Epoch 15/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6772 - loss: 1.3696"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6771 - loss: 1.3698 - val_accuracy: 0.5758 - val_loss: 1.6451\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6733 - loss: 1.3820 - val_accuracy: 0.4958 - val_loss: 2.1648\n",
            "Epoch 17/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6909 - loss: 1.3311"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6904 - loss: 1.3324 - val_accuracy: 0.6046 - val_loss: 1.7448\n",
            "Epoch 18/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7024 - loss: 1.3107"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7021 - loss: 1.3110 - val_accuracy: 0.6550 - val_loss: 1.4185\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6908 - loss: 1.3292 - val_accuracy: 0.6042 - val_loss: 1.6161\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6968 - loss: 1.3250 - val_accuracy: 0.3833 - val_loss: 2.8239\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7050 - loss: 1.2801 - val_accuracy: 0.5579 - val_loss: 1.7652\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6908 - loss: 1.3284 - val_accuracy: 0.3283 - val_loss: 3.6422\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6951 - loss: 1.3121"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6950 - loss: 1.3123 - val_accuracy: 0.6888 - val_loss: 1.4150\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7130 - loss: 1.2676 - val_accuracy: 0.5412 - val_loss: 2.1630\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7092 - loss: 1.2933 - val_accuracy: 0.6550 - val_loss: 1.4799\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7222 - loss: 1.2253 - val_accuracy: 0.5204 - val_loss: 2.5114\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7134 - loss: 1.2959 - val_accuracy: 0.5054 - val_loss: 2.9406\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7142 - loss: 1.2670 - val_accuracy: 0.6192 - val_loss: 1.7091\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7190 - loss: 1.2307"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7189 - loss: 1.2310 - val_accuracy: 0.6988 - val_loss: 1.3111\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7299 - loss: 1.2516 - val_accuracy: 0.5221 - val_loss: 1.8939\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7105 - loss: 1.2901 - val_accuracy: 0.6729 - val_loss: 1.5737\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7224 - loss: 1.2679 - val_accuracy: 0.6004 - val_loss: 2.1166\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7523 - loss: 1.1796 - val_accuracy: 0.6417 - val_loss: 1.5489\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7455 - loss: 1.2001 - val_accuracy: 0.6204 - val_loss: 1.5795\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7500 - loss: 1.1895 - val_accuracy: 0.6804 - val_loss: 1.4291\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7436 - loss: 1.2142 - val_accuracy: 0.6333 - val_loss: 1.6619\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7405 - loss: 1.1617 - val_accuracy: 0.5971 - val_loss: 1.7515\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7656 - loss: 1.1562 - val_accuracy: 0.6263 - val_loss: 1.6354\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7470 - loss: 1.1780 - val_accuracy: 0.5142 - val_loss: 2.3761\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7479 - loss: 1.1726 - val_accuracy: 0.6867 - val_loss: 1.4790\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7506 - loss: 1.1742 - val_accuracy: 0.6058 - val_loss: 1.6084\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7596 - loss: 1.1536 - val_accuracy: 0.6450 - val_loss: 1.6124\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7431 - loss: 1.1880 - val_accuracy: 0.5058 - val_loss: 2.0502\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7552 - loss: 1.1686 - val_accuracy: 0.5550 - val_loss: 1.7153\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7539 - loss: 1.1567 - val_accuracy: 0.5750 - val_loss: 1.7317\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7600 - loss: 1.1367 - val_accuracy: 0.5508 - val_loss: 2.0110\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7557 - loss: 1.1459 - val_accuracy: 0.6633 - val_loss: 1.5275\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7598 - loss: 1.1299 - val_accuracy: 0.5896 - val_loss: 1.9985\n",
            "Epoch 49/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7529 - loss: 1.1773"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7531 - loss: 1.1765 - val_accuracy: 0.7121 - val_loss: 1.2649\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7648 - loss: 1.1431 - val_accuracy: 0.6779 - val_loss: 1.5470\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7713 - loss: 1.0902 - val_accuracy: 0.6363 - val_loss: 1.4847\n",
            "Epoch 52/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7730 - loss: 1.0895"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7728 - loss: 1.0903 - val_accuracy: 0.7904 - val_loss: 1.0972\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7782 - loss: 1.0849 - val_accuracy: 0.5608 - val_loss: 2.6722\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7794 - loss: 1.1083 - val_accuracy: 0.6687 - val_loss: 1.5138\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7744 - loss: 1.0982 - val_accuracy: 0.4721 - val_loss: 3.5061\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7824 - loss: 1.0659 - val_accuracy: 0.6625 - val_loss: 1.4965\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7680 - loss: 1.1132 - val_accuracy: 0.5971 - val_loss: 1.8066\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7994 - loss: 1.0556 - val_accuracy: 0.7825 - val_loss: 1.0720\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7784 - loss: 1.0798 - val_accuracy: 0.7088 - val_loss: 1.3286\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7825 - loss: 1.0593 - val_accuracy: 0.7258 - val_loss: 1.2469\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7868 - loss: 1.0461 - val_accuracy: 0.7692 - val_loss: 1.1608\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7844 - loss: 1.0885 - val_accuracy: 0.6925 - val_loss: 1.3363\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7853 - loss: 1.0747 - val_accuracy: 0.7779 - val_loss: 1.1292\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8062 - loss: 1.0036 - val_accuracy: 0.6621 - val_loss: 1.4338\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8013 - loss: 1.0535 - val_accuracy: 0.6658 - val_loss: 1.4499\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7878 - loss: 1.0700 - val_accuracy: 0.7508 - val_loss: 1.2266\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8004 - loss: 1.0181 - val_accuracy: 0.6408 - val_loss: 1.5865\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7926 - loss: 1.0405 - val_accuracy: 0.6812 - val_loss: 1.3997\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7917 - loss: 1.0438 - val_accuracy: 0.7275 - val_loss: 1.3170\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7999 - loss: 1.0129 - val_accuracy: 0.7713 - val_loss: 1.2229\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7993 - loss: 1.0206 - val_accuracy: 0.6446 - val_loss: 1.5493\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8065 - loss: 0.9897 - val_accuracy: 0.7154 - val_loss: 1.2629\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8083 - loss: 0.9889 - val_accuracy: 0.7304 - val_loss: 1.2792\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8099 - loss: 1.0214 - val_accuracy: 0.5750 - val_loss: 2.9790\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7933 - loss: 1.0379 - val_accuracy: 0.6963 - val_loss: 1.3539\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8031 - loss: 0.9948 - val_accuracy: 0.5667 - val_loss: 1.8312\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8101 - loss: 0.9985 - val_accuracy: 0.5908 - val_loss: 2.1435\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8091 - loss: 1.0061 - val_accuracy: 0.7462 - val_loss: 1.3004\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8106 - loss: 0.9903 - val_accuracy: 0.7308 - val_loss: 1.2680\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8086 - loss: 1.0014 - val_accuracy: 0.7613 - val_loss: 1.1205\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7903 - loss: 1.0341 - val_accuracy: 0.7733 - val_loss: 1.1532\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8108 - loss: 0.9905 - val_accuracy: 0.6571 - val_loss: 1.6600\n",
            "Model Validation Accuracy: 0.7904\n",
            "\n",
            "Testing Model with 5 layers, 304 neurons, L2=0.0015\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.2196 - loss: 3.9235"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 420ms/step - accuracy: 0.2201 - loss: 3.9195 - val_accuracy: 0.1025 - val_loss: 3.6183\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.3458 - loss: 2.6775 - val_accuracy: 0.0892 - val_loss: 3.0760\n",
            "Epoch 3/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4194 - loss: 2.1510"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4201 - loss: 2.1485 - val_accuracy: 0.1213 - val_loss: 3.3388\n",
            "Epoch 4/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4816 - loss: 1.9048"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4815 - loss: 1.9051 - val_accuracy: 0.2237 - val_loss: 2.7864\n",
            "Epoch 5/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4886 - loss: 1.8043"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4892 - loss: 1.8038 - val_accuracy: 0.2288 - val_loss: 2.6277\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5448 - loss: 1.7100 - val_accuracy: 0.1338 - val_loss: 2.8861\n",
            "Epoch 7/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5590 - loss: 1.6958"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5589 - loss: 1.6963 - val_accuracy: 0.5171 - val_loss: 1.8078\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5764 - loss: 1.6418 - val_accuracy: 0.3604 - val_loss: 2.7730\n",
            "Epoch 9/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5889 - loss: 1.6463"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5889 - loss: 1.6465 - val_accuracy: 0.5604 - val_loss: 1.7069\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6149 - loss: 1.5778 - val_accuracy: 0.5362 - val_loss: 1.8258\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6179 - loss: 1.5412 - val_accuracy: 0.5104 - val_loss: 1.8683\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6305 - loss: 1.5521 - val_accuracy: 0.2775 - val_loss: 4.0226\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6310 - loss: 1.5529 - val_accuracy: 0.5113 - val_loss: 2.1401\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6483 - loss: 1.4904 - val_accuracy: 0.3596 - val_loss: 2.9727\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6617 - loss: 1.4312 - val_accuracy: 0.5358 - val_loss: 1.9469\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6442 - loss: 1.5073 - val_accuracy: 0.5325 - val_loss: 1.9106\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6701 - loss: 1.4627 - val_accuracy: 0.5046 - val_loss: 2.0707\n",
            "Epoch 18/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6632 - loss: 1.4530"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6631 - loss: 1.4531 - val_accuracy: 0.6654 - val_loss: 1.5046\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6897 - loss: 1.3897 - val_accuracy: 0.3283 - val_loss: 3.5607\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6785 - loss: 1.4110 - val_accuracy: 0.6175 - val_loss: 1.6349\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6776 - loss: 1.4148 - val_accuracy: 0.2054 - val_loss: 6.4121\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6781 - loss: 1.3932 - val_accuracy: 0.4775 - val_loss: 2.0433\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6853 - loss: 1.4150 - val_accuracy: 0.6192 - val_loss: 1.6473\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6927 - loss: 1.3700 - val_accuracy: 0.4408 - val_loss: 2.4030\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6900 - loss: 1.3540 - val_accuracy: 0.4100 - val_loss: 2.4823\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6961 - loss: 1.3626 - val_accuracy: 0.5558 - val_loss: 1.7640\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7110 - loss: 1.3191 - val_accuracy: 0.5754 - val_loss: 1.8754\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7147 - loss: 1.2838 - val_accuracy: 0.4988 - val_loss: 2.0372\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6958 - loss: 1.3549 - val_accuracy: 0.4563 - val_loss: 2.4279\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7093 - loss: 1.2986 - val_accuracy: 0.5942 - val_loss: 1.8327\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7044 - loss: 1.3264 - val_accuracy: 0.6133 - val_loss: 1.6600\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7105 - loss: 1.2941 - val_accuracy: 0.5942 - val_loss: 1.7156\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7027 - loss: 1.2955 - val_accuracy: 0.6075 - val_loss: 1.8741\n",
            "Epoch 34/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7077 - loss: 1.3409"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7079 - loss: 1.3401 - val_accuracy: 0.7046 - val_loss: 1.3709\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7211 - loss: 1.2602 - val_accuracy: 0.5254 - val_loss: 1.9076\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7173 - loss: 1.2837 - val_accuracy: 0.6796 - val_loss: 1.4798\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7341 - loss: 1.2373 - val_accuracy: 0.6575 - val_loss: 1.6117\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7288 - loss: 1.2510 - val_accuracy: 0.6042 - val_loss: 1.9885\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7263 - loss: 1.2495 - val_accuracy: 0.6454 - val_loss: 1.4943\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7273 - loss: 1.2664 - val_accuracy: 0.4892 - val_loss: 3.4810\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7352 - loss: 1.2338 - val_accuracy: 0.5833 - val_loss: 2.0215\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7365 - loss: 1.2244 - val_accuracy: 0.4529 - val_loss: 2.1135\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7270 - loss: 1.2438 - val_accuracy: 0.6642 - val_loss: 1.5548\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7409 - loss: 1.2115 - val_accuracy: 0.5721 - val_loss: 1.8744\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7399 - loss: 1.2173 - val_accuracy: 0.6450 - val_loss: 1.6895\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7267 - loss: 1.2560 - val_accuracy: 0.3900 - val_loss: 3.0756\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7410 - loss: 1.2162 - val_accuracy: 0.6396 - val_loss: 1.6358\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7342 - loss: 1.1916 - val_accuracy: 0.6942 - val_loss: 1.3541\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7423 - loss: 1.1956 - val_accuracy: 0.6787 - val_loss: 1.3840\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7554 - loss: 1.1656 - val_accuracy: 0.6458 - val_loss: 1.5110\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7352 - loss: 1.1973 - val_accuracy: 0.6100 - val_loss: 1.9432\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7472 - loss: 1.1922 - val_accuracy: 0.6317 - val_loss: 1.6035\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7671 - loss: 1.1440 - val_accuracy: 0.5517 - val_loss: 2.5513\n",
            "Epoch 54/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7571 - loss: 1.1741"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7570 - loss: 1.1745 - val_accuracy: 0.7104 - val_loss: 1.4268\n",
            "Epoch 55/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7531 - loss: 1.1503"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7533 - loss: 1.1503 - val_accuracy: 0.7783 - val_loss: 1.1259\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7767 - loss: 1.0992 - val_accuracy: 0.6708 - val_loss: 1.4690\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7562 - loss: 1.1416 - val_accuracy: 0.6267 - val_loss: 1.9094\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7546 - loss: 1.1582 - val_accuracy: 0.6467 - val_loss: 1.8058\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7746 - loss: 1.1009 - val_accuracy: 0.5138 - val_loss: 2.6704\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7587 - loss: 1.1509 - val_accuracy: 0.4908 - val_loss: 2.4123\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7584 - loss: 1.1328 - val_accuracy: 0.6012 - val_loss: 2.1364\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7647 - loss: 1.1617 - val_accuracy: 0.6321 - val_loss: 2.0531\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7644 - loss: 1.1299 - val_accuracy: 0.7142 - val_loss: 1.3282\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7804 - loss: 1.1069 - val_accuracy: 0.6321 - val_loss: 1.8555\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7877 - loss: 1.0761 - val_accuracy: 0.6775 - val_loss: 1.4746\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7622 - loss: 1.1431 - val_accuracy: 0.6504 - val_loss: 1.5082\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7681 - loss: 1.1184 - val_accuracy: 0.7108 - val_loss: 1.4861\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7798 - loss: 1.1063 - val_accuracy: 0.6862 - val_loss: 1.4340\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7790 - loss: 1.1072 - val_accuracy: 0.7063 - val_loss: 1.3576\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7900 - loss: 1.0537 - val_accuracy: 0.7233 - val_loss: 1.2751\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7813 - loss: 1.0944 - val_accuracy: 0.4542 - val_loss: 3.2430\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7828 - loss: 1.1129 - val_accuracy: 0.6475 - val_loss: 1.6462\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7920 - loss: 1.0664 - val_accuracy: 0.7075 - val_loss: 1.4004\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7798 - loss: 1.0938 - val_accuracy: 0.6279 - val_loss: 2.0474\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7874 - loss: 1.0583 - val_accuracy: 0.5267 - val_loss: 1.7797\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7782 - loss: 1.1119 - val_accuracy: 0.7242 - val_loss: 1.3382\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7932 - loss: 1.0459 - val_accuracy: 0.5933 - val_loss: 1.7511\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7809 - loss: 1.0938 - val_accuracy: 0.5987 - val_loss: 1.6545\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7989 - loss: 1.0224 - val_accuracy: 0.6346 - val_loss: 1.9386\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7750 - loss: 1.1056 - val_accuracy: 0.7050 - val_loss: 1.3421\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7944 - loss: 1.0458 - val_accuracy: 0.6704 - val_loss: 1.5986\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7888 - loss: 1.0513 - val_accuracy: 0.6117 - val_loss: 2.4404\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7865 - loss: 1.0521 - val_accuracy: 0.5038 - val_loss: 2.4464\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7941 - loss: 1.0548 - val_accuracy: 0.6267 - val_loss: 1.8105\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.7916 - loss: 1.0442 - val_accuracy: 0.6829 - val_loss: 1.5707\n",
            "Model Validation Accuracy: 0.7783\n",
            "\n",
            "Testing Model with 5 layers, 320 neurons, L2=0.0005\n",
            "Epoch 1/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - accuracy: 0.1974 - loss: 2.9063"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 468ms/step - accuracy: 0.1992 - loss: 2.9011 - val_accuracy: 0.1150 - val_loss: 3.2604\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.3540 - loss: 2.3779 - val_accuracy: 0.1025 - val_loss: 3.0559\n",
            "Epoch 3/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4382 - loss: 2.0401"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4385 - loss: 2.0388 - val_accuracy: 0.1342 - val_loss: 3.0064\n",
            "Epoch 4/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4943 - loss: 1.8131"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.4942 - loss: 1.8131 - val_accuracy: 0.1912 - val_loss: 2.6613\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5298 - loss: 1.6760"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5299 - loss: 1.6762 - val_accuracy: 0.2679 - val_loss: 2.6613\n",
            "Epoch 6/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5758 - loss: 1.5776"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5757 - loss: 1.5773 - val_accuracy: 0.3396 - val_loss: 2.4687\n",
            "Epoch 7/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5740 - loss: 1.5334"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.5743 - loss: 1.5326 - val_accuracy: 0.3954 - val_loss: 2.0931\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6214 - loss: 1.4309"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6214 - loss: 1.4310 - val_accuracy: 0.4692 - val_loss: 1.8698\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6312 - loss: 1.4085 - val_accuracy: 0.4521 - val_loss: 2.0769\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6523 - loss: 1.3281 - val_accuracy: 0.4529 - val_loss: 2.0941\n",
            "Epoch 11/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6773 - loss: 1.2817"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6771 - loss: 1.2826 - val_accuracy: 0.5362 - val_loss: 1.8596\n",
            "Epoch 12/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6760 - loss: 1.2893"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6760 - loss: 1.2895 - val_accuracy: 0.5950 - val_loss: 1.5457\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6840 - loss: 1.2974 - val_accuracy: 0.5733 - val_loss: 1.6330\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7010 - loss: 1.2876"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7010 - loss: 1.2873 - val_accuracy: 0.6550 - val_loss: 1.4244\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6920 - loss: 1.2657 - val_accuracy: 0.6221 - val_loss: 1.4766\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7083 - loss: 1.2354 - val_accuracy: 0.5646 - val_loss: 1.9830\n",
            "Epoch 17/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7073 - loss: 1.2554"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7074 - loss: 1.2551 - val_accuracy: 0.6629 - val_loss: 1.3890\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7243 - loss: 1.1637 - val_accuracy: 0.4892 - val_loss: 2.5719\n",
            "Epoch 19/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7131 - loss: 1.2176"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7132 - loss: 1.2175 - val_accuracy: 0.6888 - val_loss: 1.3123\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7284 - loss: 1.1783 - val_accuracy: 0.6338 - val_loss: 1.5511\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7276 - loss: 1.1797 - val_accuracy: 0.4837 - val_loss: 2.5388\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7490 - loss: 1.1392 - val_accuracy: 0.5550 - val_loss: 1.8367\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7462 - loss: 1.1424 - val_accuracy: 0.4946 - val_loss: 2.2461\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7310 - loss: 1.1947 - val_accuracy: 0.4158 - val_loss: 2.4209\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7488 - loss: 1.1350 - val_accuracy: 0.6879 - val_loss: 1.3283\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7415 - loss: 1.1637 - val_accuracy: 0.5013 - val_loss: 2.2317\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7519 - loss: 1.0986 - val_accuracy: 0.5617 - val_loss: 1.7330\n",
            "Epoch 28/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7562 - loss: 1.0944"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7562 - loss: 1.0944 - val_accuracy: 0.7283 - val_loss: 1.1951\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7557 - loss: 1.0996 - val_accuracy: 0.6496 - val_loss: 1.4081\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7637 - loss: 1.1043 - val_accuracy: 0.6600 - val_loss: 1.4210\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7874 - loss: 1.0291 - val_accuracy: 0.4979 - val_loss: 2.4288\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7572 - loss: 1.1200 - val_accuracy: 0.6146 - val_loss: 1.8340\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7633 - loss: 1.0815 - val_accuracy: 0.6004 - val_loss: 1.9231\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7632 - loss: 1.1050 - val_accuracy: 0.7142 - val_loss: 1.2641\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7649 - loss: 1.0858 - val_accuracy: 0.7250 - val_loss: 1.2795\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7831 - loss: 1.0509 - val_accuracy: 0.4483 - val_loss: 3.5263\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7808 - loss: 1.0469 - val_accuracy: 0.5813 - val_loss: 1.6002\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7709 - loss: 1.0970 - val_accuracy: 0.7183 - val_loss: 1.2416\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7739 - loss: 1.0794 - val_accuracy: 0.6479 - val_loss: 1.6496\n",
            "Epoch 40/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7755 - loss: 1.0677"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7754 - loss: 1.0681 - val_accuracy: 0.7629 - val_loss: 1.1911\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7998 - loss: 1.0174 - val_accuracy: 0.5512 - val_loss: 2.0708\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7775 - loss: 1.0436 - val_accuracy: 0.5171 - val_loss: 2.0083\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7926 - loss: 1.0148 - val_accuracy: 0.6129 - val_loss: 1.7591\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7979 - loss: 1.0197 - val_accuracy: 0.6275 - val_loss: 1.8717\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7981 - loss: 1.0065 - val_accuracy: 0.4796 - val_loss: 3.7567\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7824 - loss: 1.0753 - val_accuracy: 0.6458 - val_loss: 1.5046\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7997 - loss: 1.0227 - val_accuracy: 0.6683 - val_loss: 1.4227\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8105 - loss: 0.9761 - val_accuracy: 0.7054 - val_loss: 1.4186\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8155 - loss: 0.9610 - val_accuracy: 0.7154 - val_loss: 1.3880\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8077 - loss: 0.9996 - val_accuracy: 0.6675 - val_loss: 1.4201\n",
            "Epoch 51/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8148 - loss: 0.9712"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8146 - loss: 0.9716 - val_accuracy: 0.7975 - val_loss: 1.0487\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8060 - loss: 0.9769 - val_accuracy: 0.7542 - val_loss: 1.2698\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7839 - loss: 1.0428 - val_accuracy: 0.7104 - val_loss: 1.2698\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8040 - loss: 0.9902 - val_accuracy: 0.7029 - val_loss: 1.4384\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8211 - loss: 0.9582 - val_accuracy: 0.7600 - val_loss: 1.1910\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8088 - loss: 0.9978 - val_accuracy: 0.7183 - val_loss: 1.3232\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8134 - loss: 0.9681 - val_accuracy: 0.7121 - val_loss: 1.3010\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8157 - loss: 0.9786 - val_accuracy: 0.6133 - val_loss: 1.5243\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8044 - loss: 0.9693 - val_accuracy: 0.7275 - val_loss: 1.2543\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8170 - loss: 0.9427 - val_accuracy: 0.6187 - val_loss: 1.7242\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8105 - loss: 0.9825 - val_accuracy: 0.7400 - val_loss: 1.2830\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8299 - loss: 0.9188 - val_accuracy: 0.7096 - val_loss: 1.2541\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8185 - loss: 0.9382 - val_accuracy: 0.7746 - val_loss: 1.1204\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8237 - loss: 0.9427 - val_accuracy: 0.5008 - val_loss: 2.9077\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8167 - loss: 0.9342 - val_accuracy: 0.7704 - val_loss: 1.1278\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8357 - loss: 0.8961 - val_accuracy: 0.5962 - val_loss: 2.5578\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8251 - loss: 0.9328 - val_accuracy: 0.6683 - val_loss: 1.7417\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8226 - loss: 0.9265 - val_accuracy: 0.7442 - val_loss: 1.1763\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8111 - loss: 0.9451 - val_accuracy: 0.7217 - val_loss: 1.3254\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8252 - loss: 0.9347 - val_accuracy: 0.7404 - val_loss: 1.2765\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8386 - loss: 0.8836 - val_accuracy: 0.7583 - val_loss: 1.1630\n",
            "Epoch 72/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8204 - loss: 0.9512"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8204 - loss: 0.9507 - val_accuracy: 0.8071 - val_loss: 1.0210\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8284 - loss: 0.9169 - val_accuracy: 0.7362 - val_loss: 1.3966\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8304 - loss: 0.9169 - val_accuracy: 0.7154 - val_loss: 1.4206\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8276 - loss: 0.9229 - val_accuracy: 0.7212 - val_loss: 1.2404\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8224 - loss: 0.9357 - val_accuracy: 0.7758 - val_loss: 1.1866\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8252 - loss: 0.9207 - val_accuracy: 0.7246 - val_loss: 1.4194\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8323 - loss: 0.8924 - val_accuracy: 0.7417 - val_loss: 1.3788\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8392 - loss: 0.8875 - val_accuracy: 0.6533 - val_loss: 2.0201\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8273 - loss: 0.9257 - val_accuracy: 0.5850 - val_loss: 1.7343\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8485 - loss: 0.8659 - val_accuracy: 0.7400 - val_loss: 1.4665\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8372 - loss: 0.8852 - val_accuracy: 0.7513 - val_loss: 1.1719\n",
            "Epoch 83/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8383 - loss: 0.8811"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8382 - loss: 0.8817 - val_accuracy: 0.8329 - val_loss: 0.9255\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8439 - loss: 0.8526 - val_accuracy: 0.7533 - val_loss: 1.3273\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8478 - loss: 0.8718 - val_accuracy: 0.7221 - val_loss: 1.2803\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8349 - loss: 0.8703 - val_accuracy: 0.7246 - val_loss: 1.2254\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8217 - loss: 0.9361 - val_accuracy: 0.6754 - val_loss: 1.5389\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8376 - loss: 0.8816 - val_accuracy: 0.7604 - val_loss: 1.2092\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8420 - loss: 0.8572 - val_accuracy: 0.7396 - val_loss: 1.3783\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8447 - loss: 0.8561 - val_accuracy: 0.7800 - val_loss: 1.2375\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8409 - loss: 0.8612 - val_accuracy: 0.7667 - val_loss: 1.1546\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8448 - loss: 0.8862 - val_accuracy: 0.7104 - val_loss: 1.4480\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8344 - loss: 0.8863 - val_accuracy: 0.8075 - val_loss: 1.0430\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8406 - loss: 0.8759 - val_accuracy: 0.6971 - val_loss: 1.8116\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8314 - loss: 0.9131 - val_accuracy: 0.7129 - val_loss: 1.2874\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8487 - loss: 0.8590 - val_accuracy: 0.7750 - val_loss: 1.2044\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8532 - loss: 0.8496 - val_accuracy: 0.6517 - val_loss: 1.7750\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8528 - loss: 0.8521 - val_accuracy: 0.6558 - val_loss: 1.5382\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8469 - loss: 0.8419 - val_accuracy: 0.6746 - val_loss: 1.5162\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8495 - loss: 0.8357 - val_accuracy: 0.6146 - val_loss: 1.8960\n",
            "Model Validation Accuracy: 0.8329\n",
            "\n",
            "Testing Model with 5 layers, 320 neurons, L2=0.001\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - accuracy: 0.1945 - loss: 3.4955"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 467ms/step - accuracy: 0.1951 - loss: 3.4926 - val_accuracy: 0.1025 - val_loss: 5.7102\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3379 - loss: 2.6170 - val_accuracy: 0.1025 - val_loss: 3.1530\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4025 - loss: 2.1692"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4025 - loss: 2.1686 - val_accuracy: 0.1688 - val_loss: 2.9703\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4519 - loss: 1.9362 - val_accuracy: 0.0967 - val_loss: 3.1225\n",
            "Epoch 5/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4868 - loss: 1.8091"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.4870 - loss: 1.8088 - val_accuracy: 0.1796 - val_loss: 2.6767\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5350 - loss: 1.6808"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5349 - loss: 1.6811 - val_accuracy: 0.4129 - val_loss: 1.9908\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5430 - loss: 1.6579 - val_accuracy: 0.2929 - val_loss: 2.7815\n",
            "Epoch 8/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5553 - loss: 1.6863"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5557 - loss: 1.6848 - val_accuracy: 0.4350 - val_loss: 2.0308\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5951 - loss: 1.5887 - val_accuracy: 0.2892 - val_loss: 3.2183\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6033 - loss: 1.5588"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6032 - loss: 1.5589 - val_accuracy: 0.5992 - val_loss: 1.6118\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6084 - loss: 1.5377 - val_accuracy: 0.4867 - val_loss: 1.9777\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6442 - loss: 1.4368 - val_accuracy: 0.3108 - val_loss: 2.5799\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6376 - loss: 1.4739 - val_accuracy: 0.3654 - val_loss: 2.5823\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6565 - loss: 1.4520 - val_accuracy: 0.4812 - val_loss: 1.9079\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6586 - loss: 1.4331 - val_accuracy: 0.4225 - val_loss: 2.5278\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6685 - loss: 1.4098 - val_accuracy: 0.4954 - val_loss: 2.1220\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6718 - loss: 1.3845 - val_accuracy: 0.5346 - val_loss: 2.0451\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6673 - loss: 1.4332 - val_accuracy: 0.4254 - val_loss: 2.7807\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6820 - loss: 1.3635 - val_accuracy: 0.4471 - val_loss: 2.6160\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6876 - loss: 1.3790 - val_accuracy: 0.5733 - val_loss: 2.1106\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6876 - loss: 1.3436 - val_accuracy: 0.4317 - val_loss: 2.7512\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6917 - loss: 1.3649 - val_accuracy: 0.5067 - val_loss: 1.9530\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7141 - loss: 1.2754 - val_accuracy: 0.5633 - val_loss: 1.6975\n",
            "Epoch 24/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6823 - loss: 1.3557"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6824 - loss: 1.3554 - val_accuracy: 0.6746 - val_loss: 1.4356\n",
            "Epoch 25/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7094 - loss: 1.3028"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7094 - loss: 1.3031 - val_accuracy: 0.6913 - val_loss: 1.3582\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.7085 - loss: 1.2935 - val_accuracy: 0.5050 - val_loss: 2.0709\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6943 - loss: 1.3383 - val_accuracy: 0.5396 - val_loss: 2.2155\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7106 - loss: 1.2698 - val_accuracy: 0.4829 - val_loss: 2.4576\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7136 - loss: 1.2734 - val_accuracy: 0.4775 - val_loss: 2.0664\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7166 - loss: 1.2757 - val_accuracy: 0.5667 - val_loss: 1.7713\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7192 - loss: 1.2602 - val_accuracy: 0.3892 - val_loss: 3.2132\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7011 - loss: 1.2648 - val_accuracy: 0.5667 - val_loss: 1.9064\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7203 - loss: 1.2381 - val_accuracy: 0.6504 - val_loss: 1.5957\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7306 - loss: 1.2163 - val_accuracy: 0.5487 - val_loss: 1.7900\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7335 - loss: 1.2285 - val_accuracy: 0.5492 - val_loss: 1.9631\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7189 - loss: 1.2724 - val_accuracy: 0.5467 - val_loss: 2.0077\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7490 - loss: 1.1811 - val_accuracy: 0.5854 - val_loss: 1.9152\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7548 - loss: 1.1802 - val_accuracy: 0.5546 - val_loss: 2.1738\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7523 - loss: 1.1861 - val_accuracy: 0.6646 - val_loss: 1.7592\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7428 - loss: 1.1824 - val_accuracy: 0.5442 - val_loss: 2.1982\n",
            "Epoch 41/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7430 - loss: 1.1832"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7430 - loss: 1.1836 - val_accuracy: 0.7650 - val_loss: 1.1436\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7414 - loss: 1.1652 - val_accuracy: 0.6521 - val_loss: 1.5054\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7528 - loss: 1.1697 - val_accuracy: 0.7417 - val_loss: 1.2525\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7465 - loss: 1.1730 - val_accuracy: 0.6733 - val_loss: 1.4084\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7511 - loss: 1.1584 - val_accuracy: 0.7462 - val_loss: 1.2187\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7626 - loss: 1.1208 - val_accuracy: 0.7633 - val_loss: 1.1657\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7639 - loss: 1.1250 - val_accuracy: 0.5179 - val_loss: 2.3444\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7689 - loss: 1.1136 - val_accuracy: 0.5542 - val_loss: 2.5317\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7484 - loss: 1.1676 - val_accuracy: 0.6637 - val_loss: 1.5853\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7600 - loss: 1.1288 - val_accuracy: 0.7425 - val_loss: 1.1533\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7746 - loss: 1.1032 - val_accuracy: 0.7146 - val_loss: 1.3185\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7701 - loss: 1.1028 - val_accuracy: 0.6396 - val_loss: 1.5100\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7716 - loss: 1.1085 - val_accuracy: 0.7179 - val_loss: 1.2565\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7730 - loss: 1.0977 - val_accuracy: 0.5725 - val_loss: 2.2973\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7578 - loss: 1.1157 - val_accuracy: 0.6712 - val_loss: 1.3834\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7791 - loss: 1.0608 - val_accuracy: 0.7013 - val_loss: 1.5044\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7884 - loss: 1.0797 - val_accuracy: 0.6587 - val_loss: 1.6842\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7715 - loss: 1.0807 - val_accuracy: 0.7063 - val_loss: 1.2735\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7781 - loss: 1.0924 - val_accuracy: 0.6725 - val_loss: 1.4210\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7924 - loss: 1.0291 - val_accuracy: 0.6683 - val_loss: 1.5754\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7808 - loss: 1.0963 - val_accuracy: 0.6308 - val_loss: 1.6718\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7855 - loss: 1.0819 - val_accuracy: 0.5258 - val_loss: 2.1484\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7735 - loss: 1.0716 - val_accuracy: 0.6187 - val_loss: 1.5772\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7768 - loss: 1.0873 - val_accuracy: 0.7038 - val_loss: 1.3050\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7846 - loss: 1.0623 - val_accuracy: 0.7329 - val_loss: 1.2209\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7812 - loss: 1.0323 - val_accuracy: 0.6008 - val_loss: 1.7034\n",
            "Epoch 67/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7822 - loss: 1.0616"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7823 - loss: 1.0617 - val_accuracy: 0.7654 - val_loss: 1.1677\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8001 - loss: 1.0361 - val_accuracy: 0.6754 - val_loss: 1.4732\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8027 - loss: 1.0062 - val_accuracy: 0.6925 - val_loss: 1.5638\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7998 - loss: 1.0435 - val_accuracy: 0.6787 - val_loss: 1.5343\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7921 - loss: 1.0472 - val_accuracy: 0.7058 - val_loss: 1.3750\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7899 - loss: 1.0593 - val_accuracy: 0.6558 - val_loss: 1.8589\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8006 - loss: 1.0278 - val_accuracy: 0.6212 - val_loss: 1.6878\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7811 - loss: 1.0708 - val_accuracy: 0.6862 - val_loss: 1.4682\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8083 - loss: 1.0151 - val_accuracy: 0.6621 - val_loss: 1.5197\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8013 - loss: 1.0160 - val_accuracy: 0.6146 - val_loss: 1.5550\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7944 - loss: 1.0234 - val_accuracy: 0.6796 - val_loss: 1.4589\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7978 - loss: 1.0257 - val_accuracy: 0.6862 - val_loss: 1.5849\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8096 - loss: 0.9801 - val_accuracy: 0.5942 - val_loss: 1.9565\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8104 - loss: 1.0248 - val_accuracy: 0.6338 - val_loss: 2.0742\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8067 - loss: 1.0202 - val_accuracy: 0.6812 - val_loss: 1.5128\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8060 - loss: 1.0096 - val_accuracy: 0.7425 - val_loss: 1.2574\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7911 - loss: 1.0291 - val_accuracy: 0.4437 - val_loss: 3.8317\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8153 - loss: 1.0162 - val_accuracy: 0.7142 - val_loss: 1.2945\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8027 - loss: 0.9979 - val_accuracy: 0.7133 - val_loss: 1.2415\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8270 - loss: 0.9428 - val_accuracy: 0.7617 - val_loss: 1.1775\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8091 - loss: 0.9699 - val_accuracy: 0.6267 - val_loss: 1.5538\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8051 - loss: 1.0014 - val_accuracy: 0.6538 - val_loss: 1.8058\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8008 - loss: 1.0051 - val_accuracy: 0.5825 - val_loss: 1.7656\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8240 - loss: 0.9679 - val_accuracy: 0.6442 - val_loss: 1.8657\n",
            "Epoch 91/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8110 - loss: 0.9975"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8109 - loss: 0.9979 - val_accuracy: 0.7929 - val_loss: 1.1246\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8096 - loss: 0.9793 - val_accuracy: 0.7692 - val_loss: 1.1094\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8051 - loss: 0.9824 - val_accuracy: 0.6950 - val_loss: 1.3623\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8204 - loss: 0.9483 - val_accuracy: 0.5679 - val_loss: 2.5231\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8206 - loss: 0.9464 - val_accuracy: 0.7704 - val_loss: 1.2219\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8123 - loss: 0.9562 - val_accuracy: 0.6625 - val_loss: 1.8208\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8211 - loss: 0.9498 - val_accuracy: 0.6975 - val_loss: 1.5362\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8065 - loss: 0.9832 - val_accuracy: 0.6917 - val_loss: 1.6357\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8164 - loss: 0.9592 - val_accuracy: 0.7242 - val_loss: 1.2519\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8253 - loss: 0.9319 - val_accuracy: 0.7892 - val_loss: 1.1555\n",
            "Model Validation Accuracy: 0.7929\n",
            "\n",
            "Testing Model with 5 layers, 320 neurons, L2=0.0015\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - accuracy: 0.2008 - loss: 4.0279"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 478ms/step - accuracy: 0.2014 - loss: 4.0235 - val_accuracy: 0.1058 - val_loss: 3.9490\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.3387 - loss: 2.7165 - val_accuracy: 0.0896 - val_loss: 3.1004\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3981 - loss: 2.1989 - val_accuracy: 0.0867 - val_loss: 3.0529\n",
            "Epoch 4/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4623 - loss: 1.9467"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4620 - loss: 1.9467 - val_accuracy: 0.1108 - val_loss: 2.8191\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4671 - loss: 1.9012"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4671 - loss: 1.9008 - val_accuracy: 0.1425 - val_loss: 2.8380\n",
            "Epoch 6/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5133 - loss: 1.7652"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5132 - loss: 1.7660 - val_accuracy: 0.2937 - val_loss: 2.3757\n",
            "Epoch 7/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5468 - loss: 1.7228"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5468 - loss: 1.7228 - val_accuracy: 0.3058 - val_loss: 2.3233\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5478 - loss: 1.7269"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5478 - loss: 1.7268 - val_accuracy: 0.3717 - val_loss: 2.1343\n",
            "Epoch 9/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5768 - loss: 1.6829"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5769 - loss: 1.6827 - val_accuracy: 0.3892 - val_loss: 2.1230\n",
            "Epoch 10/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5855 - loss: 1.6542"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5857 - loss: 1.6538 - val_accuracy: 0.4421 - val_loss: 2.1599\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5992 - loss: 1.6377 - val_accuracy: 0.2246 - val_loss: 4.3077\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6287 - loss: 1.5357 - val_accuracy: 0.4263 - val_loss: 2.8850\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6203 - loss: 1.6117 - val_accuracy: 0.1912 - val_loss: 3.3005\n",
            "Epoch 14/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6344 - loss: 1.5220"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6343 - loss: 1.5235 - val_accuracy: 0.5263 - val_loss: 1.8671\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6475 - loss: 1.5226 - val_accuracy: 0.5004 - val_loss: 1.8565\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6536 - loss: 1.5056 - val_accuracy: 0.4925 - val_loss: 1.9484\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6448 - loss: 1.5423 - val_accuracy: 0.4292 - val_loss: 2.3202\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6527 - loss: 1.5157 - val_accuracy: 0.4296 - val_loss: 2.2920\n",
            "Epoch 19/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6542 - loss: 1.5024"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6544 - loss: 1.5022 - val_accuracy: 0.5692 - val_loss: 1.8046\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6681 - loss: 1.4578 - val_accuracy: 0.5038 - val_loss: 1.8302\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6521 - loss: 1.4963 - val_accuracy: 0.5092 - val_loss: 1.8223\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6644 - loss: 1.4295 - val_accuracy: 0.5492 - val_loss: 1.8137\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6768 - loss: 1.4302 - val_accuracy: 0.5337 - val_loss: 2.0975\n",
            "Epoch 24/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6783 - loss: 1.4140"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6785 - loss: 1.4135 - val_accuracy: 0.6087 - val_loss: 1.6667\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6887 - loss: 1.3611 - val_accuracy: 0.4208 - val_loss: 4.0510\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6822 - loss: 1.4003 - val_accuracy: 0.4900 - val_loss: 2.9261\n",
            "Epoch 27/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6945 - loss: 1.3675"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6945 - loss: 1.3674 - val_accuracy: 0.6758 - val_loss: 1.4622\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6932 - loss: 1.3574 - val_accuracy: 0.5554 - val_loss: 1.9221\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7037 - loss: 1.3735 - val_accuracy: 0.5396 - val_loss: 1.8669\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6998 - loss: 1.3491 - val_accuracy: 0.5946 - val_loss: 1.6957\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7007 - loss: 1.3461 - val_accuracy: 0.3683 - val_loss: 3.8305\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6977 - loss: 1.3314 - val_accuracy: 0.5362 - val_loss: 1.9452\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7187 - loss: 1.3099 - val_accuracy: 0.6471 - val_loss: 1.5260\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7234 - loss: 1.2990 - val_accuracy: 0.4404 - val_loss: 3.3325\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7091 - loss: 1.3115 - val_accuracy: 0.5654 - val_loss: 1.8238\n",
            "Epoch 36/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7296 - loss: 1.2654"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7292 - loss: 1.2662 - val_accuracy: 0.7108 - val_loss: 1.3421\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7101 - loss: 1.2946 - val_accuracy: 0.5525 - val_loss: 1.7389\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6990 - loss: 1.3363 - val_accuracy: 0.7004 - val_loss: 1.3785\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7341 - loss: 1.2096 - val_accuracy: 0.6096 - val_loss: 1.6582\n",
            "Epoch 40/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7182 - loss: 1.2436"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7183 - loss: 1.2440 - val_accuracy: 0.7308 - val_loss: 1.2769\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7266 - loss: 1.2648 - val_accuracy: 0.5146 - val_loss: 2.3500\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7306 - loss: 1.2438 - val_accuracy: 0.6221 - val_loss: 1.6749\n",
            "Epoch 43/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7332 - loss: 1.2110"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7330 - loss: 1.2117 - val_accuracy: 0.7362 - val_loss: 1.2192\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7296 - loss: 1.2541 - val_accuracy: 0.6558 - val_loss: 1.5272\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7315 - loss: 1.2306 - val_accuracy: 0.6096 - val_loss: 1.9976\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7386 - loss: 1.1850 - val_accuracy: 0.4646 - val_loss: 2.4846\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7428 - loss: 1.2024 - val_accuracy: 0.6154 - val_loss: 1.6025\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7244 - loss: 1.2363 - val_accuracy: 0.4750 - val_loss: 2.5320\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7471 - loss: 1.1970 - val_accuracy: 0.5625 - val_loss: 1.8549\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7504 - loss: 1.1652 - val_accuracy: 0.6071 - val_loss: 1.8383\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7583 - loss: 1.1442 - val_accuracy: 0.7121 - val_loss: 1.2717\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7446 - loss: 1.1897 - val_accuracy: 0.6533 - val_loss: 1.4801\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7434 - loss: 1.1871 - val_accuracy: 0.7283 - val_loss: 1.3241\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7636 - loss: 1.1411 - val_accuracy: 0.6125 - val_loss: 1.7514\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7643 - loss: 1.1461 - val_accuracy: 0.7183 - val_loss: 1.2974\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7522 - loss: 1.1638 - val_accuracy: 0.7088 - val_loss: 1.3026\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7525 - loss: 1.1672 - val_accuracy: 0.4850 - val_loss: 2.6692\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7646 - loss: 1.1273 - val_accuracy: 0.4804 - val_loss: 2.3459\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7636 - loss: 1.1280 - val_accuracy: 0.5396 - val_loss: 2.3485\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7739 - loss: 1.1154 - val_accuracy: 0.6029 - val_loss: 1.8232\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7725 - loss: 1.1319 - val_accuracy: 0.7321 - val_loss: 1.2671\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7621 - loss: 1.1176 - val_accuracy: 0.6438 - val_loss: 1.4915\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7715 - loss: 1.1054 - val_accuracy: 0.7217 - val_loss: 1.3651\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7755 - loss: 1.1160 - val_accuracy: 0.7267 - val_loss: 1.2402\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7934 - loss: 1.0664 - val_accuracy: 0.4867 - val_loss: 2.0370\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7741 - loss: 1.1178 - val_accuracy: 0.6046 - val_loss: 1.8280\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7626 - loss: 1.1361 - val_accuracy: 0.6967 - val_loss: 1.2912\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7766 - loss: 1.1044 - val_accuracy: 0.5663 - val_loss: 2.0449\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7712 - loss: 1.1294 - val_accuracy: 0.6646 - val_loss: 1.5026\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7792 - loss: 1.0908 - val_accuracy: 0.6404 - val_loss: 1.5816\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7814 - loss: 1.1040 - val_accuracy: 0.5867 - val_loss: 2.6924\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7789 - loss: 1.1104"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7791 - loss: 1.1099 - val_accuracy: 0.7392 - val_loss: 1.2014\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7832 - loss: 1.0398 - val_accuracy: 0.5487 - val_loss: 2.3492\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7771 - loss: 1.0760 - val_accuracy: 0.5467 - val_loss: 3.1147\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7759 - loss: 1.0999 - val_accuracy: 0.4271 - val_loss: 3.8522\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7889 - loss: 1.0641 - val_accuracy: 0.6087 - val_loss: 1.9565\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7839 - loss: 1.0771 - val_accuracy: 0.6358 - val_loss: 1.7671\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7904 - loss: 1.0707 - val_accuracy: 0.7258 - val_loss: 1.2622\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7826 - loss: 1.0995 - val_accuracy: 0.5671 - val_loss: 1.9662\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8078 - loss: 1.0279 - val_accuracy: 0.5104 - val_loss: 1.8660\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7870 - loss: 1.0650 - val_accuracy: 0.6762 - val_loss: 1.6236\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7794 - loss: 1.0808 - val_accuracy: 0.5854 - val_loss: 2.3152\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7950 - loss: 1.0466 - val_accuracy: 0.5279 - val_loss: 2.5029\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7995 - loss: 1.0442 - val_accuracy: 0.6108 - val_loss: 1.6311\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7997 - loss: 1.0348 - val_accuracy: 0.5567 - val_loss: 2.9151\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8005 - loss: 1.0459 - val_accuracy: 0.6988 - val_loss: 1.5492\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7867 - loss: 1.0606 - val_accuracy: 0.4812 - val_loss: 3.5583\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7980 - loss: 1.0210 - val_accuracy: 0.7083 - val_loss: 1.3703\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7991 - loss: 1.0190 - val_accuracy: 0.7383 - val_loss: 1.2825\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8056 - loss: 1.0101 - val_accuracy: 0.5654 - val_loss: 1.8637\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8017 - loss: 1.0242 - val_accuracy: 0.6388 - val_loss: 1.6398\n",
            "Epoch 92/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7982 - loss: 1.0548"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7981 - loss: 1.0549 - val_accuracy: 0.7404 - val_loss: 1.2987\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8048 - loss: 1.0143 - val_accuracy: 0.6000 - val_loss: 1.6372\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8121 - loss: 1.0086 - val_accuracy: 0.6662 - val_loss: 2.0160\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7906 - loss: 1.0741 - val_accuracy: 0.7054 - val_loss: 1.3921\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7927 - loss: 1.0426 - val_accuracy: 0.7142 - val_loss: 1.3225\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8079 - loss: 1.0045 - val_accuracy: 0.5158 - val_loss: 3.1783\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8227 - loss: 0.9776 - val_accuracy: 0.6279 - val_loss: 1.9782\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8019 - loss: 1.0305 - val_accuracy: 0.5350 - val_loss: 2.3199\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8163 - loss: 0.9718 - val_accuracy: 0.6921 - val_loss: 1.4315\n",
            "Model Validation Accuracy: 0.7404\n",
            "\n",
            "Testing Model with 5 layers, 128 neurons, L2=0.0005\n",
            "Epoch 1/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 0.1826 - loss: 2.5279"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 330ms/step - accuracy: 0.1839 - loss: 2.5247 - val_accuracy: 0.1608 - val_loss: 2.6028\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.3220 - loss: 2.2188 - val_accuracy: 0.1096 - val_loss: 2.7948\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.4097 - loss: 1.9800 - val_accuracy: 0.1483 - val_loss: 2.6686\n",
            "Epoch 4/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4610 - loss: 1.7857"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4611 - loss: 1.7854 - val_accuracy: 0.2108 - val_loss: 2.5219\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5159 - loss: 1.6307"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5158 - loss: 1.6307 - val_accuracy: 0.3304 - val_loss: 2.1085\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5428 - loss: 1.5471"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5428 - loss: 1.5470 - val_accuracy: 0.5071 - val_loss: 1.7003\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5875 - loss: 1.4525 - val_accuracy: 0.3896 - val_loss: 1.9786\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.6119 - loss: 1.3762 - val_accuracy: 0.4387 - val_loss: 1.9919\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6247 - loss: 1.3140"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6247 - loss: 1.3139 - val_accuracy: 0.5892 - val_loss: 1.3966\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6430 - loss: 1.2633 - val_accuracy: 0.4979 - val_loss: 1.8981\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6647 - loss: 1.2228"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6646 - loss: 1.2227 - val_accuracy: 0.6292 - val_loss: 1.3107\n",
            "Epoch 12/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6953 - loss: 1.1333"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6950 - loss: 1.1340 - val_accuracy: 0.6862 - val_loss: 1.1216\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.6792 - loss: 1.1557 - val_accuracy: 0.6296 - val_loss: 1.3932\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7016 - loss: 1.1209 - val_accuracy: 0.5587 - val_loss: 1.8430\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6850 - loss: 1.1227 - val_accuracy: 0.6508 - val_loss: 1.2906\n",
            "Epoch 16/100\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6994 - loss: 1.1054"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6998 - loss: 1.1049 - val_accuracy: 0.6983 - val_loss: 1.1843\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.7045 - loss: 1.0645 - val_accuracy: 0.5842 - val_loss: 1.5940\n",
            "Epoch 18/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7211 - loss: 1.0306"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7209 - loss: 1.0316 - val_accuracy: 0.7008 - val_loss: 1.1283\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7258 - loss: 1.0225 - val_accuracy: 0.5983 - val_loss: 1.4584\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7248 - loss: 1.0170 - val_accuracy: 0.4867 - val_loss: 2.4801\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7411 - loss: 0.9844 - val_accuracy: 0.5046 - val_loss: 1.9288\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7345 - loss: 1.0000 - val_accuracy: 0.6242 - val_loss: 1.5162\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7439 - loss: 1.0000 - val_accuracy: 0.6017 - val_loss: 1.6108\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7432 - loss: 0.9602 - val_accuracy: 0.6308 - val_loss: 1.5723\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7452 - loss: 0.9773"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7451 - loss: 0.9773 - val_accuracy: 0.7117 - val_loss: 1.1101\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7543 - loss: 0.9614 - val_accuracy: 0.6925 - val_loss: 1.1863\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7601 - loss: 0.9586 - val_accuracy: 0.7096 - val_loss: 1.1684\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7593 - loss: 0.9349 - val_accuracy: 0.7004 - val_loss: 1.2333\n",
            "Epoch 29/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7567 - loss: 0.9514"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7569 - loss: 0.9510 - val_accuracy: 0.7462 - val_loss: 0.9936\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7771 - loss: 0.9050 - val_accuracy: 0.6717 - val_loss: 1.3304\n",
            "Epoch 31/100\n",
            "\u001b[1m17/88\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7710 - loss: 0.9142"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "# Focused Grid Search\n",
        "if best_hyperparams is not None:\n",
        "    best_num_layers, best_neurons, best_reg = best_hyperparams\n",
        "    focused_num_layers_options = [max(1, best_num_layers-1), best_num_layers, min(6, best_num_layers+1)]\n",
        "    focused_neurons_options = [max(16, best_neurons-16), best_neurons, min(128, best_neurons+16)]\n",
        "    focused_regs = [max(0.0001, best_reg-0.0005), best_reg, best_reg+0.0005]\n",
        "\n",
        "    focused_hyperparameter_combinations = list(itertools.product(focused_num_layers_options, focused_neurons_options, focused_regs))\n",
        "\n",
        "    for num_layers, neurons, reg in focused_hyperparameter_combinations:\n",
        "        print(f\"\\nTesting Model with {num_layers} layers, {neurons} neurons, L2={reg}\")\n",
        "\n",
        "        with strategy.scope():\n",
        "            model = Sequential()\n",
        "            model.add(Input(shape=(28, 28, 3)))\n",
        "\n",
        "            for i in range(num_layers):\n",
        "                model.add(Conv2D(filters=neurons, kernel_size=(3,3), padding='same', kernel_regularizer=l2(reg)))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(LeakyReLU())\n",
        "                # Apply MaxPooling2D every other layer to avoid excessive downsampling\n",
        "                if i % 2 == 1:\n",
        "                    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "                model.add(Dropout(0.3))\n",
        "\n",
        "            model.add(GlobalAveragePooling2D())\n",
        "            model.add(Dense(neurons, activation=\"relu\"))\n",
        "            model.add(Dropout(0.5))\n",
        "            model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "            optimizer = AdamW(learning_rate=0.001)\n",
        "            model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, verbose=0)\n",
        "            checkpoint = ModelCheckpoint('model_checkpoint.h5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "            csv_logger = CSVLogger('training_log.csv', append=True)\n",
        "\n",
        "            history = model.fit(\n",
        "                datagen.flow(X_train, y_train, batch_size=64),\n",
        "                validation_data=(X_valid, y_valid),\n",
        "                epochs=100,  # Reduced number of epochs\n",
        "                callbacks=[early_stopping, checkpoint, csv_logger],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "        best_epoch = np.argmax(history.history['val_accuracy'])\n",
        "        train_acc = history.history['accuracy'][best_epoch]\n",
        "        val_acc = history.history['val_accuracy'][best_epoch]\n",
        "        print(f\"Model Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "        results_G.append({\n",
        "            \"num_layers\": num_layers,\n",
        "            \"neurons\": neurons,\n",
        "            \"l2_reg\": reg,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_acc\": val_acc\n",
        "        })\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model = model\n",
        "            best_hyperparams = (num_layers, neurons, reg)\n",
        "\n",
        "df_results_G = pd.DataFrame(results_G)\n",
        "\n",
        "if best_hyperparams is not None:\n",
        "    print(\"\\nBest Model Found:\")\n",
        "    print(f\"Layers: {best_hyperparams[0]}, Neurons: {best_hyperparams[1]}, L2 Regularization: {best_hyperparams[2]}\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "else:\n",
        "    print(\"No model achieved a validation accuracy higher than the initial value.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "cg1liZtOd5L9",
        "outputId": "f5280291-8adf-469e-f84c-838c198bb87f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_results_G' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4c666c1e2254>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the DataFrame to a CSV file in your Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_results_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/results_r_gr.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results saved to Google Drive at /content/drive/My Drive/results_r_gr.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_results_G' is not defined"
          ]
        }
      ],
      "source": [
        "# Save the DataFrame to a CSV file in your Google Drive\n",
        "df_results_G.to_csv('/content/drive/My Drive/results_r_gr.csv', index=False)\n",
        "\n",
        "print(\"Results saved to Google Drive at /content/drive/My Drive/results_r_gr.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3K4YlPHsECs",
        "outputId": "f4403244-44f8-4a62-87d2-31a20ce81b7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Model with 5 layers, 992 neurons, L2=0.001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 451ms/step - accuracy: 0.1834 - loss: 6.1226 - val_accuracy: 0.1025 - val_loss: 3.6522\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.3062 - loss: 2.8478 - val_accuracy: 0.0867 - val_loss: 4.2786\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.3454 - loss: 2.2841 - val_accuracy: 0.0867 - val_loss: 4.0139\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.3598 - loss: 2.2310 - val_accuracy: 0.0875 - val_loss: 3.1137\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.3944 - loss: 2.1765 - val_accuracy: 0.0892 - val_loss: 3.5772\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 77ms/step - accuracy: 0.3998 - loss: 2.1633 - val_accuracy: 0.2267 - val_loss: 2.7473\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - accuracy: 0.4341 - loss: 2.1375 - val_accuracy: 0.3108 - val_loss: 2.5214\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.4436 - loss: 2.0801 - val_accuracy: 0.2783 - val_loss: 2.5635\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.4604 - loss: 2.1287 - val_accuracy: 0.2663 - val_loss: 2.6860\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.4616 - loss: 2.1461 - val_accuracy: 0.3100 - val_loss: 2.5002\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.4707 - loss: 2.0913 - val_accuracy: 0.2713 - val_loss: 3.1767\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.4347 - loss: 2.2154 - val_accuracy: 0.2858 - val_loss: 2.8260\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 79ms/step - accuracy: 0.4751 - loss: 2.0977 - val_accuracy: 0.4075 - val_loss: 2.1882\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.4823 - loss: 2.0975 - val_accuracy: 0.1800 - val_loss: 4.0068\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5088 - loss: 2.0247 - val_accuracy: 0.2442 - val_loss: 3.0717\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 82ms/step - accuracy: 0.4913 - loss: 2.0619 - val_accuracy: 0.4125 - val_loss: 2.4478\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5006 - loss: 2.0614 - val_accuracy: 0.2650 - val_loss: 3.1127\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.4949 - loss: 2.0654 - val_accuracy: 0.1346 - val_loss: 11.0195\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5156 - loss: 2.0261 - val_accuracy: 0.2521 - val_loss: 3.6238\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.5192 - loss: 2.0723 - val_accuracy: 0.2408 - val_loss: 3.2932\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5373 - loss: 1.9507 - val_accuracy: 0.2992 - val_loss: 2.5859\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 82ms/step - accuracy: 0.5347 - loss: 1.9852 - val_accuracy: 0.4550 - val_loss: 2.2200\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.5429 - loss: 2.0229 - val_accuracy: 0.2192 - val_loss: 4.3592\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5435 - loss: 1.9533 - val_accuracy: 0.3733 - val_loss: 2.7450\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5629 - loss: 1.9428 - val_accuracy: 0.4092 - val_loss: 2.3024\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.5585 - loss: 1.9063 - val_accuracy: 0.3542 - val_loss: 2.9271\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.5652 - loss: 1.8668 - val_accuracy: 0.2837 - val_loss: 3.6865\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.5736 - loss: 1.8954 - val_accuracy: 0.2562 - val_loss: 4.0967\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 82ms/step - accuracy: 0.5847 - loss: 1.8807 - val_accuracy: 0.4650 - val_loss: 2.2060\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5985 - loss: 1.8040 - val_accuracy: 0.4496 - val_loss: 2.4829\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 82ms/step - accuracy: 0.6156 - loss: 1.7590 - val_accuracy: 0.5679 - val_loss: 1.9557\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5934 - loss: 1.8323 - val_accuracy: 0.3775 - val_loss: 2.4392\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6231 - loss: 1.7125 - val_accuracy: 0.3200 - val_loss: 3.0855\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6127 - loss: 1.7273 - val_accuracy: 0.4279 - val_loss: 2.4060\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 81ms/step - accuracy: 0.6339 - loss: 1.7081 - val_accuracy: 0.6212 - val_loss: 1.7318\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6337 - loss: 1.6811 - val_accuracy: 0.3892 - val_loss: 2.6233\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6296 - loss: 1.6676 - val_accuracy: 0.5450 - val_loss: 1.9394\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.6484 - loss: 1.6200 - val_accuracy: 0.4512 - val_loss: 2.5375\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6401 - loss: 1.6231 - val_accuracy: 0.3258 - val_loss: 3.7561\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6513 - loss: 1.6509 - val_accuracy: 0.2338 - val_loss: 6.2791\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6603 - loss: 1.5441 - val_accuracy: 0.2783 - val_loss: 4.8112\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6608 - loss: 1.5607 - val_accuracy: 0.5033 - val_loss: 2.1894\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6739 - loss: 1.5531 - val_accuracy: 0.5171 - val_loss: 2.0684\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6718 - loss: 1.4808 - val_accuracy: 0.4796 - val_loss: 2.3816\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6723 - loss: 1.4948 - val_accuracy: 0.4325 - val_loss: 2.4830\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6807 - loss: 1.4837 - val_accuracy: 0.5792 - val_loss: 1.7420\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.6761 - loss: 1.5013 - val_accuracy: 0.5029 - val_loss: 1.9374\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6899 - loss: 1.4404 - val_accuracy: 0.6012 - val_loss: 1.7639\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6845 - loss: 1.4903 - val_accuracy: 0.6150 - val_loss: 1.6567\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6931 - loss: 1.4043 - val_accuracy: 0.4321 - val_loss: 2.8373\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.6884 - loss: 1.4548 - val_accuracy: 0.5892 - val_loss: 1.7482\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.7024 - loss: 1.4136 - val_accuracy: 0.3525 - val_loss: 4.4071\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.7075 - loss: 1.3997 - val_accuracy: 0.2854 - val_loss: 3.1914\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7090 - loss: 1.4017 - val_accuracy: 0.3771 - val_loss: 3.1548\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.7305 - loss: 1.3509 - val_accuracy: 0.3692 - val_loss: 3.5692\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.7132 - loss: 1.3586 - val_accuracy: 0.5763 - val_loss: 1.9222\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.7197 - loss: 1.3384 - val_accuracy: 0.3429 - val_loss: 3.5600\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7216 - loss: 1.3481 - val_accuracy: 0.4162 - val_loss: 3.4978\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7091 - loss: 1.4072 - val_accuracy: 0.3771 - val_loss: 2.6690\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7318 - loss: 1.3217 - val_accuracy: 0.4196 - val_loss: 2.4991\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7203 - loss: 1.3601 - val_accuracy: 0.4638 - val_loss: 2.4822\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.7316 - loss: 1.2941 - val_accuracy: 0.3642 - val_loss: 3.6895\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.7396 - loss: 1.3124 - val_accuracy: 0.5196 - val_loss: 2.3426\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7316 - loss: 1.3475 - val_accuracy: 0.3821 - val_loss: 3.3084\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.7365 - loss: 1.2844 - val_accuracy: 0.5496 - val_loss: 2.3904\n",
            "Model Validation Accuracy: 0.6212\n",
            "Best Validation Accuracy: 0.6212\n",
            "Best Model: <Sequential name=sequential, built=True>\n",
            "Best Hyperparameters: Layers=5, Neurons=992, L2=0.001\n",
            "\n",
            "Testing Model with 9 layers, 352 neurons, L2=0.001\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 604ms/step - accuracy: 0.1808 - loss: 5.1021 - val_accuracy: 0.1388 - val_loss: 6.1482\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3607 - loss: 3.9190 - val_accuracy: 0.1233 - val_loss: 4.0385\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.4819 - loss: 2.9785 - val_accuracy: 0.2046 - val_loss: 3.2901\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.5387 - loss: 2.4104 - val_accuracy: 0.3600 - val_loss: 2.7223\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.5651 - loss: 2.1550 - val_accuracy: 0.4479 - val_loss: 2.3642\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.5745 - loss: 2.0396 - val_accuracy: 0.4808 - val_loss: 2.2974\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6056 - loss: 1.9190 - val_accuracy: 0.4650 - val_loss: 2.2871\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6128 - loss: 1.9198 - val_accuracy: 0.5746 - val_loss: 2.2474\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6267 - loss: 1.8986 - val_accuracy: 0.5708 - val_loss: 2.1853\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6403 - loss: 1.8675 - val_accuracy: 0.5650 - val_loss: 2.0495\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6375 - loss: 1.8440 - val_accuracy: 0.5038 - val_loss: 2.4630\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.6503 - loss: 1.8482 - val_accuracy: 0.5987 - val_loss: 2.0243\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6453 - loss: 1.9030 - val_accuracy: 0.6579 - val_loss: 1.8164\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6656 - loss: 1.8018 - val_accuracy: 0.6208 - val_loss: 1.9850\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6548 - loss: 1.8273 - val_accuracy: 0.5050 - val_loss: 2.6053\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6666 - loss: 1.7997 - val_accuracy: 0.5288 - val_loss: 2.3700\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6553 - loss: 1.8621 - val_accuracy: 0.6488 - val_loss: 1.8984\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.6785 - loss: 1.8225 - val_accuracy: 0.6792 - val_loss: 1.7997\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6900 - loss: 1.7740 - val_accuracy: 0.6250 - val_loss: 1.9757\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6731 - loss: 1.8370 - val_accuracy: 0.5029 - val_loss: 2.8025\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6961 - loss: 1.7859 - val_accuracy: 0.5954 - val_loss: 2.1214\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7018 - loss: 1.7416 - val_accuracy: 0.5688 - val_loss: 2.1312\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6833 - loss: 1.8289 - val_accuracy: 0.6167 - val_loss: 2.1073\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6960 - loss: 1.8131 - val_accuracy: 0.5054 - val_loss: 2.5160\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6949 - loss: 1.7751 - val_accuracy: 0.6125 - val_loss: 2.0012\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6888 - loss: 1.8057 - val_accuracy: 0.6629 - val_loss: 1.9094\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6996 - loss: 1.7603 - val_accuracy: 0.6196 - val_loss: 2.2035\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7128 - loss: 1.7247 - val_accuracy: 0.5621 - val_loss: 2.1570\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7149 - loss: 1.7503 - val_accuracy: 0.5646 - val_loss: 2.2183\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7069 - loss: 1.7309 - val_accuracy: 0.5412 - val_loss: 2.3228\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7080 - loss: 1.7784 - val_accuracy: 0.6737 - val_loss: 1.8912\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7197 - loss: 1.6844 - val_accuracy: 0.6733 - val_loss: 1.8080\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7170 - loss: 1.7304 - val_accuracy: 0.6367 - val_loss: 2.0140\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7049 - loss: 1.7695 - val_accuracy: 0.6346 - val_loss: 2.1682\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7155 - loss: 1.7365 - val_accuracy: 0.6783 - val_loss: 1.8150\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7167 - loss: 1.7195 - val_accuracy: 0.6321 - val_loss: 2.0199\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7125 - loss: 1.6867 - val_accuracy: 0.6000 - val_loss: 2.1362\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7218 - loss: 1.7291 - val_accuracy: 0.6367 - val_loss: 1.9088\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7118 - loss: 1.7219 - val_accuracy: 0.4921 - val_loss: 2.7213\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.7089 - loss: 1.7287 - val_accuracy: 0.7004 - val_loss: 1.8231\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7077 - loss: 1.7156 - val_accuracy: 0.5446 - val_loss: 2.4092\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.7122 - loss: 1.7153 - val_accuracy: 0.7483 - val_loss: 1.6125\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7150 - loss: 1.6759 - val_accuracy: 0.6212 - val_loss: 1.9562\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7447 - loss: 1.6311 - val_accuracy: 0.5408 - val_loss: 2.4324\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7375 - loss: 1.6500 - val_accuracy: 0.6033 - val_loss: 2.0267\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.7472 - loss: 1.6087 - val_accuracy: 0.7504 - val_loss: 1.6331\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7252 - loss: 1.6576 - val_accuracy: 0.5675 - val_loss: 2.1105\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7471 - loss: 1.6013 - val_accuracy: 0.6800 - val_loss: 1.7131\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7184 - loss: 1.6752 - val_accuracy: 0.7446 - val_loss: 1.5890\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7453 - loss: 1.6242 - val_accuracy: 0.7492 - val_loss: 1.6042\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7393 - loss: 1.6157 - val_accuracy: 0.6092 - val_loss: 2.2620\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7344 - loss: 1.6710 - val_accuracy: 0.6475 - val_loss: 1.9536\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7498 - loss: 1.5808 - val_accuracy: 0.7133 - val_loss: 1.7186\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7322 - loss: 1.6106 - val_accuracy: 0.6458 - val_loss: 2.0918\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7505 - loss: 1.5757 - val_accuracy: 0.6096 - val_loss: 1.9974\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7472 - loss: 1.5713 - val_accuracy: 0.6529 - val_loss: 1.9967\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7373 - loss: 1.6116 - val_accuracy: 0.6671 - val_loss: 1.9310\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7577 - loss: 1.5724 - val_accuracy: 0.7150 - val_loss: 1.7642\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7410 - loss: 1.6091 - val_accuracy: 0.6671 - val_loss: 1.8958\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7408 - loss: 1.5911 - val_accuracy: 0.4179 - val_loss: 3.8998\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7617 - loss: 1.5573 - val_accuracy: 0.7212 - val_loss: 1.6212\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7471 - loss: 1.6117 - val_accuracy: 0.5804 - val_loss: 2.6220\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7533 - loss: 1.5927 - val_accuracy: 0.7175 - val_loss: 1.7411\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7626 - loss: 1.5337 - val_accuracy: 0.7912 - val_loss: 1.4260\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7767 - loss: 1.5190 - val_accuracy: 0.5025 - val_loss: 3.5464\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7498 - loss: 1.5752 - val_accuracy: 0.6729 - val_loss: 1.8430\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7648 - loss: 1.5067 - val_accuracy: 0.6933 - val_loss: 1.7977\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7634 - loss: 1.5261 - val_accuracy: 0.7458 - val_loss: 1.5991\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7651 - loss: 1.4879 - val_accuracy: 0.6254 - val_loss: 2.2167\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7637 - loss: 1.5290 - val_accuracy: 0.7287 - val_loss: 1.6155\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7497 - loss: 1.5135 - val_accuracy: 0.6742 - val_loss: 1.7758\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7539 - loss: 1.5568 - val_accuracy: 0.7013 - val_loss: 1.7529\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7613 - loss: 1.5153 - val_accuracy: 0.6404 - val_loss: 2.0677\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7599 - loss: 1.5268 - val_accuracy: 0.5908 - val_loss: 2.2396\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7680 - loss: 1.4843 - val_accuracy: 0.7142 - val_loss: 1.6923\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7738 - loss: 1.4719 - val_accuracy: 0.5983 - val_loss: 2.3867\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7740 - loss: 1.4648 - val_accuracy: 0.7212 - val_loss: 1.6043\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7732 - loss: 1.4805 - val_accuracy: 0.7167 - val_loss: 1.6355\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7717 - loss: 1.4852 - val_accuracy: 0.6696 - val_loss: 1.7956\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7829 - loss: 1.4215 - val_accuracy: 0.7800 - val_loss: 1.4458\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7721 - loss: 1.4668 - val_accuracy: 0.7271 - val_loss: 1.6189\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7743 - loss: 1.4513 - val_accuracy: 0.7321 - val_loss: 1.5843\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7677 - loss: 1.4756 - val_accuracy: 0.7171 - val_loss: 1.6269\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7809 - loss: 1.4598 - val_accuracy: 0.6562 - val_loss: 2.0940\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7842 - loss: 1.4063 - val_accuracy: 0.6717 - val_loss: 1.8980\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7777 - loss: 1.4537 - val_accuracy: 0.7362 - val_loss: 1.5345\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7894 - loss: 1.4153 - val_accuracy: 0.7271 - val_loss: 1.6222\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7828 - loss: 1.4225 - val_accuracy: 0.7371 - val_loss: 1.5330\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7898 - loss: 1.4019 - val_accuracy: 0.7525 - val_loss: 1.4887\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8004 - loss: 1.3660 - val_accuracy: 0.6546 - val_loss: 1.9442\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7893 - loss: 1.4254 - val_accuracy: 0.7067 - val_loss: 1.7342\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7966 - loss: 1.3969 - val_accuracy: 0.6600 - val_loss: 1.9174\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.7930 - loss: 1.3666 - val_accuracy: 0.8071 - val_loss: 1.3614\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7813 - loss: 1.4182 - val_accuracy: 0.6567 - val_loss: 2.0638\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8009 - loss: 1.3717 - val_accuracy: 0.5721 - val_loss: 2.3762\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7900 - loss: 1.3827 - val_accuracy: 0.7092 - val_loss: 1.8573\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7711 - loss: 1.4360 - val_accuracy: 0.7733 - val_loss: 1.4818\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7939 - loss: 1.3963 - val_accuracy: 0.7296 - val_loss: 1.5533\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7939 - loss: 1.3791 - val_accuracy: 0.8008 - val_loss: 1.3617\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7979 - loss: 1.3313 - val_accuracy: 0.7625 - val_loss: 1.4524\n",
            "Model Validation Accuracy: 0.8071\n",
            "Best Validation Accuracy: 0.8071\n",
            "Best Model: <Sequential name=sequential_1, built=True>\n",
            "Best Hyperparameters: Layers=9, Neurons=352, L2=0.001\n",
            "\n",
            "Testing Model with 12 layers, 896 neurons, L2=0.005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 875ms/step - accuracy: 0.1082 - loss: 40.3873 - val_accuracy: 0.0867 - val_loss: 11.5412\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 123ms/step - accuracy: 0.1368 - loss: 8.7533 - val_accuracy: 0.0996 - val_loss: 4.5834\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.2114 - loss: 3.8487 - val_accuracy: 0.0867 - val_loss: 3.4543\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 88ms/step - accuracy: 0.2538 - loss: 3.2229 - val_accuracy: 0.1587 - val_loss: 3.4407\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 87ms/step - accuracy: 0.2604 - loss: 3.1921 - val_accuracy: 0.0867 - val_loss: 3.7867\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.2736 - loss: 3.2198 - val_accuracy: 0.1325 - val_loss: 3.6170\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3144 - loss: 3.2428 - val_accuracy: 0.1138 - val_loss: 3.7524\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 124ms/step - accuracy: 0.3060 - loss: 3.2530 - val_accuracy: 0.1592 - val_loss: 3.4902\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 129ms/step - accuracy: 0.3007 - loss: 3.3681 - val_accuracy: 0.2121 - val_loss: 3.6911\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3117 - loss: 3.3621 - val_accuracy: 0.1096 - val_loss: 3.6373\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3166 - loss: 3.2479 - val_accuracy: 0.2100 - val_loss: 3.5369\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 118ms/step - accuracy: 0.3080 - loss: 3.2902 - val_accuracy: 0.2621 - val_loss: 3.7670\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.3018 - loss: 3.4067 - val_accuracy: 0.1650 - val_loss: 3.5963\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3067 - loss: 3.2713 - val_accuracy: 0.1292 - val_loss: 3.5036\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.3236 - loss: 3.1784 - val_accuracy: 0.1446 - val_loss: 3.7404\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.2998 - loss: 3.3712 - val_accuracy: 0.1400 - val_loss: 5.3077\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.3175 - loss: 3.2445 - val_accuracy: 0.2833 - val_loss: 3.5091\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.3130 - loss: 3.3464 - val_accuracy: 0.1937 - val_loss: 3.6322\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.3161 - loss: 3.4088 - val_accuracy: 0.1358 - val_loss: 3.5627\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.3009 - loss: 3.3025 - val_accuracy: 0.1004 - val_loss: 3.8463\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.3081 - loss: 3.3640 - val_accuracy: 0.1842 - val_loss: 3.6242\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.3215 - loss: 3.4731 - val_accuracy: 0.1088 - val_loss: 3.9438\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 130ms/step - accuracy: 0.2897 - loss: 3.3732 - val_accuracy: 0.3075 - val_loss: 3.4348\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.3201 - loss: 3.3332 - val_accuracy: 0.3304 - val_loss: 3.4208\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3093 - loss: 3.4292 - val_accuracy: 0.2046 - val_loss: 3.9893\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3114 - loss: 3.3728 - val_accuracy: 0.1750 - val_loss: 3.6340\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3157 - loss: 3.4633 - val_accuracy: 0.1704 - val_loss: 3.6937\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.3075 - loss: 3.3810 - val_accuracy: 0.1250 - val_loss: 3.6564\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3011 - loss: 3.3920 - val_accuracy: 0.2571 - val_loss: 3.2855\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3234 - loss: 3.2983 - val_accuracy: 0.1404 - val_loss: 3.8866\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2945 - loss: 3.4802 - val_accuracy: 0.1621 - val_loss: 3.5224\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3088 - loss: 3.4372 - val_accuracy: 0.2279 - val_loss: 3.9474\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3066 - loss: 3.4800 - val_accuracy: 0.2071 - val_loss: 3.7954\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3322 - loss: 3.5218 - val_accuracy: 0.2517 - val_loss: 3.5346\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.2986 - loss: 3.4100 - val_accuracy: 0.1546 - val_loss: 3.6961\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3119 - loss: 3.4277 - val_accuracy: 0.1583 - val_loss: 4.4998\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3124 - loss: 3.4139 - val_accuracy: 0.2350 - val_loss: 3.7976\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2969 - loss: 3.4303 - val_accuracy: 0.2788 - val_loss: 3.8413\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3059 - loss: 3.5586 - val_accuracy: 0.1246 - val_loss: 4.4009\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3119 - loss: 3.5179 - val_accuracy: 0.1379 - val_loss: 5.4273\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3271 - loss: 3.4240 - val_accuracy: 0.2271 - val_loss: 3.8267\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3300 - loss: 3.4406 - val_accuracy: 0.2387 - val_loss: 3.3829\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2991 - loss: 3.2947 - val_accuracy: 0.1896 - val_loss: 4.0144\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3132 - loss: 3.4541 - val_accuracy: 0.1321 - val_loss: 4.0062\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3053 - loss: 3.4851 - val_accuracy: 0.1950 - val_loss: 3.8853\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3132 - loss: 3.4774 - val_accuracy: 0.2517 - val_loss: 3.7616\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3154 - loss: 3.4940 - val_accuracy: 0.1558 - val_loss: 3.6493\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3123 - loss: 3.3154 - val_accuracy: 0.1533 - val_loss: 3.8323\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3028 - loss: 3.3771 - val_accuracy: 0.2663 - val_loss: 3.7259\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3081 - loss: 3.3366 - val_accuracy: 0.1967 - val_loss: 3.8996\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3203 - loss: 3.4171 - val_accuracy: 0.1008 - val_loss: 3.8762\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3036 - loss: 3.3722 - val_accuracy: 0.1504 - val_loss: 4.2388\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3123 - loss: 3.4512 - val_accuracy: 0.2517 - val_loss: 3.4931\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3329 - loss: 3.2755 - val_accuracy: 0.1842 - val_loss: 3.7838\n",
            "Model Validation Accuracy: 0.3304\n",
            "\n",
            "Testing Model with 8 layers, 448 neurons, L2=0.0025\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 542ms/step - accuracy: 0.2023 - loss: 9.9253 - val_accuracy: 0.0975 - val_loss: 7.6809\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.3679 - loss: 5.1922 - val_accuracy: 0.1025 - val_loss: 4.3269\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.4360 - loss: 3.0241 - val_accuracy: 0.1042 - val_loss: 3.4849\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4702 - loss: 2.4417 - val_accuracy: 0.0896 - val_loss: 3.8878\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.4941 - loss: 2.2810 - val_accuracy: 0.2233 - val_loss: 3.0013\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5191 - loss: 2.1847 - val_accuracy: 0.1979 - val_loss: 3.4865\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.5328 - loss: 2.2106 - val_accuracy: 0.5358 - val_loss: 2.2442\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5304 - loss: 2.2561 - val_accuracy: 0.5075 - val_loss: 2.2938\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5428 - loss: 2.2118 - val_accuracy: 0.4754 - val_loss: 2.3456\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5411 - loss: 2.2453 - val_accuracy: 0.4050 - val_loss: 2.8007\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5443 - loss: 2.2997 - val_accuracy: 0.3667 - val_loss: 2.9548\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5520 - loss: 2.3047 - val_accuracy: 0.3654 - val_loss: 3.0844\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5372 - loss: 2.3539 - val_accuracy: 0.3446 - val_loss: 2.8193\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5613 - loss: 2.2670 - val_accuracy: 0.4387 - val_loss: 2.6502\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.5851 - loss: 2.1984 - val_accuracy: 0.3450 - val_loss: 3.0353\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5585 - loss: 2.3032 - val_accuracy: 0.2463 - val_loss: 4.3892\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.5711 - loss: 2.2862 - val_accuracy: 0.5542 - val_loss: 2.3260\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5738 - loss: 2.2847 - val_accuracy: 0.4175 - val_loss: 2.8952\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5691 - loss: 2.2967 - val_accuracy: 0.4062 - val_loss: 2.7264\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5919 - loss: 2.1844 - val_accuracy: 0.4917 - val_loss: 2.4603\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6024 - loss: 2.2377 - val_accuracy: 0.4729 - val_loss: 2.6531\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5855 - loss: 2.2224 - val_accuracy: 0.4867 - val_loss: 2.7367\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5882 - loss: 2.2025 - val_accuracy: 0.4167 - val_loss: 2.7653\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5976 - loss: 2.2034 - val_accuracy: 0.2612 - val_loss: 5.8474\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6062 - loss: 2.1817 - val_accuracy: 0.4846 - val_loss: 2.6661\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5878 - loss: 2.2720 - val_accuracy: 0.5000 - val_loss: 2.5441\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5893 - loss: 2.1945 - val_accuracy: 0.5337 - val_loss: 2.3963\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6056 - loss: 2.1889 - val_accuracy: 0.4412 - val_loss: 2.7390\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.5980 - loss: 2.1446 - val_accuracy: 0.5621 - val_loss: 2.2977\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5873 - loss: 2.1918 - val_accuracy: 0.4933 - val_loss: 2.5033\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.5979 - loss: 2.1870 - val_accuracy: 0.5775 - val_loss: 2.2409\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6246 - loss: 2.1402 - val_accuracy: 0.5587 - val_loss: 2.3989\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6060 - loss: 2.1962 - val_accuracy: 0.3187 - val_loss: 3.3505\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6049 - loss: 2.1394 - val_accuracy: 0.3171 - val_loss: 4.8070\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6180 - loss: 2.1776 - val_accuracy: 0.4629 - val_loss: 2.5041\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6151 - loss: 2.1460 - val_accuracy: 0.2879 - val_loss: 4.2208\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6243 - loss: 2.1458 - val_accuracy: 0.3558 - val_loss: 3.5600\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6414 - loss: 2.0923 - val_accuracy: 0.3104 - val_loss: 4.3613\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6352 - loss: 2.0747 - val_accuracy: 0.4958 - val_loss: 2.5869\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6448 - loss: 2.0515 - val_accuracy: 0.4883 - val_loss: 2.5131\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6334 - loss: 2.0875 - val_accuracy: 0.4621 - val_loss: 2.8456\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6417 - loss: 2.0619 - val_accuracy: 0.4125 - val_loss: 2.7048\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6293 - loss: 2.1239 - val_accuracy: 0.5300 - val_loss: 2.5954\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6478 - loss: 2.0313 - val_accuracy: 0.4692 - val_loss: 2.6473\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.6517 - loss: 2.0170 - val_accuracy: 0.6317 - val_loss: 2.0819\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6504 - loss: 1.9717 - val_accuracy: 0.4421 - val_loss: 3.0093\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6404 - loss: 1.9846 - val_accuracy: 0.5821 - val_loss: 2.1926\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6463 - loss: 2.0474 - val_accuracy: 0.5475 - val_loss: 2.2839\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6557 - loss: 1.9856 - val_accuracy: 0.3404 - val_loss: 3.3295\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6739 - loss: 1.9254 - val_accuracy: 0.5683 - val_loss: 2.3146\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6524 - loss: 1.9964 - val_accuracy: 0.4896 - val_loss: 2.4716\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6699 - loss: 1.8823 - val_accuracy: 0.4650 - val_loss: 2.6050\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6727 - loss: 1.9075 - val_accuracy: 0.3783 - val_loss: 3.5218\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6838 - loss: 1.8462 - val_accuracy: 0.4258 - val_loss: 2.5294\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6437 - loss: 1.8978 - val_accuracy: 0.5754 - val_loss: 2.1411\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.6612 - loss: 1.8763 - val_accuracy: 0.6400 - val_loss: 2.0027\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6798 - loss: 1.8127 - val_accuracy: 0.3108 - val_loss: 6.3994\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6726 - loss: 1.8720 - val_accuracy: 0.5946 - val_loss: 2.1068\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6699 - loss: 1.8587 - val_accuracy: 0.3646 - val_loss: 3.7641\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6725 - loss: 1.8959 - val_accuracy: 0.3562 - val_loss: 4.5406\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6466 - loss: 1.8656 - val_accuracy: 0.4792 - val_loss: 2.6340\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6902 - loss: 1.7757 - val_accuracy: 0.5767 - val_loss: 2.1320\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6939 - loss: 1.7551 - val_accuracy: 0.5000 - val_loss: 2.9239\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6964 - loss: 1.7106 - val_accuracy: 0.4225 - val_loss: 3.4991\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6957 - loss: 1.7911 - val_accuracy: 0.5788 - val_loss: 2.2749\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6935 - loss: 1.7295 - val_accuracy: 0.5783 - val_loss: 2.2802\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6952 - loss: 1.7315 - val_accuracy: 0.5008 - val_loss: 2.5040\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7064 - loss: 1.7187 - val_accuracy: 0.4508 - val_loss: 2.9446\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6942 - loss: 1.7461 - val_accuracy: 0.5583 - val_loss: 2.2337\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6933 - loss: 1.7536 - val_accuracy: 0.6279 - val_loss: 1.8300\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7041 - loss: 1.6915 - val_accuracy: 0.4879 - val_loss: 3.0311\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7125 - loss: 1.6894 - val_accuracy: 0.5092 - val_loss: 2.7583\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.6907 - loss: 1.7693 - val_accuracy: 0.7175 - val_loss: 1.7153\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7097 - loss: 1.6962 - val_accuracy: 0.5208 - val_loss: 2.2412\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7092 - loss: 1.7007 - val_accuracy: 0.5846 - val_loss: 2.0193\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6954 - loss: 1.7103 - val_accuracy: 0.4746 - val_loss: 2.9052\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7308 - loss: 1.6206 - val_accuracy: 0.6825 - val_loss: 1.8037\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7229 - loss: 1.6643 - val_accuracy: 0.5250 - val_loss: 2.4114\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7249 - loss: 1.6144 - val_accuracy: 0.7167 - val_loss: 1.6372\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7145 - loss: 1.6468 - val_accuracy: 0.5225 - val_loss: 2.4802\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7297 - loss: 1.6280 - val_accuracy: 0.5138 - val_loss: 2.5941\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7248 - loss: 1.6024 - val_accuracy: 0.5879 - val_loss: 2.5886\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7147 - loss: 1.6101 - val_accuracy: 0.4996 - val_loss: 2.7865\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7202 - loss: 1.5941 - val_accuracy: 0.4888 - val_loss: 2.9405\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7347 - loss: 1.5936 - val_accuracy: 0.6429 - val_loss: 1.9366\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7337 - loss: 1.5830 - val_accuracy: 0.6908 - val_loss: 1.7107\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7240 - loss: 1.5912 - val_accuracy: 0.5675 - val_loss: 2.1277\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7281 - loss: 1.5685 - val_accuracy: 0.5354 - val_loss: 2.4387\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7236 - loss: 1.6008 - val_accuracy: 0.6254 - val_loss: 2.0315\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7379 - loss: 1.5463 - val_accuracy: 0.3683 - val_loss: 5.4311\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7296 - loss: 1.5787 - val_accuracy: 0.6162 - val_loss: 1.8595\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7283 - loss: 1.5663 - val_accuracy: 0.4983 - val_loss: 2.4684\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7260 - loss: 1.5879 - val_accuracy: 0.7100 - val_loss: 1.6452\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.7480 - loss: 1.5208 - val_accuracy: 0.7442 - val_loss: 1.5287\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7379 - loss: 1.5085 - val_accuracy: 0.6783 - val_loss: 1.7219\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7470 - loss: 1.5077 - val_accuracy: 0.7088 - val_loss: 1.7249\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7399 - loss: 1.5085 - val_accuracy: 0.5921 - val_loss: 2.1119\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7351 - loss: 1.5276 - val_accuracy: 0.5821 - val_loss: 2.5975\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7312 - loss: 1.5567 - val_accuracy: 0.6696 - val_loss: 1.6762\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7360 - loss: 1.4989 - val_accuracy: 0.6500 - val_loss: 1.8101\n",
            "Model Validation Accuracy: 0.7442\n",
            "\n",
            "Testing Model with 6 layers, 832 neurons, L2=0.0015\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 465ms/step - accuracy: 0.1661 - loss: 8.4197 - val_accuracy: 0.1021 - val_loss: 4.3256\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.3080 - loss: 3.3633 - val_accuracy: 0.1004 - val_loss: 3.5473\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.3494 - loss: 2.3940 - val_accuracy: 0.1004 - val_loss: 3.4154\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.4012 - loss: 2.2667 - val_accuracy: 0.0917 - val_loss: 3.0889\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.4241 - loss: 2.2686 - val_accuracy: 0.1408 - val_loss: 3.0892\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 78ms/step - accuracy: 0.4371 - loss: 2.1722 - val_accuracy: 0.1462 - val_loss: 2.9773\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - accuracy: 0.4571 - loss: 2.2076 - val_accuracy: 0.4058 - val_loss: 2.4339\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.4639 - loss: 2.2550 - val_accuracy: 0.3029 - val_loss: 2.7969\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.4922 - loss: 2.1836 - val_accuracy: 0.3900 - val_loss: 2.6738\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.4845 - loss: 2.1885 - val_accuracy: 0.3233 - val_loss: 3.0355\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.4912 - loss: 2.2321 - val_accuracy: 0.3058 - val_loss: 3.0006\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.4900 - loss: 2.2302 - val_accuracy: 0.4013 - val_loss: 2.5170\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5022 - loss: 2.2096 - val_accuracy: 0.2550 - val_loss: 3.7429\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5192 - loss: 2.1940 - val_accuracy: 0.3896 - val_loss: 2.5297\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5135 - loss: 2.1645 - val_accuracy: 0.2812 - val_loss: 3.6709\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5322 - loss: 2.1507 - val_accuracy: 0.3025 - val_loss: 3.2311\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5245 - loss: 2.2017 - val_accuracy: 0.2467 - val_loss: 4.0394\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5197 - loss: 2.1788 - val_accuracy: 0.2792 - val_loss: 4.0167\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 78ms/step - accuracy: 0.5414 - loss: 2.1303 - val_accuracy: 0.4329 - val_loss: 2.5175\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5491 - loss: 2.1437 - val_accuracy: 0.4137 - val_loss: 2.6675\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5276 - loss: 2.1704 - val_accuracy: 0.3187 - val_loss: 3.2716\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5290 - loss: 2.1820 - val_accuracy: 0.2867 - val_loss: 4.5214\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5438 - loss: 2.1601 - val_accuracy: 0.4175 - val_loss: 2.6806\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5498 - loss: 2.1339 - val_accuracy: 0.1767 - val_loss: 4.5660\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5531 - loss: 2.0880 - val_accuracy: 0.3242 - val_loss: 2.9151\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5595 - loss: 2.0724 - val_accuracy: 0.3325 - val_loss: 4.7399\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5794 - loss: 2.0734 - val_accuracy: 0.4033 - val_loss: 2.6133\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5634 - loss: 2.0781 - val_accuracy: 0.1925 - val_loss: 5.4659\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5861 - loss: 2.0124 - val_accuracy: 0.2533 - val_loss: 3.9984\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5794 - loss: 2.0701 - val_accuracy: 0.1637 - val_loss: 7.4414\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5905 - loss: 1.9803 - val_accuracy: 0.4013 - val_loss: 2.6780\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.5973 - loss: 1.9570 - val_accuracy: 0.2083 - val_loss: 5.0040\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 77ms/step - accuracy: 0.5917 - loss: 1.9642 - val_accuracy: 0.4604 - val_loss: 2.6556\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6078 - loss: 1.8785 - val_accuracy: 0.3738 - val_loss: 2.6759\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - accuracy: 0.6134 - loss: 1.8858 - val_accuracy: 0.5021 - val_loss: 2.2850\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6140 - loss: 1.8670 - val_accuracy: 0.2429 - val_loss: 3.2615\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.6409 - loss: 1.7962 - val_accuracy: 0.3113 - val_loss: 4.8416\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 78ms/step - accuracy: 0.6232 - loss: 1.7844 - val_accuracy: 0.5163 - val_loss: 2.1144\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.6220 - loss: 1.8418 - val_accuracy: 0.5888 - val_loss: 1.9535\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.6309 - loss: 1.8052 - val_accuracy: 0.3004 - val_loss: 3.1717\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6619 - loss: 1.7405 - val_accuracy: 0.3254 - val_loss: 4.5106\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.6545 - loss: 1.6889 - val_accuracy: 0.3371 - val_loss: 3.4381\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.6640 - loss: 1.7094 - val_accuracy: 0.2754 - val_loss: 4.4726\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6500 - loss: 1.7315 - val_accuracy: 0.5696 - val_loss: 2.0238\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.6588 - loss: 1.6793 - val_accuracy: 0.3333 - val_loss: 4.5236\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6743 - loss: 1.6545 - val_accuracy: 0.4225 - val_loss: 2.5180\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6772 - loss: 1.6273 - val_accuracy: 0.5275 - val_loss: 1.9812\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 78ms/step - accuracy: 0.6846 - loss: 1.6215 - val_accuracy: 0.6350 - val_loss: 1.7788\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6945 - loss: 1.5607 - val_accuracy: 0.2862 - val_loss: 3.9384\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6720 - loss: 1.5940 - val_accuracy: 0.4458 - val_loss: 3.5699\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.6878 - loss: 1.5819 - val_accuracy: 0.4504 - val_loss: 2.6085\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.6962 - loss: 1.5543 - val_accuracy: 0.4279 - val_loss: 2.2916\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.6984 - loss: 1.5316 - val_accuracy: 0.3212 - val_loss: 4.1759\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.6911 - loss: 1.5745 - val_accuracy: 0.4867 - val_loss: 2.1628\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.6994 - loss: 1.5261 - val_accuracy: 0.3667 - val_loss: 2.8697\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7019 - loss: 1.5174 - val_accuracy: 0.5800 - val_loss: 1.7721\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6920 - loss: 1.5068 - val_accuracy: 0.4496 - val_loss: 2.8253\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6957 - loss: 1.5182 - val_accuracy: 0.3350 - val_loss: 4.3879\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.6903 - loss: 1.5235 - val_accuracy: 0.5633 - val_loss: 2.0710\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7254 - loss: 1.4754 - val_accuracy: 0.5529 - val_loss: 2.1210\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.7012 - loss: 1.5360 - val_accuracy: 0.5842 - val_loss: 1.9827\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7253 - loss: 1.4557 - val_accuracy: 0.5025 - val_loss: 2.1098\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.7136 - loss: 1.4552 - val_accuracy: 0.4338 - val_loss: 3.8088\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7160 - loss: 1.4566 - val_accuracy: 0.3442 - val_loss: 3.0377\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step - accuracy: 0.7230 - loss: 1.4182 - val_accuracy: 0.6463 - val_loss: 1.5960\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7502 - loss: 1.3627 - val_accuracy: 0.5867 - val_loss: 2.2304\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7277 - loss: 1.4240 - val_accuracy: 0.6083 - val_loss: 1.8403\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7298 - loss: 1.3997 - val_accuracy: 0.3846 - val_loss: 3.4369\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7420 - loss: 1.3737 - val_accuracy: 0.5242 - val_loss: 2.3725\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7248 - loss: 1.4337 - val_accuracy: 0.2783 - val_loss: 6.7463\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7355 - loss: 1.3757 - val_accuracy: 0.5271 - val_loss: 2.0048\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7422 - loss: 1.3748 - val_accuracy: 0.4913 - val_loss: 2.6545\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7338 - loss: 1.3966 - val_accuracy: 0.4833 - val_loss: 2.6890\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7441 - loss: 1.3471 - val_accuracy: 0.6304 - val_loss: 1.8054\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7501 - loss: 1.3709 - val_accuracy: 0.4850 - val_loss: 2.8902\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 77ms/step - accuracy: 0.7519 - loss: 1.3387 - val_accuracy: 0.7096 - val_loss: 1.5294\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7660 - loss: 1.3288 - val_accuracy: 0.5387 - val_loss: 2.4596\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.7421 - loss: 1.3671 - val_accuracy: 0.5779 - val_loss: 1.9494\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.7638 - loss: 1.3392 - val_accuracy: 0.4767 - val_loss: 3.0802\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.7586 - loss: 1.3440 - val_accuracy: 0.5700 - val_loss: 2.1587\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.7628 - loss: 1.3354 - val_accuracy: 0.3325 - val_loss: 4.7905\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7459 - loss: 1.3531 - val_accuracy: 0.3796 - val_loss: 4.6620\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7515 - loss: 1.3382 - val_accuracy: 0.5892 - val_loss: 1.8758\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7772 - loss: 1.2709 - val_accuracy: 0.5213 - val_loss: 2.2705\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.7583 - loss: 1.3207 - val_accuracy: 0.5933 - val_loss: 2.0848\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7628 - loss: 1.2996 - val_accuracy: 0.4104 - val_loss: 3.2910\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7645 - loss: 1.3248 - val_accuracy: 0.6783 - val_loss: 1.5781\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7529 - loss: 1.3380 - val_accuracy: 0.3942 - val_loss: 2.9724\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7720 - loss: 1.2574 - val_accuracy: 0.5113 - val_loss: 2.5832\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7674 - loss: 1.3078 - val_accuracy: 0.5721 - val_loss: 2.3318\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7917 - loss: 1.2171 - val_accuracy: 0.4392 - val_loss: 2.6667\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7755 - loss: 1.2416 - val_accuracy: 0.6012 - val_loss: 2.2543\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.7636 - loss: 1.2608 - val_accuracy: 0.6508 - val_loss: 1.8462\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7777 - loss: 1.2379 - val_accuracy: 0.4833 - val_loss: 2.7922\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7860 - loss: 1.2510 - val_accuracy: 0.6279 - val_loss: 2.1120\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7898 - loss: 1.2228 - val_accuracy: 0.6538 - val_loss: 1.9607\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7842 - loss: 1.2327 - val_accuracy: 0.5779 - val_loss: 2.3218\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7779 - loss: 1.2508 - val_accuracy: 0.7046 - val_loss: 1.6196\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7825 - loss: 1.2379 - val_accuracy: 0.6742 - val_loss: 1.6886\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.7894 - loss: 1.2013 - val_accuracy: 0.6700 - val_loss: 1.6038\n",
            "Model Validation Accuracy: 0.7096\n",
            "\n",
            "Testing Model with 9 layers, 320 neurons, L2=0.001\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 600ms/step - accuracy: 0.1748 - loss: 4.8632 - val_accuracy: 0.1083 - val_loss: 5.5189\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.3806 - loss: 3.7248 - val_accuracy: 0.1058 - val_loss: 4.8770\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4727 - loss: 2.9059 - val_accuracy: 0.2567 - val_loss: 3.1825\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5253 - loss: 2.4167 - val_accuracy: 0.3004 - val_loss: 2.8234\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.5704 - loss: 2.1278 - val_accuracy: 0.5325 - val_loss: 2.2960\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5707 - loss: 2.0359 - val_accuracy: 0.3646 - val_loss: 2.5780\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6253 - loss: 1.8597 - val_accuracy: 0.4550 - val_loss: 2.3127\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6296 - loss: 1.8252 - val_accuracy: 0.4808 - val_loss: 2.3802\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6360 - loss: 1.8065 - val_accuracy: 0.4733 - val_loss: 2.4874\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.6406 - loss: 1.7993 - val_accuracy: 0.5954 - val_loss: 1.8751\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6391 - loss: 1.8116 - val_accuracy: 0.5150 - val_loss: 2.3511\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6476 - loss: 1.7881 - val_accuracy: 0.5517 - val_loss: 2.1712\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6675 - loss: 1.7781 - val_accuracy: 0.5479 - val_loss: 2.2969\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6732 - loss: 1.7673 - val_accuracy: 0.4442 - val_loss: 3.2030\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6686 - loss: 1.7865 - val_accuracy: 0.5913 - val_loss: 2.0239\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6803 - loss: 1.7558 - val_accuracy: 0.5133 - val_loss: 2.4712\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6806 - loss: 1.7623 - val_accuracy: 0.5562 - val_loss: 2.1733\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.6772 - loss: 1.7644 - val_accuracy: 0.6333 - val_loss: 1.8930\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6845 - loss: 1.7329 - val_accuracy: 0.6225 - val_loss: 1.9480\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6944 - loss: 1.7459 - val_accuracy: 0.5400 - val_loss: 2.3520\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6883 - loss: 1.7250 - val_accuracy: 0.5642 - val_loss: 2.1219\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7084 - loss: 1.6853 - val_accuracy: 0.5683 - val_loss: 2.0534\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7072 - loss: 1.7216 - val_accuracy: 0.6367 - val_loss: 1.9079\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6991 - loss: 1.7355 - val_accuracy: 0.5908 - val_loss: 1.9706\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7136 - loss: 1.6889 - val_accuracy: 0.6496 - val_loss: 1.8165\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.6990 - loss: 1.7354 - val_accuracy: 0.6800 - val_loss: 1.7824\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6903 - loss: 1.7260 - val_accuracy: 0.4750 - val_loss: 2.9920\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6951 - loss: 1.7178 - val_accuracy: 0.6008 - val_loss: 1.9999\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7279 - loss: 1.6780 - val_accuracy: 0.6787 - val_loss: 1.7754\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7244 - loss: 1.6510 - val_accuracy: 0.6675 - val_loss: 1.8121\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7116 - loss: 1.7070 - val_accuracy: 0.6058 - val_loss: 1.9824\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7083 - loss: 1.7064 - val_accuracy: 0.6662 - val_loss: 1.9217\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7180 - loss: 1.7136 - val_accuracy: 0.6779 - val_loss: 1.8266\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7549 - loss: 1.6404 - val_accuracy: 0.6267 - val_loss: 1.8982\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7315 - loss: 1.6497 - val_accuracy: 0.6542 - val_loss: 1.8747\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7141 - loss: 1.7024 - val_accuracy: 0.5950 - val_loss: 2.1408\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7263 - loss: 1.6897 - val_accuracy: 0.5479 - val_loss: 2.5225\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7393 - loss: 1.6238 - val_accuracy: 0.5121 - val_loss: 2.6582\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7359 - loss: 1.6163 - val_accuracy: 0.6225 - val_loss: 2.0709\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7286 - loss: 1.6465 - val_accuracy: 0.6329 - val_loss: 1.9138\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7215 - loss: 1.6573 - val_accuracy: 0.6729 - val_loss: 1.7287\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.7489 - loss: 1.5852 - val_accuracy: 0.6867 - val_loss: 1.7547\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7453 - loss: 1.5973 - val_accuracy: 0.7188 - val_loss: 1.6878\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7405 - loss: 1.6005 - val_accuracy: 0.7071 - val_loss: 1.7456\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7403 - loss: 1.6142 - val_accuracy: 0.7150 - val_loss: 1.7594\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7395 - loss: 1.6082 - val_accuracy: 0.7063 - val_loss: 1.7319\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7416 - loss: 1.6136 - val_accuracy: 0.5838 - val_loss: 2.0934\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7416 - loss: 1.6264 - val_accuracy: 0.6754 - val_loss: 1.8701\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7375 - loss: 1.6357 - val_accuracy: 0.7296 - val_loss: 1.6558\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.7478 - loss: 1.5570 - val_accuracy: 0.7408 - val_loss: 1.6191\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7523 - loss: 1.5644 - val_accuracy: 0.5679 - val_loss: 2.3909\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7496 - loss: 1.5868 - val_accuracy: 0.7038 - val_loss: 1.7849\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7485 - loss: 1.5981 - val_accuracy: 0.7212 - val_loss: 1.6224\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7650 - loss: 1.5545 - val_accuracy: 0.6504 - val_loss: 1.8107\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7357 - loss: 1.6215 - val_accuracy: 0.6525 - val_loss: 2.0738\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7477 - loss: 1.6048 - val_accuracy: 0.7396 - val_loss: 1.5830\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7579 - loss: 1.5274 - val_accuracy: 0.6683 - val_loss: 1.7849\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7419 - loss: 1.5829 - val_accuracy: 0.6854 - val_loss: 1.7316\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7497 - loss: 1.5575 - val_accuracy: 0.7075 - val_loss: 1.7831\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7657 - loss: 1.5330 - val_accuracy: 0.6546 - val_loss: 2.0068\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7634 - loss: 1.5139 - val_accuracy: 0.6825 - val_loss: 1.9111\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7703 - loss: 1.4903 - val_accuracy: 0.7308 - val_loss: 1.6475\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.7723 - loss: 1.5049 - val_accuracy: 0.7600 - val_loss: 1.5239\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7636 - loss: 1.5181 - val_accuracy: 0.6104 - val_loss: 2.0920\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7625 - loss: 1.5350 - val_accuracy: 0.6587 - val_loss: 2.1611\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7656 - loss: 1.5155 - val_accuracy: 0.6733 - val_loss: 1.8295\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7654 - loss: 1.5127 - val_accuracy: 0.6958 - val_loss: 1.7752\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7698 - loss: 1.5131 - val_accuracy: 0.7121 - val_loss: 1.5719\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7762 - loss: 1.4492 - val_accuracy: 0.6708 - val_loss: 1.8046\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7683 - loss: 1.4878 - val_accuracy: 0.7563 - val_loss: 1.5249\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7728 - loss: 1.4569 - val_accuracy: 0.7387 - val_loss: 1.5742\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7687 - loss: 1.4868 - val_accuracy: 0.6746 - val_loss: 1.7948\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7643 - loss: 1.4902 - val_accuracy: 0.6621 - val_loss: 2.0848\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.7576 - loss: 1.5187 - val_accuracy: 0.7850 - val_loss: 1.4513\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7766 - loss: 1.4655 - val_accuracy: 0.7421 - val_loss: 1.5567\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7831 - loss: 1.4612 - val_accuracy: 0.7713 - val_loss: 1.4189\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7776 - loss: 1.4523 - val_accuracy: 0.6550 - val_loss: 1.8900\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7847 - loss: 1.4516 - val_accuracy: 0.7163 - val_loss: 1.6933\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7830 - loss: 1.4146 - val_accuracy: 0.7467 - val_loss: 1.6294\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7867 - loss: 1.4457 - val_accuracy: 0.6946 - val_loss: 1.8596\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7790 - loss: 1.4455 - val_accuracy: 0.7392 - val_loss: 1.5389\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7875 - loss: 1.4248 - val_accuracy: 0.7233 - val_loss: 1.6268\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7867 - loss: 1.4304 - val_accuracy: 0.7146 - val_loss: 1.6392\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7936 - loss: 1.4059 - val_accuracy: 0.8008 - val_loss: 1.4111\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8011 - loss: 1.3813 - val_accuracy: 0.6783 - val_loss: 1.8341\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7826 - loss: 1.4379 - val_accuracy: 0.7604 - val_loss: 1.5130\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7836 - loss: 1.3984 - val_accuracy: 0.7192 - val_loss: 1.6309\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7837 - loss: 1.4145 - val_accuracy: 0.6946 - val_loss: 1.7691\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7905 - loss: 1.3820 - val_accuracy: 0.7733 - val_loss: 1.4471\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7832 - loss: 1.4063 - val_accuracy: 0.6025 - val_loss: 2.5870\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7777 - loss: 1.4247 - val_accuracy: 0.6871 - val_loss: 1.8994\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7859 - loss: 1.4023 - val_accuracy: 0.6517 - val_loss: 2.0706\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7963 - loss: 1.3718 - val_accuracy: 0.7108 - val_loss: 1.6571\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8004 - loss: 1.3515 - val_accuracy: 0.7567 - val_loss: 1.5135\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7813 - loss: 1.4251 - val_accuracy: 0.6683 - val_loss: 1.9373\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7991 - loss: 1.3211 - val_accuracy: 0.6733 - val_loss: 1.9480\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7880 - loss: 1.3812 - val_accuracy: 0.7025 - val_loss: 1.6127\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8031 - loss: 1.3580 - val_accuracy: 0.6696 - val_loss: 1.8819\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7886 - loss: 1.4077 - val_accuracy: 0.6612 - val_loss: 1.7799\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8029 - loss: 1.3532 - val_accuracy: 0.7937 - val_loss: 1.4174\n",
            "Model Validation Accuracy: 0.8008\n",
            "\n",
            "Testing Model with 5 layers, 256 neurons, L2=0.01\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 423ms/step - accuracy: 0.1889 - loss: 8.4181 - val_accuracy: 0.1025 - val_loss: 2.9996\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.3068 - loss: 2.4296 - val_accuracy: 0.1717 - val_loss: 2.8065\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3514 - loss: 2.2637 - val_accuracy: 0.1683 - val_loss: 3.0748\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3807 - loss: 2.2701 - val_accuracy: 0.0867 - val_loss: 3.4342\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3950 - loss: 2.1976 - val_accuracy: 0.1317 - val_loss: 2.8454\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4195 - loss: 2.1277 - val_accuracy: 0.2321 - val_loss: 2.8766\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.4357 - loss: 2.1267 - val_accuracy: 0.2688 - val_loss: 2.5939\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4306 - loss: 2.0992 - val_accuracy: 0.3575 - val_loss: 2.3066\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4358 - loss: 2.0741 - val_accuracy: 0.2371 - val_loss: 3.5981\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4570 - loss: 2.0553 - val_accuracy: 0.3133 - val_loss: 2.6260\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4618 - loss: 2.0180 - val_accuracy: 0.3088 - val_loss: 2.4516\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4538 - loss: 2.0532 - val_accuracy: 0.3246 - val_loss: 2.4718\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4579 - loss: 2.0172 - val_accuracy: 0.3192 - val_loss: 2.3200\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4717 - loss: 1.9989 - val_accuracy: 0.4121 - val_loss: 2.2115\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4909 - loss: 1.9776 - val_accuracy: 0.2108 - val_loss: 3.9458\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4862 - loss: 2.0020 - val_accuracy: 0.4383 - val_loss: 2.1228\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4886 - loss: 1.9640 - val_accuracy: 0.4737 - val_loss: 2.0907\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5072 - loss: 1.9573 - val_accuracy: 0.1929 - val_loss: 3.8030\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5113 - loss: 1.9477 - val_accuracy: 0.3812 - val_loss: 2.7949\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5138 - loss: 1.9339 - val_accuracy: 0.2113 - val_loss: 4.4928\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5333 - loss: 1.9309 - val_accuracy: 0.3750 - val_loss: 2.2862\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5362 - loss: 1.9114 - val_accuracy: 0.2558 - val_loss: 3.2875\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5356 - loss: 1.9445 - val_accuracy: 0.3504 - val_loss: 2.9529\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5561 - loss: 1.8340 - val_accuracy: 0.3846 - val_loss: 3.3340\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5619 - loss: 1.8531 - val_accuracy: 0.4604 - val_loss: 2.1601\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5603 - loss: 1.8216 - val_accuracy: 0.2842 - val_loss: 3.2668\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5606 - loss: 1.8468 - val_accuracy: 0.2163 - val_loss: 5.5071\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5750 - loss: 1.7653 - val_accuracy: 0.3292 - val_loss: 2.6120\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5803 - loss: 1.7383 - val_accuracy: 0.5033 - val_loss: 1.9958\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5865 - loss: 1.7220 - val_accuracy: 0.2221 - val_loss: 4.0813\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6034 - loss: 1.7008 - val_accuracy: 0.3150 - val_loss: 2.5943\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6020 - loss: 1.6915 - val_accuracy: 0.3650 - val_loss: 2.5116\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6026 - loss: 1.6910 - val_accuracy: 0.4483 - val_loss: 2.3113\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6181 - loss: 1.6368 - val_accuracy: 0.5117 - val_loss: 2.0982\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6100 - loss: 1.6748 - val_accuracy: 0.4167 - val_loss: 3.0383\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6109 - loss: 1.6855 - val_accuracy: 0.4183 - val_loss: 3.0416\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6143 - loss: 1.6547 - val_accuracy: 0.5142 - val_loss: 1.9971\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6323 - loss: 1.6276 - val_accuracy: 0.4925 - val_loss: 2.0334\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6344 - loss: 1.5664 - val_accuracy: 0.4196 - val_loss: 2.7343\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6360 - loss: 1.5941 - val_accuracy: 0.6600 - val_loss: 1.5383\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6571 - loss: 1.5021 - val_accuracy: 0.2646 - val_loss: 4.1274\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6360 - loss: 1.6029 - val_accuracy: 0.5792 - val_loss: 1.7116\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6540 - loss: 1.5315 - val_accuracy: 0.4071 - val_loss: 2.1339\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6647 - loss: 1.4989 - val_accuracy: 0.3979 - val_loss: 2.4943\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6402 - loss: 1.5534 - val_accuracy: 0.4471 - val_loss: 2.4634\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6439 - loss: 1.5254 - val_accuracy: 0.5788 - val_loss: 1.8361\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6611 - loss: 1.4729 - val_accuracy: 0.4692 - val_loss: 2.6971\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6688 - loss: 1.4956 - val_accuracy: 0.4533 - val_loss: 3.4070\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6572 - loss: 1.5122 - val_accuracy: 0.4529 - val_loss: 2.5043\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6571 - loss: 1.4885 - val_accuracy: 0.3700 - val_loss: 2.6848\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6485 - loss: 1.5041 - val_accuracy: 0.2946 - val_loss: 3.1743\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6643 - loss: 1.4769 - val_accuracy: 0.4212 - val_loss: 2.4918\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6610 - loss: 1.4763 - val_accuracy: 0.3921 - val_loss: 3.1906\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6822 - loss: 1.4363 - val_accuracy: 0.5054 - val_loss: 1.9867\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6668 - loss: 1.4604 - val_accuracy: 0.3025 - val_loss: 4.4631\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6606 - loss: 1.4698 - val_accuracy: 0.6017 - val_loss: 1.6981\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6886 - loss: 1.4124 - val_accuracy: 0.4104 - val_loss: 2.9566\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6884 - loss: 1.4263 - val_accuracy: 0.4412 - val_loss: 2.8513\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6716 - loss: 1.4181 - val_accuracy: 0.3992 - val_loss: 2.6231\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6845 - loss: 1.4045 - val_accuracy: 0.3762 - val_loss: 3.8977\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6790 - loss: 1.4005 - val_accuracy: 0.4154 - val_loss: 2.3113\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6810 - loss: 1.4118 - val_accuracy: 0.3479 - val_loss: 4.0875\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6759 - loss: 1.4191 - val_accuracy: 0.4442 - val_loss: 2.7161\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6899 - loss: 1.3752 - val_accuracy: 0.4567 - val_loss: 2.4223\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6802 - loss: 1.4041 - val_accuracy: 0.6363 - val_loss: 1.6173\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6963 - loss: 1.3842 - val_accuracy: 0.3117 - val_loss: 5.7297\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6817 - loss: 1.4206 - val_accuracy: 0.4967 - val_loss: 2.3586\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6827 - loss: 1.4263 - val_accuracy: 0.4688 - val_loss: 3.2976\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6943 - loss: 1.3930 - val_accuracy: 0.2746 - val_loss: 5.9886\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6795 - loss: 1.3996 - val_accuracy: 0.5813 - val_loss: 1.7804\n",
            "Model Validation Accuracy: 0.6600\n",
            "\n",
            "Testing Model with 10 layers, 800 neurons, L2=0.0025\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 653ms/step - accuracy: 0.0989 - loss: 21.3639 - val_accuracy: 0.0867 - val_loss: 12.6536\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - accuracy: 0.0992 - loss: 10.5477 - val_accuracy: 0.0996 - val_loss: 5.8632\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 89ms/step - accuracy: 0.1341 - loss: 5.0786 - val_accuracy: 0.1025 - val_loss: 3.7296\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 78ms/step - accuracy: 0.1994 - loss: 3.3714 - val_accuracy: 0.1596 - val_loss: 3.1676\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.2556 - loss: 2.8083 - val_accuracy: 0.1358 - val_loss: 3.3641\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.2930 - loss: 2.7398 - val_accuracy: 0.2429 - val_loss: 2.7953\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.3166 - loss: 2.5807 - val_accuracy: 0.3325 - val_loss: 2.5965\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3231 - loss: 2.5898 - val_accuracy: 0.2000 - val_loss: 2.9394\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3272 - loss: 2.6622 - val_accuracy: 0.3004 - val_loss: 2.6815\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3462 - loss: 2.6994 - val_accuracy: 0.2983 - val_loss: 2.7241\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3310 - loss: 2.6616 - val_accuracy: 0.3079 - val_loss: 2.6914\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3234 - loss: 2.6546 - val_accuracy: 0.3196 - val_loss: 2.7466\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.3650 - loss: 2.7039 - val_accuracy: 0.2321 - val_loss: 4.5173\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3367 - loss: 2.7888 - val_accuracy: 0.1329 - val_loss: 3.2130\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.3290 - loss: 2.7483 - val_accuracy: 0.2146 - val_loss: 4.0836\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3432 - loss: 2.7761 - val_accuracy: 0.2663 - val_loss: 2.8351\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3425 - loss: 2.6887 - val_accuracy: 0.3125 - val_loss: 2.7760\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3473 - loss: 2.6794 - val_accuracy: 0.2067 - val_loss: 3.5708\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3317 - loss: 2.7020 - val_accuracy: 0.3325 - val_loss: 2.7562\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3491 - loss: 2.7443 - val_accuracy: 0.1887 - val_loss: 4.5785\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3235 - loss: 2.8314 - val_accuracy: 0.2862 - val_loss: 2.9025\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.3477 - loss: 2.7238 - val_accuracy: 0.3592 - val_loss: 2.6388\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3323 - loss: 2.7141 - val_accuracy: 0.2183 - val_loss: 2.9578\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3432 - loss: 2.7353 - val_accuracy: 0.2104 - val_loss: 2.9636\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3625 - loss: 2.6888 - val_accuracy: 0.3350 - val_loss: 2.9874\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3302 - loss: 2.7409 - val_accuracy: 0.2283 - val_loss: 2.9780\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3354 - loss: 2.7138 - val_accuracy: 0.2792 - val_loss: 2.8824\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3239 - loss: 2.7279 - val_accuracy: 0.2829 - val_loss: 2.8624\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3390 - loss: 2.7639 - val_accuracy: 0.3471 - val_loss: 2.6682\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3251 - loss: 2.7409 - val_accuracy: 0.3592 - val_loss: 2.7391\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3424 - loss: 2.7874 - val_accuracy: 0.2113 - val_loss: 3.1394\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3442 - loss: 2.7642 - val_accuracy: 0.2208 - val_loss: 3.0194\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3216 - loss: 2.7425 - val_accuracy: 0.1942 - val_loss: 3.7118\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3247 - loss: 2.7985 - val_accuracy: 0.1946 - val_loss: 3.4011\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3487 - loss: 2.7823 - val_accuracy: 0.2846 - val_loss: 2.9386\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3291 - loss: 2.7989 - val_accuracy: 0.1975 - val_loss: 2.9296\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3329 - loss: 2.7549 - val_accuracy: 0.2408 - val_loss: 2.8609\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3298 - loss: 2.7457 - val_accuracy: 0.1996 - val_loss: 3.6984\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3136 - loss: 2.7634 - val_accuracy: 0.2887 - val_loss: 2.7881\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3328 - loss: 2.7126 - val_accuracy: 0.2579 - val_loss: 2.8274\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3408 - loss: 2.7832 - val_accuracy: 0.3104 - val_loss: 2.8229\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3356 - loss: 2.7958 - val_accuracy: 0.2483 - val_loss: 2.9133\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3275 - loss: 2.8123 - val_accuracy: 0.3175 - val_loss: 2.7962\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3214 - loss: 2.7785 - val_accuracy: 0.2512 - val_loss: 3.2594\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3155 - loss: 2.8283 - val_accuracy: 0.2937 - val_loss: 3.2578\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3356 - loss: 2.8590 - val_accuracy: 0.2733 - val_loss: 2.9343\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3407 - loss: 2.8159 - val_accuracy: 0.1787 - val_loss: 3.1634\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.3281 - loss: 2.8223 - val_accuracy: 0.2042 - val_loss: 3.3731\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3299 - loss: 2.8319 - val_accuracy: 0.2225 - val_loss: 3.0502\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3357 - loss: 2.7766 - val_accuracy: 0.2500 - val_loss: 3.3812\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3360 - loss: 2.7923 - val_accuracy: 0.1358 - val_loss: 6.4825\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.3303 - loss: 2.8051 - val_accuracy: 0.1475 - val_loss: 3.1910\n",
            "Model Validation Accuracy: 0.3592\n",
            "\n",
            "Testing Model with 13 layers, 288 neurons, L2=0.0005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 821ms/step - accuracy: 0.1150 - loss: 4.1493 - val_accuracy: 0.1088 - val_loss: 3.6728\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.1708 - loss: 3.6196 - val_accuracy: 0.0896 - val_loss: 3.5603\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.2392 - loss: 3.3164 - val_accuracy: 0.1150 - val_loss: 3.4314\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.2871 - loss: 3.0492 - val_accuracy: 0.1608 - val_loss: 3.2400\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.3071 - loss: 2.8448 - val_accuracy: 0.2371 - val_loss: 2.9433\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.3623 - loss: 2.6184 - val_accuracy: 0.4242 - val_loss: 2.3988\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4014 - loss: 2.4364 - val_accuracy: 0.3613 - val_loss: 2.5794\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.4547 - loss: 2.2336 - val_accuracy: 0.3096 - val_loss: 3.4582\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.4969 - loss: 2.0846 - val_accuracy: 0.5183 - val_loss: 2.0684\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.5403 - loss: 1.9897 - val_accuracy: 0.5533 - val_loss: 2.0016\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.5619 - loss: 1.8722 - val_accuracy: 0.4125 - val_loss: 2.6584\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5554 - loss: 1.8717 - val_accuracy: 0.4400 - val_loss: 2.5575\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.5669 - loss: 1.8212 - val_accuracy: 0.5971 - val_loss: 1.7670\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6033 - loss: 1.7487 - val_accuracy: 0.5788 - val_loss: 1.8490\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6049 - loss: 1.7460 - val_accuracy: 0.5654 - val_loss: 1.9631\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6230 - loss: 1.6910 - val_accuracy: 0.4000 - val_loss: 2.8373\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6224 - loss: 1.7269 - val_accuracy: 0.5050 - val_loss: 2.3808\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.6377 - loss: 1.6940 - val_accuracy: 0.6629 - val_loss: 1.6265\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6447 - loss: 1.6859 - val_accuracy: 0.4583 - val_loss: 2.7323\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6530 - loss: 1.6736 - val_accuracy: 0.5492 - val_loss: 1.9914\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.6518 - loss: 1.6625 - val_accuracy: 0.6938 - val_loss: 1.5224\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6709 - loss: 1.6349 - val_accuracy: 0.4817 - val_loss: 2.4738\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6789 - loss: 1.6173 - val_accuracy: 0.5333 - val_loss: 2.1826\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6921 - loss: 1.6276 - val_accuracy: 0.5333 - val_loss: 2.3153\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6680 - loss: 1.6617 - val_accuracy: 0.6817 - val_loss: 1.6256\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6818 - loss: 1.6088 - val_accuracy: 0.6500 - val_loss: 1.7950\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.6889 - loss: 1.6415 - val_accuracy: 0.7304 - val_loss: 1.5290\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6945 - loss: 1.5898 - val_accuracy: 0.5983 - val_loss: 1.9932\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7193 - loss: 1.5736 - val_accuracy: 0.6317 - val_loss: 1.7782\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6801 - loss: 1.6565 - val_accuracy: 0.5008 - val_loss: 2.5863\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6946 - loss: 1.6225 - val_accuracy: 0.6329 - val_loss: 1.8167\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6999 - loss: 1.6280 - val_accuracy: 0.6737 - val_loss: 1.7352\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7026 - loss: 1.6502 - val_accuracy: 0.6837 - val_loss: 1.7352\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7062 - loss: 1.6340 - val_accuracy: 0.6471 - val_loss: 1.7658\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7176 - loss: 1.5750 - val_accuracy: 0.7358 - val_loss: 1.4600\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7179 - loss: 1.5813 - val_accuracy: 0.6317 - val_loss: 1.8450\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7174 - loss: 1.5595 - val_accuracy: 0.6821 - val_loss: 1.6722\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6999 - loss: 1.6826 - val_accuracy: 0.7133 - val_loss: 1.5937\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7190 - loss: 1.6119 - val_accuracy: 0.6550 - val_loss: 1.7410\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7216 - loss: 1.5736 - val_accuracy: 0.7096 - val_loss: 1.6085\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.7220 - loss: 1.6040 - val_accuracy: 0.6604 - val_loss: 1.7549\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7205 - loss: 1.6082 - val_accuracy: 0.7504 - val_loss: 1.5014\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7506 - loss: 1.5369 - val_accuracy: 0.7221 - val_loss: 1.5385\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7188 - loss: 1.6040 - val_accuracy: 0.6608 - val_loss: 1.7244\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7279 - loss: 1.5766 - val_accuracy: 0.6783 - val_loss: 1.6875\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7293 - loss: 1.5537 - val_accuracy: 0.7242 - val_loss: 1.6846\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.7286 - loss: 1.5683 - val_accuracy: 0.7517 - val_loss: 1.4952\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7418 - loss: 1.5531 - val_accuracy: 0.5333 - val_loss: 2.5564\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7252 - loss: 1.5950 - val_accuracy: 0.6787 - val_loss: 1.7475\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7467 - loss: 1.5287 - val_accuracy: 0.5650 - val_loss: 2.3399\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7482 - loss: 1.5455 - val_accuracy: 0.6500 - val_loss: 1.8425\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7399 - loss: 1.5580 - val_accuracy: 0.6333 - val_loss: 1.8404\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7338 - loss: 1.5958 - val_accuracy: 0.6042 - val_loss: 2.1975\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7375 - loss: 1.5819 - val_accuracy: 0.5342 - val_loss: 2.6321\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7061 - loss: 1.6704 - val_accuracy: 0.7108 - val_loss: 1.7090\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7525 - loss: 1.5132 - val_accuracy: 0.5442 - val_loss: 2.5796\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7431 - loss: 1.5424 - val_accuracy: 0.7158 - val_loss: 1.5976\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7533 - loss: 1.5350 - val_accuracy: 0.6792 - val_loss: 1.8801\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7437 - loss: 1.5624 - val_accuracy: 0.6421 - val_loss: 1.9178\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7553 - loss: 1.5401 - val_accuracy: 0.6704 - val_loss: 1.7351\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7598 - loss: 1.5491 - val_accuracy: 0.7429 - val_loss: 1.5382\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7404 - loss: 1.6124 - val_accuracy: 0.6579 - val_loss: 1.9937\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7607 - loss: 1.5340 - val_accuracy: 0.6754 - val_loss: 1.7934\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.7612 - loss: 1.5270 - val_accuracy: 0.7625 - val_loss: 1.4806\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7507 - loss: 1.5468 - val_accuracy: 0.7229 - val_loss: 1.6184\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7598 - loss: 1.5119 - val_accuracy: 0.7113 - val_loss: 1.6624\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7481 - loss: 1.5482 - val_accuracy: 0.7154 - val_loss: 1.6422\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7496 - loss: 1.5873 - val_accuracy: 0.6879 - val_loss: 1.8395\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7585 - loss: 1.5181 - val_accuracy: 0.6733 - val_loss: 1.7759\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7726 - loss: 1.4760 - val_accuracy: 0.7287 - val_loss: 1.6523\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7592 - loss: 1.5290 - val_accuracy: 0.6950 - val_loss: 1.7647\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7568 - loss: 1.5425 - val_accuracy: 0.7487 - val_loss: 1.6006\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7715 - loss: 1.4828 - val_accuracy: 0.6925 - val_loss: 1.8297\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7591 - loss: 1.5212 - val_accuracy: 0.7017 - val_loss: 1.7290\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7736 - loss: 1.4948 - val_accuracy: 0.7625 - val_loss: 1.5109\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7703 - loss: 1.4849 - val_accuracy: 0.7683 - val_loss: 1.4699\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7889 - loss: 1.4358 - val_accuracy: 0.7479 - val_loss: 1.5077\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7655 - loss: 1.5160 - val_accuracy: 0.7021 - val_loss: 1.6767\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7710 - loss: 1.4769 - val_accuracy: 0.4446 - val_loss: 3.5071\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7740 - loss: 1.5179 - val_accuracy: 0.7275 - val_loss: 1.5976\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7653 - loss: 1.5405 - val_accuracy: 0.7763 - val_loss: 1.4574\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7715 - loss: 1.4947 - val_accuracy: 0.7921 - val_loss: 1.4778\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7745 - loss: 1.4751 - val_accuracy: 0.6979 - val_loss: 1.8275\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7767 - loss: 1.4766 - val_accuracy: 0.6275 - val_loss: 2.0344\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7709 - loss: 1.4774 - val_accuracy: 0.6629 - val_loss: 1.9339\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7720 - loss: 1.5097 - val_accuracy: 0.7671 - val_loss: 1.5343\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7665 - loss: 1.5208 - val_accuracy: 0.7892 - val_loss: 1.4538\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7930 - loss: 1.4645 - val_accuracy: 0.6633 - val_loss: 1.8493\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7745 - loss: 1.4871 - val_accuracy: 0.6938 - val_loss: 1.8116\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7735 - loss: 1.4908 - val_accuracy: 0.7667 - val_loss: 1.4960\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7820 - loss: 1.4454 - val_accuracy: 0.7775 - val_loss: 1.5052\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7816 - loss: 1.4612 - val_accuracy: 0.7117 - val_loss: 1.6796\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7985 - loss: 1.4345 - val_accuracy: 0.7904 - val_loss: 1.4563\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7864 - loss: 1.4539 - val_accuracy: 0.7733 - val_loss: 1.5656\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7923 - loss: 1.4320 - val_accuracy: 0.7862 - val_loss: 1.4307\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7871 - loss: 1.4465 - val_accuracy: 0.7604 - val_loss: 1.6376\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7843 - loss: 1.4573 - val_accuracy: 0.7600 - val_loss: 1.4691\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7810 - loss: 1.4376 - val_accuracy: 0.7121 - val_loss: 1.6561\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8034 - loss: 1.3924 - val_accuracy: 0.7708 - val_loss: 1.5416\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7927 - loss: 1.4168 - val_accuracy: 0.7229 - val_loss: 1.6939\n",
            "Model Validation Accuracy: 0.7921\n",
            "\n",
            "Testing Model with 14 layers, 288 neurons, L2=0.005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 871ms/step - accuracy: 0.1140 - loss: 17.3395 - val_accuracy: 0.1083 - val_loss: 10.7836\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.1892 - loss: 9.2264 - val_accuracy: 0.1004 - val_loss: 6.0273\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.2392 - loss: 5.1227 - val_accuracy: 0.1004 - val_loss: 4.3163\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.2505 - loss: 3.5923 - val_accuracy: 0.1021 - val_loss: 3.7712\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3000 - loss: 2.9827 - val_accuracy: 0.1671 - val_loss: 3.0493\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.2938 - loss: 2.7278 - val_accuracy: 0.3283 - val_loss: 2.6341\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3036 - loss: 2.6845 - val_accuracy: 0.3762 - val_loss: 2.5080\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3478 - loss: 2.6226 - val_accuracy: 0.1200 - val_loss: 5.4131\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3608 - loss: 2.6408 - val_accuracy: 0.3529 - val_loss: 2.5293\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.3796 - loss: 2.5179 - val_accuracy: 0.3938 - val_loss: 2.4795\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3777 - loss: 2.5502 - val_accuracy: 0.3617 - val_loss: 2.6226\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.3759 - loss: 2.6040 - val_accuracy: 0.2983 - val_loss: 2.7825\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3687 - loss: 2.6145 - val_accuracy: 0.4162 - val_loss: 2.4139\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3918 - loss: 2.4804 - val_accuracy: 0.3579 - val_loss: 2.7023\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.3679 - loss: 2.5985 - val_accuracy: 0.3717 - val_loss: 2.5239\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3741 - loss: 2.5695 - val_accuracy: 0.3704 - val_loss: 2.5004\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3801 - loss: 2.5247 - val_accuracy: 0.4071 - val_loss: 2.4288\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.3721 - loss: 2.5152 - val_accuracy: 0.3158 - val_loss: 2.6732\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3803 - loss: 2.5154 - val_accuracy: 0.3438 - val_loss: 2.6200\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3843 - loss: 2.5676 - val_accuracy: 0.3208 - val_loss: 2.5601\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3739 - loss: 2.4680 - val_accuracy: 0.3508 - val_loss: 2.5582\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.3792 - loss: 2.5413 - val_accuracy: 0.3104 - val_loss: 2.7330\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3811 - loss: 2.5653 - val_accuracy: 0.3713 - val_loss: 2.6313\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3809 - loss: 2.5701 - val_accuracy: 0.2596 - val_loss: 2.8079\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3769 - loss: 2.5535 - val_accuracy: 0.2833 - val_loss: 2.9665\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3854 - loss: 2.5913 - val_accuracy: 0.3408 - val_loss: 2.6961\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.3692 - loss: 2.5921 - val_accuracy: 0.2637 - val_loss: 2.7931\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4041 - loss: 2.4698 - val_accuracy: 0.2546 - val_loss: 2.7401\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3925 - loss: 2.5540 - val_accuracy: 0.2154 - val_loss: 3.4445\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3877 - loss: 2.5215 - val_accuracy: 0.2967 - val_loss: 2.6722\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3824 - loss: 2.5124 - val_accuracy: 0.3379 - val_loss: 2.5437\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3992 - loss: 2.4800 - val_accuracy: 0.4046 - val_loss: 2.5604\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3803 - loss: 2.6023 - val_accuracy: 0.3550 - val_loss: 2.5538\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3886 - loss: 2.5100 - val_accuracy: 0.2262 - val_loss: 2.9142\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3800 - loss: 2.5528 - val_accuracy: 0.3554 - val_loss: 2.5702\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3602 - loss: 2.5255 - val_accuracy: 0.1679 - val_loss: 4.7526\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.3798 - loss: 2.5249 - val_accuracy: 0.1737 - val_loss: 3.2595\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3774 - loss: 2.5132 - val_accuracy: 0.2392 - val_loss: 3.2998\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.4017 - loss: 2.5091 - val_accuracy: 0.4171 - val_loss: 2.5013\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.3679 - loss: 2.5914 - val_accuracy: 0.2221 - val_loss: 3.1790\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3910 - loss: 2.5026 - val_accuracy: 0.2108 - val_loss: 3.3799\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3943 - loss: 2.5044 - val_accuracy: 0.3192 - val_loss: 2.7924\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3828 - loss: 2.5196 - val_accuracy: 0.1075 - val_loss: 3.6696\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.3715 - loss: 2.5255 - val_accuracy: 0.3338 - val_loss: 2.5580\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3490 - loss: 2.4909 - val_accuracy: 0.3837 - val_loss: 2.4450\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3621 - loss: 2.4875 - val_accuracy: 0.3771 - val_loss: 2.4993\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.3731 - loss: 2.4422 - val_accuracy: 0.4288 - val_loss: 2.4241\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.3726 - loss: 2.5383 - val_accuracy: 0.3383 - val_loss: 2.5296\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.3808 - loss: 2.4349 - val_accuracy: 0.2483 - val_loss: 2.8943\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.3846 - loss: 2.4790 - val_accuracy: 0.3663 - val_loss: 2.4830\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3837 - loss: 2.4846 - val_accuracy: 0.2979 - val_loss: 3.1520\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.3972 - loss: 2.4429 - val_accuracy: 0.2700 - val_loss: 2.8100\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3865 - loss: 2.4640 - val_accuracy: 0.3408 - val_loss: 3.0019\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4022 - loss: 2.4498 - val_accuracy: 0.2821 - val_loss: 2.9209\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4030 - loss: 2.4384 - val_accuracy: 0.3717 - val_loss: 2.6261\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3999 - loss: 2.4462 - val_accuracy: 0.3458 - val_loss: 2.6880\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3836 - loss: 2.5232 - val_accuracy: 0.3225 - val_loss: 2.8604\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.3886 - loss: 2.5122 - val_accuracy: 0.3525 - val_loss: 2.8450\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.3767 - loss: 2.5487 - val_accuracy: 0.2688 - val_loss: 2.6695\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3821 - loss: 2.4132 - val_accuracy: 0.1846 - val_loss: 3.0213\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3747 - loss: 2.4816 - val_accuracy: 0.3442 - val_loss: 2.4989\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4004 - loss: 2.3785 - val_accuracy: 0.1846 - val_loss: 2.9322\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3956 - loss: 2.3828 - val_accuracy: 0.4179 - val_loss: 2.3990\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4005 - loss: 2.3437 - val_accuracy: 0.2004 - val_loss: 3.9132\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3913 - loss: 2.3660 - val_accuracy: 0.2571 - val_loss: 3.2184\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4017 - loss: 2.3837 - val_accuracy: 0.4092 - val_loss: 2.4143\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4282 - loss: 2.3638 - val_accuracy: 0.3000 - val_loss: 2.6735\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4385 - loss: 2.3190 - val_accuracy: 0.1725 - val_loss: 3.6972\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4094 - loss: 2.3875 - val_accuracy: 0.2804 - val_loss: 2.7659\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4129 - loss: 2.3997 - val_accuracy: 0.2929 - val_loss: 2.7116\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4090 - loss: 2.3590 - val_accuracy: 0.3438 - val_loss: 2.4495\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4038 - loss: 2.3398 - val_accuracy: 0.1583 - val_loss: 3.1533\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4143 - loss: 2.3584 - val_accuracy: 0.2100 - val_loss: 3.4019\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4365 - loss: 2.3560 - val_accuracy: 0.3008 - val_loss: 3.0088\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4229 - loss: 2.3702 - val_accuracy: 0.2217 - val_loss: 4.5850\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4122 - loss: 2.3495 - val_accuracy: 0.2271 - val_loss: 3.2615\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4330 - loss: 2.3416 - val_accuracy: 0.3587 - val_loss: 2.4272\n",
            "Model Validation Accuracy: 0.4288\n"
          ]
        }
      ],
      "source": [
        "# second random search / higher complexity\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "# TPU initialization\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Leave tpu= argument empty for Colab TPU\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# Define the hyperparameter space\n",
        "num_layers_options = list(range(5, 15))  # Reduce maximum number of layers to avoid excessive downsampling\n",
        "neurons_options = list(range(256, 1025, 32))\n",
        "regs = [0.0005, 0.001, 0.0015, 0.0025, 0.005, 0.01]\n",
        "\n",
        "# Set the number of random searches\n",
        "n_random_searches = 10\n",
        "\n",
        "best_model = None\n",
        "best_val_acc = 0\n",
        "best_hyperparams = None\n",
        "\n",
        "results_G = []\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Initial Random Search\n",
        "for _ in range(n_random_searches):\n",
        "    # Randomly select a combination of hyperparameters\n",
        "    num_layers = random.choice(num_layers_options)\n",
        "    neurons = random.choice(neurons_options)\n",
        "    reg = random.choice(regs)\n",
        "\n",
        "    print(f\"\\nTesting Model with {num_layers} layers, {neurons} neurons, L2={reg}\")\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(28, 28, 3)))\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            model.add(Conv2D(filters=neurons, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(LeakyReLU())\n",
        "            # Apply MaxPooling2D every other layer to avoid excessive downsampling\n",
        "            if i % 2 == 1:\n",
        "                model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(neurons, activation=\"relu\"))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "        optimizer = AdamW(learning_rate=0.001)\n",
        "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, verbose=0)\n",
        "        checkpoint = ModelCheckpoint('model_checkpoint.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "        csv_logger = CSVLogger('training_log.csv', append=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            datagen.flow(X_train, y_train, batch_size=64),\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            epochs=100,  # Reduced number of epochs\n",
        "            callbacks=[early_stopping, checkpoint, csv_logger],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    best_epoch = np.argmax(history.history['val_accuracy'])\n",
        "    train_acc = history.history['accuracy'][best_epoch]\n",
        "    val_acc = history.history['val_accuracy'][best_epoch]\n",
        "    print(f\"Model Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    results_G.append({\n",
        "        \"num_layers\": num_layers,\n",
        "        \"neurons\": neurons,\n",
        "        \"l2_reg\": reg,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_acc\": val_acc\n",
        "    })\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = model\n",
        "        best_hyperparams = (num_layers, neurons, reg)\n",
        "\n",
        "        print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "        print(f\"Best Model: {best_model}\")\n",
        "        print(f\"Best Hyperparameters: Layers={num_layers}, Neurons={neurons}, L2={reg}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvn_Nculd5XL",
        "outputId": "db116e6c-6254-4afd-8063-d096ddfe722f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Model with 11 layers, 320 neurons, L2=0.0005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 663ms/step - accuracy: 0.1124 - loss: 4.1935 - val_accuracy: 0.1112 - val_loss: 3.7824\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.1287 - loss: 3.7901 - val_accuracy: 0.1138 - val_loss: 3.5586\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.2400 - loss: 3.3160 - val_accuracy: 0.1096 - val_loss: 4.5001\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.2976 - loss: 2.9953 - val_accuracy: 0.1513 - val_loss: 3.5721\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3767 - loss: 2.6558 - val_accuracy: 0.3271 - val_loss: 2.6793\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.4320 - loss: 2.4208 - val_accuracy: 0.3625 - val_loss: 2.6544\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4662 - loss: 2.2587 - val_accuracy: 0.3454 - val_loss: 3.5057\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.5055 - loss: 2.0815 - val_accuracy: 0.4575 - val_loss: 2.3057\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5340 - loss: 1.9870 - val_accuracy: 0.4421 - val_loss: 2.2753\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5662 - loss: 1.8957 - val_accuracy: 0.4675 - val_loss: 2.2560\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5910 - loss: 1.8135 - val_accuracy: 0.3508 - val_loss: 3.8179\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5981 - loss: 1.7789 - val_accuracy: 0.5663 - val_loss: 1.9641\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6009 - loss: 1.7684 - val_accuracy: 0.5883 - val_loss: 2.0019\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.6057 - loss: 1.7423 - val_accuracy: 0.5696 - val_loss: 1.9048\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.6422 - loss: 1.6824 - val_accuracy: 0.4592 - val_loss: 2.4188\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6500 - loss: 1.6453 - val_accuracy: 0.6008 - val_loss: 1.8260\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.6712 - loss: 1.6065 - val_accuracy: 0.5258 - val_loss: 2.1263\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6755 - loss: 1.6151 - val_accuracy: 0.5679 - val_loss: 1.8794\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.6809 - loss: 1.6073 - val_accuracy: 0.6400 - val_loss: 1.7478\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6797 - loss: 1.6173 - val_accuracy: 0.6521 - val_loss: 1.7672\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.6714 - loss: 1.6202 - val_accuracy: 0.6696 - val_loss: 1.7243\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6861 - loss: 1.5921 - val_accuracy: 0.5517 - val_loss: 2.0225\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6764 - loss: 1.6133 - val_accuracy: 0.6646 - val_loss: 1.6301\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6984 - loss: 1.6037 - val_accuracy: 0.6854 - val_loss: 1.6453\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7067 - loss: 1.5801 - val_accuracy: 0.5979 - val_loss: 2.1043\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7081 - loss: 1.5836 - val_accuracy: 0.6896 - val_loss: 1.6568\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.7039 - loss: 1.6117 - val_accuracy: 0.6917 - val_loss: 1.6499\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7104 - loss: 1.5891 - val_accuracy: 0.5804 - val_loss: 2.1013\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7123 - loss: 1.6097 - val_accuracy: 0.5317 - val_loss: 2.3040\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7045 - loss: 1.6195 - val_accuracy: 0.6925 - val_loss: 1.6216\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7233 - loss: 1.5400 - val_accuracy: 0.7179 - val_loss: 1.6662\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7093 - loss: 1.6047 - val_accuracy: 0.3542 - val_loss: 4.0428\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7386 - loss: 1.5358 - val_accuracy: 0.6183 - val_loss: 2.0799\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7131 - loss: 1.6319 - val_accuracy: 0.6896 - val_loss: 1.7766\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7365 - loss: 1.5597 - val_accuracy: 0.6762 - val_loss: 1.7670\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7236 - loss: 1.6132 - val_accuracy: 0.6900 - val_loss: 1.6603\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7282 - loss: 1.5968 - val_accuracy: 0.6725 - val_loss: 1.7093\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7373 - loss: 1.5848 - val_accuracy: 0.6754 - val_loss: 1.7538\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7271 - loss: 1.6001 - val_accuracy: 0.6225 - val_loss: 1.9767\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7288 - loss: 1.5514 - val_accuracy: 0.7467 - val_loss: 1.5344\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7568 - loss: 1.5024 - val_accuracy: 0.6812 - val_loss: 1.7091\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7323 - loss: 1.5559 - val_accuracy: 0.7454 - val_loss: 1.5195\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7541 - loss: 1.5444 - val_accuracy: 0.6904 - val_loss: 1.7852\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7289 - loss: 1.6209 - val_accuracy: 0.5250 - val_loss: 2.5863\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7415 - loss: 1.5684 - val_accuracy: 0.6700 - val_loss: 1.8040\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7371 - loss: 1.5671 - val_accuracy: 0.5008 - val_loss: 2.3767\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7479 - loss: 1.5650 - val_accuracy: 0.6467 - val_loss: 2.0138\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7465 - loss: 1.5631 - val_accuracy: 0.6971 - val_loss: 1.7163\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7507 - loss: 1.5547 - val_accuracy: 0.5537 - val_loss: 2.2878\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7589 - loss: 1.5246 - val_accuracy: 0.7204 - val_loss: 1.6482\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.7583 - loss: 1.5113 - val_accuracy: 0.7713 - val_loss: 1.4795\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7691 - loss: 1.4803 - val_accuracy: 0.7471 - val_loss: 1.5213\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7576 - loss: 1.5011 - val_accuracy: 0.6392 - val_loss: 2.1158\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7633 - loss: 1.5195 - val_accuracy: 0.6833 - val_loss: 1.7022\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7674 - loss: 1.5261 - val_accuracy: 0.7617 - val_loss: 1.4927\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7752 - loss: 1.4852 - val_accuracy: 0.7017 - val_loss: 1.7629\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7697 - loss: 1.5129 - val_accuracy: 0.6808 - val_loss: 1.7977\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7646 - loss: 1.4960 - val_accuracy: 0.7487 - val_loss: 1.5500\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7802 - loss: 1.4765 - val_accuracy: 0.7533 - val_loss: 1.5165\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7757 - loss: 1.4929 - val_accuracy: 0.7096 - val_loss: 1.6622\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7701 - loss: 1.5167 - val_accuracy: 0.7371 - val_loss: 1.5814\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7693 - loss: 1.5040 - val_accuracy: 0.7517 - val_loss: 1.6054\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7707 - loss: 1.5322 - val_accuracy: 0.7412 - val_loss: 1.5784\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7746 - loss: 1.4789 - val_accuracy: 0.6679 - val_loss: 1.8523\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7732 - loss: 1.4959 - val_accuracy: 0.6925 - val_loss: 1.8468\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7787 - loss: 1.5049 - val_accuracy: 0.7125 - val_loss: 1.7508\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7725 - loss: 1.4985 - val_accuracy: 0.7317 - val_loss: 1.6389\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7767 - loss: 1.4635 - val_accuracy: 0.7321 - val_loss: 1.6571\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7704 - loss: 1.4902 - val_accuracy: 0.7008 - val_loss: 1.8049\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7768 - loss: 1.4965 - val_accuracy: 0.6762 - val_loss: 1.7882\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7745 - loss: 1.5005 - val_accuracy: 0.7900 - val_loss: 1.4093\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7864 - loss: 1.4333 - val_accuracy: 0.7217 - val_loss: 1.6220\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7787 - loss: 1.4572 - val_accuracy: 0.7858 - val_loss: 1.4748\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7797 - loss: 1.4823 - val_accuracy: 0.6967 - val_loss: 1.7108\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7907 - loss: 1.4309 - val_accuracy: 0.6538 - val_loss: 2.0350\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7661 - loss: 1.5182 - val_accuracy: 0.7008 - val_loss: 1.8036\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7831 - loss: 1.4575 - val_accuracy: 0.7325 - val_loss: 1.6308\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7822 - loss: 1.4537 - val_accuracy: 0.7258 - val_loss: 1.6436\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7860 - loss: 1.4464 - val_accuracy: 0.7450 - val_loss: 1.5738\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7981 - loss: 1.4117 - val_accuracy: 0.7408 - val_loss: 1.5928\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7840 - loss: 1.4137 - val_accuracy: 0.5775 - val_loss: 2.5859\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7722 - loss: 1.4849 - val_accuracy: 0.6946 - val_loss: 1.7260\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7835 - loss: 1.4306 - val_accuracy: 0.7837 - val_loss: 1.4715\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7865 - loss: 1.4446 - val_accuracy: 0.7275 - val_loss: 1.6539\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8029 - loss: 1.4119 - val_accuracy: 0.7196 - val_loss: 1.7458\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7917 - loss: 1.4306 - val_accuracy: 0.7479 - val_loss: 1.5608\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8042 - loss: 1.3950 - val_accuracy: 0.7254 - val_loss: 1.7176\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.7889 - loss: 1.4344 - val_accuracy: 0.7937 - val_loss: 1.3943\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7766 - loss: 1.4832 - val_accuracy: 0.7208 - val_loss: 1.7346\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8176 - loss: 1.3821 - val_accuracy: 0.7788 - val_loss: 1.4943\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8099 - loss: 1.3351 - val_accuracy: 0.7817 - val_loss: 1.4237\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7883 - loss: 1.4298 - val_accuracy: 0.7846 - val_loss: 1.4119\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7908 - loss: 1.4223 - val_accuracy: 0.7404 - val_loss: 1.5694\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7980 - loss: 1.3976 - val_accuracy: 0.7621 - val_loss: 1.5240\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7943 - loss: 1.4338 - val_accuracy: 0.7800 - val_loss: 1.5025\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8060 - loss: 1.3564 - val_accuracy: 0.7867 - val_loss: 1.4442\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8101 - loss: 1.3449 - val_accuracy: 0.7317 - val_loss: 1.6297\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7999 - loss: 1.4184 - val_accuracy: 0.7675 - val_loss: 1.5192\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8034 - loss: 1.3569 - val_accuracy: 0.7417 - val_loss: 1.5836\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.8089 - loss: 1.3518 - val_accuracy: 0.8267 - val_loss: 1.3065\n",
            "Model Validation Accuracy: 0.8267\n",
            "Best Validation Accuracy: 0.8267\n",
            "Best Model: <Sequential name=sequential, built=True>\n",
            "Best Hyperparameters: Layers=11, Neurons=320, L2=0.0005\n",
            "\n",
            "Testing Model with 9 layers, 352 neurons, L2=0.0015\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 667ms/step - accuracy: 0.2007 - loss: 6.3816 - val_accuracy: 0.1329 - val_loss: 12.0113\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.3767 - loss: 4.2506 - val_accuracy: 0.1975 - val_loss: 3.8960\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.4767 - loss: 2.9609 - val_accuracy: 0.2446 - val_loss: 3.2591\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.5276 - loss: 2.4020 - val_accuracy: 0.3204 - val_loss: 2.7708\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.5366 - loss: 2.2052 - val_accuracy: 0.4112 - val_loss: 2.4980\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.5686 - loss: 2.1054 - val_accuracy: 0.4979 - val_loss: 2.2911\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.5936 - loss: 2.0584 - val_accuracy: 0.5150 - val_loss: 2.3606\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5869 - loss: 2.0806 - val_accuracy: 0.4437 - val_loss: 2.5537\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5912 - loss: 2.0488 - val_accuracy: 0.2796 - val_loss: 4.0394\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6040 - loss: 2.0528 - val_accuracy: 0.4238 - val_loss: 2.6454\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6250 - loss: 2.0119 - val_accuracy: 0.3675 - val_loss: 3.1364\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6202 - loss: 2.0345 - val_accuracy: 0.4975 - val_loss: 2.3523\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6424 - loss: 2.0006 - val_accuracy: 0.4250 - val_loss: 3.4192\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.6289 - loss: 1.9971 - val_accuracy: 0.5442 - val_loss: 2.2858\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6249 - loss: 2.0480 - val_accuracy: 0.5429 - val_loss: 2.4436\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.6370 - loss: 2.0220 - val_accuracy: 0.6225 - val_loss: 2.0304\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6509 - loss: 1.9949 - val_accuracy: 0.4579 - val_loss: 3.1816\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6454 - loss: 1.9703 - val_accuracy: 0.5375 - val_loss: 2.4434\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6463 - loss: 1.9776 - val_accuracy: 0.4521 - val_loss: 2.8690\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6633 - loss: 1.9506 - val_accuracy: 0.4508 - val_loss: 2.8251\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6574 - loss: 1.9685 - val_accuracy: 0.4029 - val_loss: 3.4119\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6717 - loss: 1.9462 - val_accuracy: 0.5108 - val_loss: 2.5437\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6484 - loss: 1.9730 - val_accuracy: 0.6154 - val_loss: 2.0636\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.6559 - loss: 1.9709 - val_accuracy: 0.6329 - val_loss: 1.9877\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6728 - loss: 1.9053 - val_accuracy: 0.4592 - val_loss: 2.6879\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.6677 - loss: 1.9486 - val_accuracy: 0.6708 - val_loss: 1.8986\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6698 - loss: 1.9058 - val_accuracy: 0.6375 - val_loss: 2.0475\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6561 - loss: 1.9218 - val_accuracy: 0.3938 - val_loss: 3.4201\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6722 - loss: 1.9189 - val_accuracy: 0.6133 - val_loss: 2.0442\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6622 - loss: 1.9150 - val_accuracy: 0.5204 - val_loss: 2.4345\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6774 - loss: 1.8991 - val_accuracy: 0.6463 - val_loss: 2.0694\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6922 - loss: 1.8280 - val_accuracy: 0.5967 - val_loss: 2.1013\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6727 - loss: 1.9146 - val_accuracy: 0.2858 - val_loss: 4.2337\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6649 - loss: 1.9018 - val_accuracy: 0.5296 - val_loss: 2.3061\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6799 - loss: 1.8878 - val_accuracy: 0.6237 - val_loss: 1.9457\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6789 - loss: 1.8308 - val_accuracy: 0.5804 - val_loss: 2.1841\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6861 - loss: 1.8426 - val_accuracy: 0.6929 - val_loss: 1.8008\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7067 - loss: 1.7752 - val_accuracy: 0.6737 - val_loss: 1.9523\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6962 - loss: 1.8331 - val_accuracy: 0.6717 - val_loss: 1.9170\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6975 - loss: 1.8265 - val_accuracy: 0.5525 - val_loss: 2.3395\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6981 - loss: 1.8487 - val_accuracy: 0.6675 - val_loss: 1.8818\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6938 - loss: 1.7908 - val_accuracy: 0.5225 - val_loss: 2.5126\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6711 - loss: 1.8592 - val_accuracy: 0.5821 - val_loss: 2.1241\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7076 - loss: 1.7660 - val_accuracy: 0.5038 - val_loss: 2.3342\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6790 - loss: 1.8516 - val_accuracy: 0.6367 - val_loss: 1.9490\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6981 - loss: 1.7843 - val_accuracy: 0.5496 - val_loss: 2.3936\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6976 - loss: 1.7954 - val_accuracy: 0.6050 - val_loss: 2.0166\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.7115 - loss: 1.7689 - val_accuracy: 0.7196 - val_loss: 1.7353\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6990 - loss: 1.7829 - val_accuracy: 0.6579 - val_loss: 1.9964\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7056 - loss: 1.7908 - val_accuracy: 0.5342 - val_loss: 2.3144\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7010 - loss: 1.7671 - val_accuracy: 0.6171 - val_loss: 2.0974\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7079 - loss: 1.7626 - val_accuracy: 0.5163 - val_loss: 2.4148\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7040 - loss: 1.7700 - val_accuracy: 0.5696 - val_loss: 2.2734\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7132 - loss: 1.6961 - val_accuracy: 0.5950 - val_loss: 2.1584\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6879 - loss: 1.7870 - val_accuracy: 0.5088 - val_loss: 2.3297\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7234 - loss: 1.6770 - val_accuracy: 0.5896 - val_loss: 2.2413\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7078 - loss: 1.7253 - val_accuracy: 0.7046 - val_loss: 1.7899\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7226 - loss: 1.6568 - val_accuracy: 0.4821 - val_loss: 2.7703\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6993 - loss: 1.7309 - val_accuracy: 0.4954 - val_loss: 2.3237\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7178 - loss: 1.6797 - val_accuracy: 0.5358 - val_loss: 2.6338\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7353 - loss: 1.6179 - val_accuracy: 0.5558 - val_loss: 2.3416\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7323 - loss: 1.6582 - val_accuracy: 0.7387 - val_loss: 1.6247\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7166 - loss: 1.6807 - val_accuracy: 0.6288 - val_loss: 2.0804\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7375 - loss: 1.6013 - val_accuracy: 0.6204 - val_loss: 2.0790\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7436 - loss: 1.5902 - val_accuracy: 0.5671 - val_loss: 2.3866\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7395 - loss: 1.6293 - val_accuracy: 0.5958 - val_loss: 2.1443\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7305 - loss: 1.6361 - val_accuracy: 0.6325 - val_loss: 1.9129\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7297 - loss: 1.6283 - val_accuracy: 0.7104 - val_loss: 1.6968\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7329 - loss: 1.6089 - val_accuracy: 0.6263 - val_loss: 2.1874\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7381 - loss: 1.6300 - val_accuracy: 0.6358 - val_loss: 1.8935\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7355 - loss: 1.6035 - val_accuracy: 0.7217 - val_loss: 1.6892\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7441 - loss: 1.5911 - val_accuracy: 0.7421 - val_loss: 1.5660\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7402 - loss: 1.5502 - val_accuracy: 0.5013 - val_loss: 2.4153\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7518 - loss: 1.5592 - val_accuracy: 0.6142 - val_loss: 1.9577\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7535 - loss: 1.5886 - val_accuracy: 0.7083 - val_loss: 1.6879\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7417 - loss: 1.5709 - val_accuracy: 0.6754 - val_loss: 1.7461\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7423 - loss: 1.5612 - val_accuracy: 0.6050 - val_loss: 2.0176\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7419 - loss: 1.5859 - val_accuracy: 0.7083 - val_loss: 1.7045\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7375 - loss: 1.5771 - val_accuracy: 0.6229 - val_loss: 2.0623\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7245 - loss: 1.5607 - val_accuracy: 0.7121 - val_loss: 1.6453\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7617 - loss: 1.5051 - val_accuracy: 0.6442 - val_loss: 1.9440\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7600 - loss: 1.4986 - val_accuracy: 0.6104 - val_loss: 2.2221\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7600 - loss: 1.5144 - val_accuracy: 0.7071 - val_loss: 1.6561\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7563 - loss: 1.4631 - val_accuracy: 0.5417 - val_loss: 2.4674\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7423 - loss: 1.5649 - val_accuracy: 0.7013 - val_loss: 1.6835\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7469 - loss: 1.5620 - val_accuracy: 0.7333 - val_loss: 1.5825\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7740 - loss: 1.4450 - val_accuracy: 0.6779 - val_loss: 1.7712\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7614 - loss: 1.4683 - val_accuracy: 0.6696 - val_loss: 1.8166\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7461 - loss: 1.5612 - val_accuracy: 0.6879 - val_loss: 1.7236\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7476 - loss: 1.5089 - val_accuracy: 0.6396 - val_loss: 2.1100\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7646 - loss: 1.4692 - val_accuracy: 0.6696 - val_loss: 1.8353\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.7501 - loss: 1.4942 - val_accuracy: 0.7971 - val_loss: 1.4009\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7690 - loss: 1.4318 - val_accuracy: 0.6496 - val_loss: 1.9048\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7522 - loss: 1.4895 - val_accuracy: 0.6154 - val_loss: 2.3137\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7624 - loss: 1.4761 - val_accuracy: 0.7567 - val_loss: 1.5195\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7579 - loss: 1.4824 - val_accuracy: 0.7667 - val_loss: 1.4528\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7778 - loss: 1.4196 - val_accuracy: 0.7054 - val_loss: 1.6511\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7672 - loss: 1.4590 - val_accuracy: 0.6525 - val_loss: 1.7723\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7736 - loss: 1.4358 - val_accuracy: 0.7246 - val_loss: 1.6834\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7783 - loss: 1.3985 - val_accuracy: 0.7496 - val_loss: 1.4906\n",
            "Model Validation Accuracy: 0.7971\n",
            "\n",
            "Testing Model with 10 layers, 288 neurons, L2=0.0015\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 773ms/step - accuracy: 0.1197 - loss: 6.7132 - val_accuracy: 0.0867 - val_loss: 5.8747\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.1446 - loss: 5.5692 - val_accuracy: 0.1004 - val_loss: 5.0136\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.2036 - loss: 4.5089 - val_accuracy: 0.1238 - val_loss: 4.1911\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2561 - loss: 3.6741 - val_accuracy: 0.1879 - val_loss: 3.4601\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.2895 - loss: 3.1021 - val_accuracy: 0.2117 - val_loss: 2.9737\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3330 - loss: 2.6802 - val_accuracy: 0.2637 - val_loss: 2.7602\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3812 - loss: 2.4247 - val_accuracy: 0.3046 - val_loss: 3.0853\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.4115 - loss: 2.3035 - val_accuracy: 0.3438 - val_loss: 2.9674\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.4585 - loss: 2.1583 - val_accuracy: 0.4104 - val_loss: 2.2924\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4792 - loss: 2.1046 - val_accuracy: 0.3613 - val_loss: 3.5314\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.4942 - loss: 2.1553 - val_accuracy: 0.4883 - val_loss: 2.1956\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.5360 - loss: 2.0770 - val_accuracy: 0.4917 - val_loss: 2.3881\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5621 - loss: 2.0867 - val_accuracy: 0.4842 - val_loss: 2.2646\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5580 - loss: 2.0777 - val_accuracy: 0.3542 - val_loss: 3.1565\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.5600 - loss: 2.1042 - val_accuracy: 0.5400 - val_loss: 2.1371\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.5690 - loss: 2.1230 - val_accuracy: 0.5629 - val_loss: 2.1188\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6198 - loss: 2.0112 - val_accuracy: 0.5008 - val_loss: 2.4176\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6062 - loss: 2.0542 - val_accuracy: 0.5633 - val_loss: 2.2249\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6218 - loss: 1.9985 - val_accuracy: 0.5546 - val_loss: 2.1580\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6239 - loss: 2.0186 - val_accuracy: 0.3313 - val_loss: 3.5400\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6153 - loss: 2.0270 - val_accuracy: 0.5587 - val_loss: 2.0832\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6428 - loss: 1.9657 - val_accuracy: 0.4733 - val_loss: 2.7040\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.6080 - loss: 2.0744 - val_accuracy: 0.5888 - val_loss: 2.0960\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6316 - loss: 1.9647 - val_accuracy: 0.4712 - val_loss: 2.4785\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.6104 - loss: 2.0929 - val_accuracy: 0.6575 - val_loss: 1.8846\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6336 - loss: 1.9754 - val_accuracy: 0.5046 - val_loss: 2.3353\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6371 - loss: 1.9912 - val_accuracy: 0.4854 - val_loss: 2.4947\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6435 - loss: 1.9705 - val_accuracy: 0.4963 - val_loss: 2.4764\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6514 - loss: 2.0129 - val_accuracy: 0.3242 - val_loss: 3.6882\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6548 - loss: 1.9913 - val_accuracy: 0.4567 - val_loss: 2.5233\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6342 - loss: 1.9972 - val_accuracy: 0.5683 - val_loss: 2.3318\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6483 - loss: 1.9954 - val_accuracy: 0.4546 - val_loss: 3.1489\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6637 - loss: 1.9903 - val_accuracy: 0.5246 - val_loss: 2.6652\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6660 - loss: 1.9717 - val_accuracy: 0.5708 - val_loss: 2.1634\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6499 - loss: 1.9834 - val_accuracy: 0.5267 - val_loss: 2.3008\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6654 - loss: 1.9176 - val_accuracy: 0.6142 - val_loss: 2.0926\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6555 - loss: 1.9346 - val_accuracy: 0.4804 - val_loss: 2.6191\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6521 - loss: 1.9675 - val_accuracy: 0.5758 - val_loss: 2.1871\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6593 - loss: 1.9264 - val_accuracy: 0.5875 - val_loss: 2.2149\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6585 - loss: 1.9252 - val_accuracy: 0.5921 - val_loss: 2.0974\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6515 - loss: 1.9214 - val_accuracy: 0.5704 - val_loss: 2.2403\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6559 - loss: 1.9287 - val_accuracy: 0.3950 - val_loss: 3.3228\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6679 - loss: 1.9343 - val_accuracy: 0.5996 - val_loss: 2.2655\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6754 - loss: 1.8965 - val_accuracy: 0.6067 - val_loss: 2.0731\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6643 - loss: 1.9156 - val_accuracy: 0.6408 - val_loss: 1.9662\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6733 - loss: 1.8817 - val_accuracy: 0.6488 - val_loss: 1.9424\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6805 - loss: 1.8712 - val_accuracy: 0.6575 - val_loss: 1.9448\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6825 - loss: 1.8440 - val_accuracy: 0.6129 - val_loss: 2.0247\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.6857 - loss: 1.8632 - val_accuracy: 0.6992 - val_loss: 1.7590\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6861 - loss: 1.8434 - val_accuracy: 0.6104 - val_loss: 2.0433\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6881 - loss: 1.8880 - val_accuracy: 0.5700 - val_loss: 2.6830\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7002 - loss: 1.8288 - val_accuracy: 0.3817 - val_loss: 3.9101\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6823 - loss: 1.9093 - val_accuracy: 0.4950 - val_loss: 2.8979\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6715 - loss: 1.9209 - val_accuracy: 0.3933 - val_loss: 3.9485\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6839 - loss: 1.8535 - val_accuracy: 0.6821 - val_loss: 1.9140\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6700 - loss: 1.9106 - val_accuracy: 0.6629 - val_loss: 1.9910\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7081 - loss: 1.8421 - val_accuracy: 0.3200 - val_loss: 4.0900\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6982 - loss: 1.8581 - val_accuracy: 0.6321 - val_loss: 1.9968\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7021 - loss: 1.8161 - val_accuracy: 0.5096 - val_loss: 2.5578\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7009 - loss: 1.8450 - val_accuracy: 0.5621 - val_loss: 2.3291\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7100 - loss: 1.7600 - val_accuracy: 0.3933 - val_loss: 3.0320\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6951 - loss: 1.8384 - val_accuracy: 0.6733 - val_loss: 1.9121\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6990 - loss: 1.7768 - val_accuracy: 0.6271 - val_loss: 1.9625\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7053 - loss: 1.7908 - val_accuracy: 0.5896 - val_loss: 2.1887\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.6939 - loss: 1.8091 - val_accuracy: 0.7221 - val_loss: 1.6975\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7002 - loss: 1.7934 - val_accuracy: 0.3046 - val_loss: 4.9702\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7001 - loss: 1.7865 - val_accuracy: 0.6950 - val_loss: 1.7893\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6897 - loss: 1.8417 - val_accuracy: 0.6737 - val_loss: 1.8899\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7202 - loss: 1.7349 - val_accuracy: 0.6408 - val_loss: 1.9156\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7114 - loss: 1.7446 - val_accuracy: 0.3804 - val_loss: 5.1771\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7127 - loss: 1.7455 - val_accuracy: 0.5575 - val_loss: 2.4930\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7066 - loss: 1.7652 - val_accuracy: 0.5983 - val_loss: 2.0357\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7179 - loss: 1.7567 - val_accuracy: 0.4575 - val_loss: 3.3389\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7185 - loss: 1.7536 - val_accuracy: 0.7096 - val_loss: 1.7515\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7313 - loss: 1.7016 - val_accuracy: 0.7192 - val_loss: 1.7571\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7302 - loss: 1.7339 - val_accuracy: 0.7083 - val_loss: 1.8261\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7141 - loss: 1.7509 - val_accuracy: 0.6033 - val_loss: 2.1397\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7280 - loss: 1.7378 - val_accuracy: 0.5921 - val_loss: 2.3252\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7143 - loss: 1.7679 - val_accuracy: 0.4583 - val_loss: 2.8415\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7265 - loss: 1.7520 - val_accuracy: 0.4900 - val_loss: 2.9243\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7193 - loss: 1.7178 - val_accuracy: 0.7592 - val_loss: 1.6053\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7288 - loss: 1.7175 - val_accuracy: 0.7138 - val_loss: 1.7034\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7153 - loss: 1.7229 - val_accuracy: 0.6787 - val_loss: 1.8921\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7226 - loss: 1.7432 - val_accuracy: 0.6587 - val_loss: 1.9246\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7361 - loss: 1.6451 - val_accuracy: 0.6888 - val_loss: 1.8637\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7365 - loss: 1.6987 - val_accuracy: 0.7125 - val_loss: 1.7292\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7370 - loss: 1.6409 - val_accuracy: 0.7179 - val_loss: 1.6568\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7382 - loss: 1.6402 - val_accuracy: 0.4200 - val_loss: 3.7819\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7163 - loss: 1.7312 - val_accuracy: 0.6592 - val_loss: 1.9395\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7364 - loss: 1.6586 - val_accuracy: 0.4263 - val_loss: 4.5502\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7299 - loss: 1.7108 - val_accuracy: 0.5596 - val_loss: 2.5052\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7503 - loss: 1.6492 - val_accuracy: 0.6296 - val_loss: 1.9042\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7586 - loss: 1.5677 - val_accuracy: 0.6208 - val_loss: 1.9523\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7460 - loss: 1.6013 - val_accuracy: 0.7246 - val_loss: 1.6148\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7481 - loss: 1.6195 - val_accuracy: 0.5675 - val_loss: 2.6784\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7507 - loss: 1.6462 - val_accuracy: 0.7446 - val_loss: 1.6296\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7602 - loss: 1.5596 - val_accuracy: 0.7038 - val_loss: 1.7420\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7401 - loss: 1.6444 - val_accuracy: 0.5629 - val_loss: 2.1984\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7404 - loss: 1.6203 - val_accuracy: 0.7104 - val_loss: 1.7374\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7532 - loss: 1.6030 - val_accuracy: 0.5408 - val_loss: 2.0956\n",
            "Model Validation Accuracy: 0.7592\n",
            "\n",
            "Testing Model with 8 layers, 320 neurons, L2=0.0005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 599ms/step - accuracy: 0.1863 - loss: 3.6143 - val_accuracy: 0.1225 - val_loss: 4.1810\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.3532 - loss: 2.9399 - val_accuracy: 0.1737 - val_loss: 3.4929\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.4692 - loss: 2.5297 - val_accuracy: 0.1787 - val_loss: 3.3032\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.5078 - loss: 2.2818 - val_accuracy: 0.2150 - val_loss: 3.0529\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.5595 - loss: 2.0544 - val_accuracy: 0.3467 - val_loss: 2.4847\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.5840 - loss: 1.8948 - val_accuracy: 0.5833 - val_loss: 1.8561\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6257 - loss: 1.7223 - val_accuracy: 0.5746 - val_loss: 1.8517\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6331 - loss: 1.6836 - val_accuracy: 0.3671 - val_loss: 3.1747\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.6466 - loss: 1.6100 - val_accuracy: 0.6363 - val_loss: 1.6680\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6589 - loss: 1.5762 - val_accuracy: 0.5846 - val_loss: 2.2635\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6746 - loss: 1.4961 - val_accuracy: 0.6050 - val_loss: 1.7796\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.6734 - loss: 1.4972 - val_accuracy: 0.6558 - val_loss: 1.7837\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.6795 - loss: 1.4751 - val_accuracy: 0.6921 - val_loss: 1.5199\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6989 - loss: 1.4502 - val_accuracy: 0.6608 - val_loss: 1.6149\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7140 - loss: 1.4206 - val_accuracy: 0.5250 - val_loss: 2.4230\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6985 - loss: 1.4607 - val_accuracy: 0.6158 - val_loss: 1.9306\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7069 - loss: 1.4163 - val_accuracy: 0.6325 - val_loss: 1.6462\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7248 - loss: 1.3757 - val_accuracy: 0.5567 - val_loss: 2.5448\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7259 - loss: 1.3848 - val_accuracy: 0.4979 - val_loss: 2.7733\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7208 - loss: 1.4138 - val_accuracy: 0.6817 - val_loss: 1.6544\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7264 - loss: 1.3886 - val_accuracy: 0.6267 - val_loss: 1.7143\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.7339 - loss: 1.3812 - val_accuracy: 0.7525 - val_loss: 1.3851\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7216 - loss: 1.4366 - val_accuracy: 0.7108 - val_loss: 1.4954\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7508 - loss: 1.3338 - val_accuracy: 0.7125 - val_loss: 1.5619\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7452 - loss: 1.3917 - val_accuracy: 0.6458 - val_loss: 1.6689\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7421 - loss: 1.3879 - val_accuracy: 0.6292 - val_loss: 1.8096\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7393 - loss: 1.4114 - val_accuracy: 0.6804 - val_loss: 1.6193\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7675 - loss: 1.3413 - val_accuracy: 0.6338 - val_loss: 1.9221\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.7539 - loss: 1.3715 - val_accuracy: 0.7742 - val_loss: 1.3252\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7609 - loss: 1.3730 - val_accuracy: 0.5504 - val_loss: 2.4210\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7472 - loss: 1.4046 - val_accuracy: 0.7029 - val_loss: 1.5942\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7559 - loss: 1.4035 - val_accuracy: 0.7150 - val_loss: 1.4701\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7573 - loss: 1.4267 - val_accuracy: 0.7346 - val_loss: 1.4411\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7708 - loss: 1.3806 - val_accuracy: 0.5654 - val_loss: 2.2159\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7546 - loss: 1.3956 - val_accuracy: 0.6929 - val_loss: 1.6109\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7581 - loss: 1.4078 - val_accuracy: 0.7362 - val_loss: 1.5232\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7824 - loss: 1.3208 - val_accuracy: 0.7008 - val_loss: 1.6010\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7634 - loss: 1.3877 - val_accuracy: 0.6958 - val_loss: 1.6572\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7712 - loss: 1.3757 - val_accuracy: 0.7375 - val_loss: 1.4575\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7785 - loss: 1.3478 - val_accuracy: 0.7487 - val_loss: 1.4843\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7735 - loss: 1.3649 - val_accuracy: 0.7154 - val_loss: 1.5913\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7789 - loss: 1.3555 - val_accuracy: 0.8062 - val_loss: 1.3093\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7887 - loss: 1.3145 - val_accuracy: 0.7271 - val_loss: 1.4865\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7797 - loss: 1.3469 - val_accuracy: 0.7437 - val_loss: 1.5272\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7959 - loss: 1.3374 - val_accuracy: 0.5929 - val_loss: 2.2970\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7745 - loss: 1.3613 - val_accuracy: 0.7879 - val_loss: 1.3618\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7790 - loss: 1.3660 - val_accuracy: 0.7563 - val_loss: 1.3960\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7919 - loss: 1.3080 - val_accuracy: 0.7221 - val_loss: 1.5696\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7890 - loss: 1.3028 - val_accuracy: 0.7400 - val_loss: 1.5305\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7971 - loss: 1.3103 - val_accuracy: 0.7117 - val_loss: 1.6019\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7803 - loss: 1.3488 - val_accuracy: 0.7779 - val_loss: 1.3562\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7981 - loss: 1.2913 - val_accuracy: 0.7517 - val_loss: 1.4801\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8064 - loss: 1.2785 - val_accuracy: 0.7200 - val_loss: 1.5323\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7830 - loss: 1.3443 - val_accuracy: 0.6254 - val_loss: 2.0331\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7820 - loss: 1.3277 - val_accuracy: 0.7475 - val_loss: 1.5053\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7858 - loss: 1.3436 - val_accuracy: 0.7129 - val_loss: 1.7435\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8058 - loss: 1.2997 - val_accuracy: 0.7133 - val_loss: 1.5914\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8188 - loss: 1.2591 - val_accuracy: 0.7992 - val_loss: 1.2969\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8110 - loss: 1.2778 - val_accuracy: 0.8146 - val_loss: 1.2517\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7974 - loss: 1.2948 - val_accuracy: 0.6963 - val_loss: 1.5835\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8032 - loss: 1.2986 - val_accuracy: 0.7883 - val_loss: 1.3231\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8178 - loss: 1.2429 - val_accuracy: 0.7950 - val_loss: 1.3006\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8130 - loss: 1.2443 - val_accuracy: 0.7750 - val_loss: 1.3927\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8109 - loss: 1.2537 - val_accuracy: 0.7058 - val_loss: 1.9143\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8104 - loss: 1.2745 - val_accuracy: 0.7804 - val_loss: 1.3865\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8081 - loss: 1.2653 - val_accuracy: 0.7892 - val_loss: 1.3840\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8077 - loss: 1.2905 - val_accuracy: 0.7096 - val_loss: 1.6562\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8037 - loss: 1.2986 - val_accuracy: 0.6725 - val_loss: 1.7980\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8196 - loss: 1.2814 - val_accuracy: 0.7421 - val_loss: 1.4324\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8033 - loss: 1.2647 - val_accuracy: 0.7908 - val_loss: 1.3436\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8105 - loss: 1.2623 - val_accuracy: 0.7296 - val_loss: 1.5952\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8041 - loss: 1.2894 - val_accuracy: 0.5813 - val_loss: 2.6474\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8082 - loss: 1.2788 - val_accuracy: 0.8058 - val_loss: 1.3009\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8215 - loss: 1.2328 - val_accuracy: 0.7467 - val_loss: 1.5133\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8170 - loss: 1.2310 - val_accuracy: 0.7887 - val_loss: 1.3372\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8283 - loss: 1.2075 - val_accuracy: 0.6633 - val_loss: 1.9846\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8152 - loss: 1.2507 - val_accuracy: 0.8083 - val_loss: 1.2777\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8344 - loss: 1.2071 - val_accuracy: 0.7433 - val_loss: 1.4316\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8160 - loss: 1.2468 - val_accuracy: 0.7967 - val_loss: 1.2830\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8110 - loss: 1.2295 - val_accuracy: 0.8012 - val_loss: 1.2775\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8226 - loss: 1.2096 - val_accuracy: 0.7621 - val_loss: 1.4295\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8168 - loss: 1.1980 - val_accuracy: 0.7542 - val_loss: 1.5015\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8210 - loss: 1.2467 - val_accuracy: 0.6542 - val_loss: 2.0665\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8229 - loss: 1.2116 - val_accuracy: 0.8046 - val_loss: 1.2837\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.8279 - loss: 1.2069 - val_accuracy: 0.7967 - val_loss: 1.2660\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8285 - loss: 1.1789 - val_accuracy: 0.7437 - val_loss: 1.4440\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8317 - loss: 1.1749 - val_accuracy: 0.7396 - val_loss: 1.5289\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.8326 - loss: 1.1793 - val_accuracy: 0.8154 - val_loss: 1.2430\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.8247 - loss: 1.1885 - val_accuracy: 0.8225 - val_loss: 1.2099\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8215 - loss: 1.2226 - val_accuracy: 0.7971 - val_loss: 1.3016\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8286 - loss: 1.1918 - val_accuracy: 0.7588 - val_loss: 1.4434\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8485 - loss: 1.1570 - val_accuracy: 0.7596 - val_loss: 1.3992\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8297 - loss: 1.1741 - val_accuracy: 0.7096 - val_loss: 1.5888\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8245 - loss: 1.2175 - val_accuracy: 0.7642 - val_loss: 1.4230\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8365 - loss: 1.1513 - val_accuracy: 0.8333 - val_loss: 1.1669\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8363 - loss: 1.1774 - val_accuracy: 0.7271 - val_loss: 1.7088\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8331 - loss: 1.1526 - val_accuracy: 0.7796 - val_loss: 1.3402\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8508 - loss: 1.1156 - val_accuracy: 0.5450 - val_loss: 2.9687\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8381 - loss: 1.1542 - val_accuracy: 0.7254 - val_loss: 1.6230\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8366 - loss: 1.1534 - val_accuracy: 0.8033 - val_loss: 1.3010\n",
            "Model Validation Accuracy: 0.8333\n",
            "Best Validation Accuracy: 0.8333\n",
            "Best Model: <Sequential name=sequential_3, built=True>\n",
            "Best Hyperparameters: Layers=8, Neurons=320, L2=0.0005\n",
            "\n",
            "Testing Model with 12 layers, 320 neurons, L2=0.001\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 720ms/step - accuracy: 0.1236 - loss: 5.8346 - val_accuracy: 0.0737 - val_loss: 5.1280\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.2051 - loss: 4.8411 - val_accuracy: 0.1508 - val_loss: 4.4107\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.2686 - loss: 4.0393 - val_accuracy: 0.1400 - val_loss: 3.8343\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.3172 - loss: 3.4052 - val_accuracy: 0.1525 - val_loss: 3.6527\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.3571 - loss: 2.9424 - val_accuracy: 0.3196 - val_loss: 3.2501\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.3969 - loss: 2.6286 - val_accuracy: 0.4379 - val_loss: 2.4167\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4340 - loss: 2.3868 - val_accuracy: 0.2421 - val_loss: 4.5112\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.4611 - loss: 2.2428 - val_accuracy: 0.4508 - val_loss: 2.5065\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5024 - loss: 2.1177 - val_accuracy: 0.3871 - val_loss: 2.5821\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.5222 - loss: 2.0552 - val_accuracy: 0.4750 - val_loss: 2.1815\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.5472 - loss: 1.9934 - val_accuracy: 0.5738 - val_loss: 2.0549\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.5395 - loss: 2.0436 - val_accuracy: 0.4371 - val_loss: 2.2923\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5686 - loss: 1.9731 - val_accuracy: 0.4729 - val_loss: 2.3035\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5647 - loss: 2.0108 - val_accuracy: 0.4375 - val_loss: 2.5211\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6065 - loss: 1.9661 - val_accuracy: 0.4646 - val_loss: 2.5685\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6055 - loss: 1.9668 - val_accuracy: 0.5254 - val_loss: 2.2211\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.6022 - loss: 1.9737 - val_accuracy: 0.5867 - val_loss: 2.0021\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.6291 - loss: 1.9285 - val_accuracy: 0.4704 - val_loss: 2.6344\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6254 - loss: 1.9160 - val_accuracy: 0.6325 - val_loss: 1.8941\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6326 - loss: 1.9351 - val_accuracy: 0.5767 - val_loss: 2.0804\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6448 - loss: 1.9167 - val_accuracy: 0.5813 - val_loss: 2.1646\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6515 - loss: 1.9069 - val_accuracy: 0.4721 - val_loss: 2.5129\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.6491 - loss: 1.8876 - val_accuracy: 0.6587 - val_loss: 1.8720\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6446 - loss: 1.9114 - val_accuracy: 0.2979 - val_loss: 3.5469\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6516 - loss: 1.8995 - val_accuracy: 0.5387 - val_loss: 2.2074\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6360 - loss: 1.9511 - val_accuracy: 0.2954 - val_loss: 4.2043\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6405 - loss: 1.9378 - val_accuracy: 0.6471 - val_loss: 1.9403\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6453 - loss: 1.9430 - val_accuracy: 0.5246 - val_loss: 2.4206\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6522 - loss: 1.8987 - val_accuracy: 0.5379 - val_loss: 2.3069\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6430 - loss: 1.9188 - val_accuracy: 0.6804 - val_loss: 1.8504\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6690 - loss: 1.8834 - val_accuracy: 0.5537 - val_loss: 2.6570\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6580 - loss: 1.9260 - val_accuracy: 0.3583 - val_loss: 3.0972\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6659 - loss: 1.9014 - val_accuracy: 0.6146 - val_loss: 2.1355\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6651 - loss: 1.9014 - val_accuracy: 0.5808 - val_loss: 2.1436\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6800 - loss: 1.8643 - val_accuracy: 0.6142 - val_loss: 2.1296\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6728 - loss: 1.8807 - val_accuracy: 0.4967 - val_loss: 2.6697\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6567 - loss: 1.8960 - val_accuracy: 0.5400 - val_loss: 2.5394\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6797 - loss: 1.8679 - val_accuracy: 0.4829 - val_loss: 2.4753\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6714 - loss: 1.8985 - val_accuracy: 0.5083 - val_loss: 2.5963\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6650 - loss: 1.8979 - val_accuracy: 0.5808 - val_loss: 2.2580\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6901 - loss: 1.8542 - val_accuracy: 0.5900 - val_loss: 2.0458\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6725 - loss: 1.8722 - val_accuracy: 0.5471 - val_loss: 2.3342\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6913 - loss: 1.8484 - val_accuracy: 0.6521 - val_loss: 1.9344\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6879 - loss: 1.9066 - val_accuracy: 0.5700 - val_loss: 2.4373\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6946 - loss: 1.8675 - val_accuracy: 0.6092 - val_loss: 2.0581\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6831 - loss: 1.8960 - val_accuracy: 0.6037 - val_loss: 2.2679\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6942 - loss: 1.8511 - val_accuracy: 0.6750 - val_loss: 1.8904\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6887 - loss: 1.8847 - val_accuracy: 0.5517 - val_loss: 2.2640\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6977 - loss: 1.8154 - val_accuracy: 0.6675 - val_loss: 2.0586\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6875 - loss: 1.9237 - val_accuracy: 0.5854 - val_loss: 2.1726\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7003 - loss: 1.8273 - val_accuracy: 0.5692 - val_loss: 2.2051\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7063 - loss: 1.8383 - val_accuracy: 0.4946 - val_loss: 2.8196\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.6974 - loss: 1.8446 - val_accuracy: 0.6888 - val_loss: 1.8637\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6910 - loss: 1.8327 - val_accuracy: 0.6879 - val_loss: 1.8490\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6983 - loss: 1.8069 - val_accuracy: 0.6708 - val_loss: 1.8646\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7037 - loss: 1.7828 - val_accuracy: 0.3121 - val_loss: 3.5784\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6856 - loss: 1.8795 - val_accuracy: 0.6329 - val_loss: 2.0495\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6970 - loss: 1.8083 - val_accuracy: 0.4554 - val_loss: 2.6759\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7101 - loss: 1.7865 - val_accuracy: 0.6283 - val_loss: 2.0227\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6993 - loss: 1.8121 - val_accuracy: 0.3583 - val_loss: 5.1900\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7070 - loss: 1.7979 - val_accuracy: 0.6883 - val_loss: 1.8160\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7021 - loss: 1.7848 - val_accuracy: 0.6417 - val_loss: 2.0065\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.7069 - loss: 1.7581 - val_accuracy: 0.7163 - val_loss: 1.7559\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6990 - loss: 1.8187 - val_accuracy: 0.6029 - val_loss: 2.1738\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7179 - loss: 1.7411 - val_accuracy: 0.6954 - val_loss: 1.7977\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7208 - loss: 1.7311 - val_accuracy: 0.6542 - val_loss: 2.0038\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7178 - loss: 1.7131 - val_accuracy: 0.5088 - val_loss: 2.3101\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7156 - loss: 1.7452 - val_accuracy: 0.5038 - val_loss: 2.7316\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7176 - loss: 1.7385 - val_accuracy: 0.6529 - val_loss: 1.9110\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7199 - loss: 1.7373 - val_accuracy: 0.7067 - val_loss: 1.7934\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7191 - loss: 1.7558 - val_accuracy: 0.5579 - val_loss: 2.2114\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7153 - loss: 1.7656 - val_accuracy: 0.6217 - val_loss: 1.9756\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.7003 - loss: 1.8143 - val_accuracy: 0.7254 - val_loss: 1.7699\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7333 - loss: 1.7000 - val_accuracy: 0.5838 - val_loss: 2.1451\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7339 - loss: 1.7038 - val_accuracy: 0.5696 - val_loss: 2.1161\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7332 - loss: 1.6666 - val_accuracy: 0.5704 - val_loss: 2.3096\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7273 - loss: 1.7001 - val_accuracy: 0.6358 - val_loss: 1.9772\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7330 - loss: 1.7007 - val_accuracy: 0.7242 - val_loss: 1.7675\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7287 - loss: 1.7150 - val_accuracy: 0.5575 - val_loss: 2.4809\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7447 - loss: 1.6642 - val_accuracy: 0.6263 - val_loss: 2.0685\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7445 - loss: 1.6807 - val_accuracy: 0.6596 - val_loss: 2.0630\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7212 - loss: 1.6871 - val_accuracy: 0.6529 - val_loss: 1.8516\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7275 - loss: 1.6874 - val_accuracy: 0.6671 - val_loss: 2.0232\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7243 - loss: 1.7281 - val_accuracy: 0.6450 - val_loss: 1.8848\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7523 - loss: 1.6373 - val_accuracy: 0.6221 - val_loss: 2.0653\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7328 - loss: 1.6944 - val_accuracy: 0.6829 - val_loss: 1.8495\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7437 - loss: 1.6774 - val_accuracy: 0.6187 - val_loss: 2.0584\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.7336 - loss: 1.7043 - val_accuracy: 0.7887 - val_loss: 1.5036\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7506 - loss: 1.6210 - val_accuracy: 0.5617 - val_loss: 2.6692\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7386 - loss: 1.6763 - val_accuracy: 0.6900 - val_loss: 1.7956\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7440 - loss: 1.6195 - val_accuracy: 0.6754 - val_loss: 1.8818\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7330 - loss: 1.6544 - val_accuracy: 0.6367 - val_loss: 2.0559\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7556 - loss: 1.6205 - val_accuracy: 0.6792 - val_loss: 1.8129\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.7285 - loss: 1.6751 - val_accuracy: 0.7996 - val_loss: 1.4547\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7693 - loss: 1.5639 - val_accuracy: 0.6817 - val_loss: 1.7923\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7568 - loss: 1.6086 - val_accuracy: 0.5821 - val_loss: 2.0735\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7628 - loss: 1.5709 - val_accuracy: 0.6637 - val_loss: 1.9767\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7614 - loss: 1.5786 - val_accuracy: 0.6117 - val_loss: 2.3835\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7616 - loss: 1.6175 - val_accuracy: 0.6092 - val_loss: 1.9987\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7628 - loss: 1.5827 - val_accuracy: 0.7337 - val_loss: 1.6775\n",
            "Model Validation Accuracy: 0.7996\n",
            "\n",
            "Testing Model with 6 layers, 288 neurons, L2=0.001\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 617ms/step - accuracy: 0.1867 - loss: 3.7788 - val_accuracy: 0.1688 - val_loss: 3.7279\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.3571 - loss: 2.8832 - val_accuracy: 0.1817 - val_loss: 3.1035\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4626 - loss: 2.2647 - val_accuracy: 0.1675 - val_loss: 3.0920\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5023 - loss: 1.9914 - val_accuracy: 0.1829 - val_loss: 2.8208\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5517 - loss: 1.7763 - val_accuracy: 0.3546 - val_loss: 2.2252\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5875 - loss: 1.6560 - val_accuracy: 0.2675 - val_loss: 2.5730\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6095 - loss: 1.5725 - val_accuracy: 0.2092 - val_loss: 4.1133\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6348 - loss: 1.5222 - val_accuracy: 0.5879 - val_loss: 1.6715\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6422 - loss: 1.4807 - val_accuracy: 0.5958 - val_loss: 1.7166\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6493 - loss: 1.4344 - val_accuracy: 0.2896 - val_loss: 3.6724\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6507 - loss: 1.5333 - val_accuracy: 0.5650 - val_loss: 1.7637\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6694 - loss: 1.4333 - val_accuracy: 0.5225 - val_loss: 1.9901\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6744 - loss: 1.4168 - val_accuracy: 0.3883 - val_loss: 2.7186\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6880 - loss: 1.4050 - val_accuracy: 0.4608 - val_loss: 2.3985\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6803 - loss: 1.3836 - val_accuracy: 0.5567 - val_loss: 2.3172\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6637 - loss: 1.4416 - val_accuracy: 0.5633 - val_loss: 1.8105\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7027 - loss: 1.3461 - val_accuracy: 0.5300 - val_loss: 2.1560\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.7107 - loss: 1.3445 - val_accuracy: 0.6417 - val_loss: 1.5398\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7043 - loss: 1.3492 - val_accuracy: 0.5825 - val_loss: 1.9973\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7157 - loss: 1.3274 - val_accuracy: 0.4988 - val_loss: 2.2022\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7013 - loss: 1.3635 - val_accuracy: 0.6233 - val_loss: 1.8261\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.7144 - loss: 1.3558 - val_accuracy: 0.6833 - val_loss: 1.4527\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7313 - loss: 1.3365 - val_accuracy: 0.6750 - val_loss: 1.5141\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7383 - loss: 1.2888 - val_accuracy: 0.7196 - val_loss: 1.3251\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7333 - loss: 1.3233 - val_accuracy: 0.5100 - val_loss: 3.0114\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7408 - loss: 1.2976 - val_accuracy: 0.5608 - val_loss: 2.0079\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7473 - loss: 1.2958 - val_accuracy: 0.6496 - val_loss: 1.6894\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7374 - loss: 1.2812 - val_accuracy: 0.7163 - val_loss: 1.4143\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7227 - loss: 1.3338 - val_accuracy: 0.5225 - val_loss: 2.4412\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7475 - loss: 1.2549 - val_accuracy: 0.5533 - val_loss: 2.0028\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7451 - loss: 1.2872 - val_accuracy: 0.6879 - val_loss: 1.4339\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7503 - loss: 1.2666 - val_accuracy: 0.6617 - val_loss: 1.5564\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7410 - loss: 1.2768 - val_accuracy: 0.5242 - val_loss: 2.7029\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7481 - loss: 1.2905 - val_accuracy: 0.6683 - val_loss: 1.6017\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7519 - loss: 1.2793 - val_accuracy: 0.7229 - val_loss: 1.4190\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7444 - loss: 1.2944 - val_accuracy: 0.4646 - val_loss: 2.5800\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7715 - loss: 1.2025 - val_accuracy: 0.5808 - val_loss: 1.9632\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7496 - loss: 1.2566 - val_accuracy: 0.7075 - val_loss: 1.3574\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7628 - loss: 1.2022 - val_accuracy: 0.6471 - val_loss: 1.7058\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.7626 - loss: 1.2321 - val_accuracy: 0.7521 - val_loss: 1.3011\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7634 - loss: 1.2207 - val_accuracy: 0.6104 - val_loss: 1.8044\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7615 - loss: 1.2175 - val_accuracy: 0.6050 - val_loss: 1.8126\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7629 - loss: 1.2623 - val_accuracy: 0.6812 - val_loss: 1.6435\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7706 - loss: 1.2053 - val_accuracy: 0.6812 - val_loss: 1.4503\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7741 - loss: 1.2199 - val_accuracy: 0.6142 - val_loss: 2.2158\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7820 - loss: 1.1786 - val_accuracy: 0.7900 - val_loss: 1.1767\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.7857 - loss: 1.2022 - val_accuracy: 0.7004 - val_loss: 1.4049\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7736 - loss: 1.2089 - val_accuracy: 0.6746 - val_loss: 1.5724\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7864 - loss: 1.2148 - val_accuracy: 0.6600 - val_loss: 1.6065\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7818 - loss: 1.1983 - val_accuracy: 0.6329 - val_loss: 1.6299\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7876 - loss: 1.1525 - val_accuracy: 0.6908 - val_loss: 1.4918\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7771 - loss: 1.2143 - val_accuracy: 0.5808 - val_loss: 1.7475\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7965 - loss: 1.1250 - val_accuracy: 0.5863 - val_loss: 2.0808\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7947 - loss: 1.1597 - val_accuracy: 0.7375 - val_loss: 1.3090\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7744 - loss: 1.1991 - val_accuracy: 0.6883 - val_loss: 1.7849\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7961 - loss: 1.1529 - val_accuracy: 0.6692 - val_loss: 1.5213\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7916 - loss: 1.1608 - val_accuracy: 0.7362 - val_loss: 1.3633\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8189 - loss: 1.1014 - val_accuracy: 0.7458 - val_loss: 1.2826\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8028 - loss: 1.1438 - val_accuracy: 0.6162 - val_loss: 2.1013\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7911 - loss: 1.1756 - val_accuracy: 0.5929 - val_loss: 2.1040\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7825 - loss: 1.1613 - val_accuracy: 0.6771 - val_loss: 1.5812\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8060 - loss: 1.1085 - val_accuracy: 0.6712 - val_loss: 1.5584\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8067 - loss: 1.1043 - val_accuracy: 0.7262 - val_loss: 1.3960\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7990 - loss: 1.1165 - val_accuracy: 0.6458 - val_loss: 1.6750\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7907 - loss: 1.1527 - val_accuracy: 0.7817 - val_loss: 1.2212\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7978 - loss: 1.1205 - val_accuracy: 0.6717 - val_loss: 1.4955\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8005 - loss: 1.1224 - val_accuracy: 0.7579 - val_loss: 1.3290\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8056 - loss: 1.0954 - val_accuracy: 0.7246 - val_loss: 1.6463\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8097 - loss: 1.1115 - val_accuracy: 0.7671 - val_loss: 1.3246\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7943 - loss: 1.1269 - val_accuracy: 0.6967 - val_loss: 1.5041\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8058 - loss: 1.0847 - val_accuracy: 0.7246 - val_loss: 1.4461\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7918 - loss: 1.1453 - val_accuracy: 0.7833 - val_loss: 1.2328\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8158 - loss: 1.0633 - val_accuracy: 0.7604 - val_loss: 1.4067\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8239 - loss: 1.0493 - val_accuracy: 0.6237 - val_loss: 1.9361\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8288 - loss: 1.0605 - val_accuracy: 0.7638 - val_loss: 1.2435\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8156 - loss: 1.0634 - val_accuracy: 0.6858 - val_loss: 1.4824\n",
            "Model Validation Accuracy: 0.7900\n",
            "\n",
            "Testing Model with 10 layers, 320 neurons, L2=0.001\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 674ms/step - accuracy: 0.1091 - loss: 5.8138 - val_accuracy: 0.0867 - val_loss: 5.0894\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.1140 - loss: 4.9450 - val_accuracy: 0.1096 - val_loss: 4.4971\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.1263 - loss: 4.3071 - val_accuracy: 0.0967 - val_loss: 4.0444\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.1799 - loss: 3.6920 - val_accuracy: 0.1004 - val_loss: 3.5737\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.2493 - loss: 3.2163 - val_accuracy: 0.2192 - val_loss: 3.3255\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.3439 - loss: 2.7689 - val_accuracy: 0.3762 - val_loss: 2.7088\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3928 - loss: 2.5014 - val_accuracy: 0.3675 - val_loss: 2.5430\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4294 - loss: 2.3247 - val_accuracy: 0.1675 - val_loss: 3.8227\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.4596 - loss: 2.1912 - val_accuracy: 0.2971 - val_loss: 2.8298\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.4981 - loss: 2.1218 - val_accuracy: 0.4808 - val_loss: 2.1617\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5183 - loss: 2.0511 - val_accuracy: 0.4396 - val_loss: 2.2250\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.5401 - loss: 2.0139 - val_accuracy: 0.5875 - val_loss: 1.8610\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5499 - loss: 1.9793 - val_accuracy: 0.4371 - val_loss: 2.3785\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.5513 - loss: 2.0302 - val_accuracy: 0.4758 - val_loss: 2.4520\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5846 - loss: 1.9594 - val_accuracy: 0.5267 - val_loss: 2.1292\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5983 - loss: 1.9609 - val_accuracy: 0.5700 - val_loss: 2.0073\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6006 - loss: 1.9554 - val_accuracy: 0.3921 - val_loss: 2.8724\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6267 - loss: 1.9171 - val_accuracy: 0.4087 - val_loss: 2.8399\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6362 - loss: 1.9223 - val_accuracy: 0.4638 - val_loss: 2.8331\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6287 - loss: 1.9805 - val_accuracy: 0.5358 - val_loss: 2.3638\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.6347 - loss: 1.8776 - val_accuracy: 0.6046 - val_loss: 1.9867\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6473 - loss: 1.9218 - val_accuracy: 0.4700 - val_loss: 2.3655\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6326 - loss: 1.9588 - val_accuracy: 0.4492 - val_loss: 2.7521\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.6478 - loss: 1.9492 - val_accuracy: 0.6208 - val_loss: 2.0547\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.6499 - loss: 1.9600 - val_accuracy: 0.6375 - val_loss: 1.9444\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6707 - loss: 1.8632 - val_accuracy: 0.5875 - val_loss: 2.0726\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6677 - loss: 1.8847 - val_accuracy: 0.4025 - val_loss: 3.3406\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6739 - loss: 1.8811 - val_accuracy: 0.6363 - val_loss: 1.9720\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6868 - loss: 1.8552 - val_accuracy: 0.5992 - val_loss: 2.1022\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6731 - loss: 1.8943 - val_accuracy: 0.5525 - val_loss: 2.3921\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6564 - loss: 1.9465 - val_accuracy: 0.5713 - val_loss: 2.2076\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6718 - loss: 1.9025 - val_accuracy: 0.7071 - val_loss: 1.8131\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6998 - loss: 1.8086 - val_accuracy: 0.7000 - val_loss: 1.7713\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6781 - loss: 1.8656 - val_accuracy: 0.6229 - val_loss: 2.0551\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6781 - loss: 1.8716 - val_accuracy: 0.5354 - val_loss: 2.4150\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6867 - loss: 1.8404 - val_accuracy: 0.6183 - val_loss: 2.0846\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6849 - loss: 1.8381 - val_accuracy: 0.6358 - val_loss: 1.9483\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6841 - loss: 1.8463 - val_accuracy: 0.6108 - val_loss: 2.1724\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6868 - loss: 1.8840 - val_accuracy: 0.5663 - val_loss: 2.4018\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6966 - loss: 1.8664 - val_accuracy: 0.6533 - val_loss: 1.9113\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6807 - loss: 1.8662 - val_accuracy: 0.6087 - val_loss: 2.0998\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6931 - loss: 1.7898 - val_accuracy: 0.3562 - val_loss: 3.9438\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6887 - loss: 1.8728 - val_accuracy: 0.4708 - val_loss: 3.0802\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7084 - loss: 1.8023 - val_accuracy: 0.5258 - val_loss: 2.8399\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6944 - loss: 1.8841 - val_accuracy: 0.5821 - val_loss: 2.1538\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6943 - loss: 1.8491 - val_accuracy: 0.6017 - val_loss: 2.2030\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7055 - loss: 1.8109 - val_accuracy: 0.6383 - val_loss: 2.0613\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.6967 - loss: 1.8514 - val_accuracy: 0.7208 - val_loss: 1.7035\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7086 - loss: 1.7900 - val_accuracy: 0.6233 - val_loss: 2.1211\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7251 - loss: 1.7786 - val_accuracy: 0.5387 - val_loss: 2.5703\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7024 - loss: 1.8496 - val_accuracy: 0.5725 - val_loss: 2.6640\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7090 - loss: 1.8351 - val_accuracy: 0.4592 - val_loss: 2.8121\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7086 - loss: 1.8241 - val_accuracy: 0.6154 - val_loss: 2.1726\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7025 - loss: 1.8287 - val_accuracy: 0.6025 - val_loss: 2.1553\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7117 - loss: 1.8205 - val_accuracy: 0.6442 - val_loss: 2.0197\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6847 - loss: 1.8381 - val_accuracy: 0.6892 - val_loss: 1.8379\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7107 - loss: 1.7678 - val_accuracy: 0.6929 - val_loss: 1.8787\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7239 - loss: 1.7625 - val_accuracy: 0.6504 - val_loss: 1.9177\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7221 - loss: 1.7467 - val_accuracy: 0.4396 - val_loss: 3.1595\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7333 - loss: 1.7528 - val_accuracy: 0.7250 - val_loss: 1.7373\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7334 - loss: 1.7271 - val_accuracy: 0.6033 - val_loss: 2.1785\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7237 - loss: 1.7099 - val_accuracy: 0.6421 - val_loss: 2.0815\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7449 - loss: 1.7336 - val_accuracy: 0.6692 - val_loss: 1.8501\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7288 - loss: 1.7585 - val_accuracy: 0.6467 - val_loss: 1.9801\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7207 - loss: 1.7297 - val_accuracy: 0.6892 - val_loss: 1.8248\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7335 - loss: 1.7196 - val_accuracy: 0.5283 - val_loss: 2.7668\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7315 - loss: 1.7413 - val_accuracy: 0.6083 - val_loss: 2.1890\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7342 - loss: 1.7284 - val_accuracy: 0.6758 - val_loss: 2.0297\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7326 - loss: 1.7338 - val_accuracy: 0.3646 - val_loss: 5.7019\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7293 - loss: 1.7543 - val_accuracy: 0.7000 - val_loss: 1.8667\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7271 - loss: 1.7518 - val_accuracy: 0.7446 - val_loss: 1.6917\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7428 - loss: 1.6938 - val_accuracy: 0.6187 - val_loss: 2.1236\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7416 - loss: 1.6826 - val_accuracy: 0.6933 - val_loss: 1.7959\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7433 - loss: 1.7000 - val_accuracy: 0.7017 - val_loss: 1.8644\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7527 - loss: 1.6707 - val_accuracy: 0.5604 - val_loss: 2.4235\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7454 - loss: 1.6724 - val_accuracy: 0.6821 - val_loss: 1.9695\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.7353 - loss: 1.6921 - val_accuracy: 0.7571 - val_loss: 1.6353\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7434 - loss: 1.6896 - val_accuracy: 0.6775 - val_loss: 1.8178\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7401 - loss: 1.6563 - val_accuracy: 0.5471 - val_loss: 2.7970\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7513 - loss: 1.6425 - val_accuracy: 0.5337 - val_loss: 2.4423\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7568 - loss: 1.6534 - val_accuracy: 0.5300 - val_loss: 2.5886\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7364 - loss: 1.7113 - val_accuracy: 0.7317 - val_loss: 1.7550\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.7537 - loss: 1.6606 - val_accuracy: 0.7704 - val_loss: 1.6403\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7682 - loss: 1.6266 - val_accuracy: 0.6529 - val_loss: 1.9133\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7499 - loss: 1.6662 - val_accuracy: 0.7300 - val_loss: 1.7172\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7549 - loss: 1.6356 - val_accuracy: 0.7100 - val_loss: 1.7742\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7637 - loss: 1.6069 - val_accuracy: 0.7621 - val_loss: 1.5774\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7451 - loss: 1.6966 - val_accuracy: 0.6413 - val_loss: 2.2785\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7608 - loss: 1.6061 - val_accuracy: 0.6762 - val_loss: 2.0920\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7467 - loss: 1.6377 - val_accuracy: 0.7583 - val_loss: 1.5815\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7689 - loss: 1.5749 - val_accuracy: 0.6796 - val_loss: 1.8531\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7506 - loss: 1.6551 - val_accuracy: 0.6325 - val_loss: 2.2448\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7549 - loss: 1.6330 - val_accuracy: 0.7000 - val_loss: 1.7275\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7725 - loss: 1.5159 - val_accuracy: 0.6104 - val_loss: 1.9706\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7579 - loss: 1.6077 - val_accuracy: 0.6375 - val_loss: 2.4245\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7634 - loss: 1.5871 - val_accuracy: 0.6521 - val_loss: 1.9858\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7623 - loss: 1.6009 - val_accuracy: 0.6879 - val_loss: 1.7774\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7691 - loss: 1.5608 - val_accuracy: 0.6463 - val_loss: 1.9115\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7564 - loss: 1.5619 - val_accuracy: 0.7508 - val_loss: 1.5778\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7731 - loss: 1.5224 - val_accuracy: 0.7858 - val_loss: 1.4816\n",
            "Model Validation Accuracy: 0.7858\n",
            "\n",
            "Testing Model with 8 layers, 320 neurons, L2=0.0005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 601ms/step - accuracy: 0.1770 - loss: 3.6259 - val_accuracy: 0.2017 - val_loss: 3.8854\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3481 - loss: 2.9622 - val_accuracy: 0.1883 - val_loss: 3.5159\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4403 - loss: 2.6393 - val_accuracy: 0.2208 - val_loss: 3.1645\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.4963 - loss: 2.3148 - val_accuracy: 0.2358 - val_loss: 3.0004\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.5503 - loss: 2.0636 - val_accuracy: 0.3692 - val_loss: 2.7547\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.5862 - loss: 1.9128 - val_accuracy: 0.6171 - val_loss: 1.7763\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6191 - loss: 1.7479 - val_accuracy: 0.5546 - val_loss: 1.9461\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6351 - loss: 1.6659 - val_accuracy: 0.5904 - val_loss: 1.8053\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6367 - loss: 1.6435 - val_accuracy: 0.5458 - val_loss: 1.8842\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6748 - loss: 1.5264 - val_accuracy: 0.5163 - val_loss: 2.2082\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.6832 - loss: 1.5229 - val_accuracy: 0.6225 - val_loss: 1.7482\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.6923 - loss: 1.4667 - val_accuracy: 0.7150 - val_loss: 1.3996\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6993 - loss: 1.4130 - val_accuracy: 0.5971 - val_loss: 2.0451\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7066 - loss: 1.4192 - val_accuracy: 0.5692 - val_loss: 2.1521\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7169 - loss: 1.4121 - val_accuracy: 0.6550 - val_loss: 1.6456\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7083 - loss: 1.4361 - val_accuracy: 0.6217 - val_loss: 1.8569\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7214 - loss: 1.4059 - val_accuracy: 0.6471 - val_loss: 1.6198\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7282 - loss: 1.3888 - val_accuracy: 0.6233 - val_loss: 1.8399\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7210 - loss: 1.4277 - val_accuracy: 0.5242 - val_loss: 2.5453\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7385 - loss: 1.3729 - val_accuracy: 0.6650 - val_loss: 1.6037\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7379 - loss: 1.3861 - val_accuracy: 0.5371 - val_loss: 2.3695\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7494 - loss: 1.4032 - val_accuracy: 0.6079 - val_loss: 1.8548\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7450 - loss: 1.4030 - val_accuracy: 0.6879 - val_loss: 1.6814\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7556 - loss: 1.3519 - val_accuracy: 0.5954 - val_loss: 1.9694\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7591 - loss: 1.3495 - val_accuracy: 0.6021 - val_loss: 2.0820\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7620 - loss: 1.3457 - val_accuracy: 0.6562 - val_loss: 1.6295\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7523 - loss: 1.3961 - val_accuracy: 0.7725 - val_loss: 1.3688\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7796 - loss: 1.3371 - val_accuracy: 0.6808 - val_loss: 1.5917\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7618 - loss: 1.3674 - val_accuracy: 0.7492 - val_loss: 1.3933\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7655 - loss: 1.3282 - val_accuracy: 0.6592 - val_loss: 1.6081\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7820 - loss: 1.3071 - val_accuracy: 0.6829 - val_loss: 1.6467\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7633 - loss: 1.3738 - val_accuracy: 0.7079 - val_loss: 1.5961\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7485 - loss: 1.4093 - val_accuracy: 0.6933 - val_loss: 1.7514\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7700 - loss: 1.3581 - val_accuracy: 0.7029 - val_loss: 1.5607\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7743 - loss: 1.3729 - val_accuracy: 0.7392 - val_loss: 1.4715\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7634 - loss: 1.3818 - val_accuracy: 0.7175 - val_loss: 1.5770\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7672 - loss: 1.4044 - val_accuracy: 0.7796 - val_loss: 1.4090\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7776 - loss: 1.3703 - val_accuracy: 0.6646 - val_loss: 1.7420\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7796 - loss: 1.3481 - val_accuracy: 0.7267 - val_loss: 1.5415\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7734 - loss: 1.3950 - val_accuracy: 0.7679 - val_loss: 1.4350\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7953 - loss: 1.3098 - val_accuracy: 0.7233 - val_loss: 1.6145\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7858 - loss: 1.3264 - val_accuracy: 0.7613 - val_loss: 1.5022\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7987 - loss: 1.3292 - val_accuracy: 0.7929 - val_loss: 1.3452\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7815 - loss: 1.3713 - val_accuracy: 0.7558 - val_loss: 1.4194\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7819 - loss: 1.3228 - val_accuracy: 0.6542 - val_loss: 1.8198\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7914 - loss: 1.3424 - val_accuracy: 0.6483 - val_loss: 1.6988\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8008 - loss: 1.3002 - val_accuracy: 0.7554 - val_loss: 1.4235\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7938 - loss: 1.3274 - val_accuracy: 0.7671 - val_loss: 1.4638\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7899 - loss: 1.3252 - val_accuracy: 0.7829 - val_loss: 1.3710\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7991 - loss: 1.2938 - val_accuracy: 0.7683 - val_loss: 1.4084\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8003 - loss: 1.3215 - val_accuracy: 0.7875 - val_loss: 1.3358\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7980 - loss: 1.3045 - val_accuracy: 0.6675 - val_loss: 1.9277\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7788 - loss: 1.3634 - val_accuracy: 0.7471 - val_loss: 1.5190\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8047 - loss: 1.3006 - val_accuracy: 0.7333 - val_loss: 1.5433\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7986 - loss: 1.3227 - val_accuracy: 0.7517 - val_loss: 1.4871\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.8011 - loss: 1.3202 - val_accuracy: 0.7300 - val_loss: 1.6206\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.8102 - loss: 1.2970 - val_accuracy: 0.8017 - val_loss: 1.3865\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8060 - loss: 1.2822 - val_accuracy: 0.6825 - val_loss: 1.9947\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8102 - loss: 1.3085 - val_accuracy: 0.7471 - val_loss: 1.4350\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8258 - loss: 1.2531 - val_accuracy: 0.7958 - val_loss: 1.3230\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8179 - loss: 1.2734 - val_accuracy: 0.7571 - val_loss: 1.5360\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8074 - loss: 1.2710 - val_accuracy: 0.6500 - val_loss: 2.0137\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7846 - loss: 1.3346 - val_accuracy: 0.7867 - val_loss: 1.3678\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8187 - loss: 1.2403 - val_accuracy: 0.6733 - val_loss: 1.8811\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7993 - loss: 1.2989 - val_accuracy: 0.7683 - val_loss: 1.4036\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8157 - loss: 1.2605 - val_accuracy: 0.7517 - val_loss: 1.6130\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7971 - loss: 1.3121 - val_accuracy: 0.7892 - val_loss: 1.4153\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8162 - loss: 1.2728 - val_accuracy: 0.7496 - val_loss: 1.5142\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7985 - loss: 1.3082 - val_accuracy: 0.8008 - val_loss: 1.3266\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.8297 - loss: 1.2245 - val_accuracy: 0.8250 - val_loss: 1.2346\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8166 - loss: 1.2530 - val_accuracy: 0.7783 - val_loss: 1.3802\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8205 - loss: 1.2611 - val_accuracy: 0.7692 - val_loss: 1.4516\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8148 - loss: 1.2704 - val_accuracy: 0.7846 - val_loss: 1.3566\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.8267 - loss: 1.2189 - val_accuracy: 0.7013 - val_loss: 1.7154\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8190 - loss: 1.2296 - val_accuracy: 0.7933 - val_loss: 1.3086\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8263 - loss: 1.2130 - val_accuracy: 0.7275 - val_loss: 1.6061\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.8175 - loss: 1.2239 - val_accuracy: 0.8033 - val_loss: 1.3397\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8350 - loss: 1.1957 - val_accuracy: 0.7250 - val_loss: 1.6776\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8175 - loss: 1.2361 - val_accuracy: 0.7350 - val_loss: 1.4915\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8177 - loss: 1.2401 - val_accuracy: 0.7854 - val_loss: 1.3627\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8292 - loss: 1.1781 - val_accuracy: 0.8012 - val_loss: 1.2926\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8345 - loss: 1.1974 - val_accuracy: 0.8054 - val_loss: 1.2719\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8435 - loss: 1.1590 - val_accuracy: 0.7788 - val_loss: 1.4365\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8185 - loss: 1.2349 - val_accuracy: 0.7621 - val_loss: 1.4669\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8288 - loss: 1.1763 - val_accuracy: 0.8146 - val_loss: 1.2583\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8388 - loss: 1.1640 - val_accuracy: 0.7358 - val_loss: 1.6907\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8224 - loss: 1.2238 - val_accuracy: 0.7658 - val_loss: 1.4728\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8256 - loss: 1.1862 - val_accuracy: 0.7325 - val_loss: 1.4943\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8241 - loss: 1.2211 - val_accuracy: 0.8233 - val_loss: 1.2937\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8432 - loss: 1.1731 - val_accuracy: 0.7688 - val_loss: 1.4091\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8416 - loss: 1.1751 - val_accuracy: 0.7471 - val_loss: 1.5320\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.8302 - loss: 1.1908 - val_accuracy: 0.8338 - val_loss: 1.2380\n",
            "Epoch 93/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8333 - loss: 1.1710 - val_accuracy: 0.7867 - val_loss: 1.3918\n",
            "Epoch 94/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.8362 - loss: 1.1617 - val_accuracy: 0.8383 - val_loss: 1.2248\n",
            "Epoch 95/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8305 - loss: 1.1795 - val_accuracy: 0.7642 - val_loss: 1.3993\n",
            "Epoch 96/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8421 - loss: 1.1634 - val_accuracy: 0.8067 - val_loss: 1.2523\n",
            "Epoch 97/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8302 - loss: 1.1727 - val_accuracy: 0.7225 - val_loss: 1.7318\n",
            "Epoch 98/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8416 - loss: 1.1509 - val_accuracy: 0.7358 - val_loss: 1.5731\n",
            "Epoch 99/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8302 - loss: 1.1990 - val_accuracy: 0.8171 - val_loss: 1.2296\n",
            "Epoch 100/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.8289 - loss: 1.1945 - val_accuracy: 0.8533 - val_loss: 1.1049\n",
            "Model Validation Accuracy: 0.8533\n",
            "Best Validation Accuracy: 0.8533\n",
            "Best Model: <Sequential name=sequential_7, built=True>\n",
            "Best Hyperparameters: Layers=8, Neurons=320, L2=0.0005\n",
            "\n",
            "Testing Model with 12 layers, 352 neurons, L2=0.0005\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 794ms/step - accuracy: 0.1262 - loss: 4.3910 - val_accuracy: 0.0917 - val_loss: 4.0682\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.2064 - loss: 3.7969 - val_accuracy: 0.1008 - val_loss: 3.9582\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.2623 - loss: 3.4423 - val_accuracy: 0.1608 - val_loss: 3.4277\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.3103 - loss: 3.0833 - val_accuracy: 0.2071 - val_loss: 3.2244\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.3596 - loss: 2.8054 - val_accuracy: 0.2492 - val_loss: 3.1817\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.4291 - loss: 2.5178 - val_accuracy: 0.3521 - val_loss: 3.2731\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.4421 - loss: 2.4085 - val_accuracy: 0.4204 - val_loss: 2.4259\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5162 - loss: 2.1270 - val_accuracy: 0.3867 - val_loss: 3.5430\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5196 - loss: 2.0873 - val_accuracy: 0.2221 - val_loss: 4.3113\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.5354 - loss: 1.9871 - val_accuracy: 0.5962 - val_loss: 1.8968\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5631 - loss: 1.9072 - val_accuracy: 0.5479 - val_loss: 1.9730\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5717 - loss: 1.8786 - val_accuracy: 0.5221 - val_loss: 2.0625\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.5809 - loss: 1.8618 - val_accuracy: 0.6008 - val_loss: 1.8656\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6129 - loss: 1.7886 - val_accuracy: 0.5979 - val_loss: 1.8852\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6306 - loss: 1.7508 - val_accuracy: 0.5096 - val_loss: 2.2424\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6398 - loss: 1.7585 - val_accuracy: 0.5529 - val_loss: 2.0548\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6401 - loss: 1.7389 - val_accuracy: 0.5450 - val_loss: 2.2469\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6717 - loss: 1.6557 - val_accuracy: 0.5779 - val_loss: 1.9769\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6702 - loss: 1.7184 - val_accuracy: 0.5575 - val_loss: 2.0705\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.6642 - loss: 1.7165 - val_accuracy: 0.6350 - val_loss: 1.7916\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.6641 - loss: 1.7345 - val_accuracy: 0.6737 - val_loss: 1.6782\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.6886 - loss: 1.6638 - val_accuracy: 0.6954 - val_loss: 1.6531\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6773 - loss: 1.6886 - val_accuracy: 0.6046 - val_loss: 1.9967\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6905 - loss: 1.7000 - val_accuracy: 0.3646 - val_loss: 3.3232\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6864 - loss: 1.7036 - val_accuracy: 0.6862 - val_loss: 1.7260\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7156 - loss: 1.6143 - val_accuracy: 0.6621 - val_loss: 1.7890\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6945 - loss: 1.6918 - val_accuracy: 0.5671 - val_loss: 2.0724\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7026 - loss: 1.6652 - val_accuracy: 0.5858 - val_loss: 2.0353\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6949 - loss: 1.6658 - val_accuracy: 0.6288 - val_loss: 1.9200\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6808 - loss: 1.7146 - val_accuracy: 0.5254 - val_loss: 2.3646\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7323 - loss: 1.6250 - val_accuracy: 0.6925 - val_loss: 1.7495\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7084 - loss: 1.6995 - val_accuracy: 0.6071 - val_loss: 1.9688\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6950 - loss: 1.7086 - val_accuracy: 0.5896 - val_loss: 2.0559\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6953 - loss: 1.7016 - val_accuracy: 0.5567 - val_loss: 2.6385\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7250 - loss: 1.6405 - val_accuracy: 0.6642 - val_loss: 1.7829\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7205 - loss: 1.6452 - val_accuracy: 0.5738 - val_loss: 2.1649\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7098 - loss: 1.7058 - val_accuracy: 0.6933 - val_loss: 1.7406\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7236 - loss: 1.6474 - val_accuracy: 0.5771 - val_loss: 2.4023\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.7350 - loss: 1.6469 - val_accuracy: 0.6996 - val_loss: 1.7105\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7123 - loss: 1.6944 - val_accuracy: 0.6800 - val_loss: 1.8523\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7137 - loss: 1.7035 - val_accuracy: 0.5879 - val_loss: 2.1940\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7381 - loss: 1.6594 - val_accuracy: 0.6375 - val_loss: 1.9349\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7351 - loss: 1.6062 - val_accuracy: 0.4267 - val_loss: 4.1596\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7280 - loss: 1.6775 - val_accuracy: 0.6179 - val_loss: 2.1556\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.7202 - loss: 1.7017 - val_accuracy: 0.7004 - val_loss: 1.8396\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7283 - loss: 1.6932 - val_accuracy: 0.6758 - val_loss: 1.8831\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7268 - loss: 1.6806 - val_accuracy: 0.6804 - val_loss: 1.8075\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7469 - loss: 1.6006 - val_accuracy: 0.6675 - val_loss: 1.8482\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7194 - loss: 1.6912 - val_accuracy: 0.6654 - val_loss: 2.0958\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7351 - loss: 1.6635 - val_accuracy: 0.6079 - val_loss: 2.1551\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7413 - loss: 1.6616 - val_accuracy: 0.6942 - val_loss: 1.7601\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7381 - loss: 1.6449 - val_accuracy: 0.6642 - val_loss: 1.8626\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7269 - loss: 1.6569 - val_accuracy: 0.4829 - val_loss: 2.8631\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7286 - loss: 1.6632 - val_accuracy: 0.6817 - val_loss: 1.7603\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.7466 - loss: 1.6371 - val_accuracy: 0.7525 - val_loss: 1.5409\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7394 - loss: 1.6215 - val_accuracy: 0.5967 - val_loss: 2.0584\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7505 - loss: 1.6166 - val_accuracy: 0.6662 - val_loss: 1.8134\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7439 - loss: 1.6046 - val_accuracy: 0.7042 - val_loss: 1.8208\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7545 - loss: 1.6097 - val_accuracy: 0.7208 - val_loss: 1.6759\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7597 - loss: 1.5736 - val_accuracy: 0.6625 - val_loss: 2.2622\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7443 - loss: 1.6104 - val_accuracy: 0.6596 - val_loss: 1.9983\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7434 - loss: 1.6787 - val_accuracy: 0.6746 - val_loss: 1.7794\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.7554 - loss: 1.6120 - val_accuracy: 0.7533 - val_loss: 1.6168\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7464 - loss: 1.6365 - val_accuracy: 0.6908 - val_loss: 1.7430\n",
            "Epoch 65/100\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7494 - loss: 1.6146"
          ]
        }
      ],
      "source": [
        "#third random search - focus on 6-13 layers with few hundred units\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "# TPU initialization\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Leave tpu= argument empty for Colab TPU\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# Define the hyperparameter space\n",
        "num_layers_options = list(range(6, 13))  # Reduce maximum number of layers to avoid excessive downsampling\n",
        "neurons_options = list(range(256, 360, 32))\n",
        "regs = [0.0005, 0.001, 0.0015]\n",
        "\n",
        "# Set the number of random searches\n",
        "n_random_searches = 10\n",
        "\n",
        "best_model = None\n",
        "best_val_acc = 0\n",
        "best_hyperparams = None\n",
        "\n",
        "results_G = []\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Initial Random Search\n",
        "for _ in range(n_random_searches):\n",
        "    # Randomly select a combination of hyperparameters\n",
        "    num_layers = random.choice(num_layers_options)\n",
        "    neurons = random.choice(neurons_options)\n",
        "    reg = random.choice(regs)\n",
        "\n",
        "    print(f\"\\nTesting Model with {num_layers} layers, {neurons} neurons, L2={reg}\")\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(28, 28, 3)))\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            model.add(Conv2D(filters=neurons, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(LeakyReLU())\n",
        "            # Apply MaxPooling2D every other layer to avoid excessive downsampling\n",
        "            if i % 2 == 1:\n",
        "                model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(neurons, activation=\"relu\"))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "        optimizer = AdamW(learning_rate=0.001)\n",
        "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, verbose=0)\n",
        "        checkpoint = ModelCheckpoint('model_checkpoint.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "        csv_logger = CSVLogger('training_log.csv', append=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            datagen.flow(X_train, y_train, batch_size=64),\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            epochs=100,  # Reduced number of epochs\n",
        "            callbacks=[early_stopping, checkpoint, csv_logger],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    best_epoch = np.argmax(history.history['val_accuracy'])\n",
        "    train_acc = history.history['accuracy'][best_epoch]\n",
        "    val_acc = history.history['val_accuracy'][best_epoch]\n",
        "    print(f\"Model Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    results_G.append({\n",
        "        \"num_layers\": num_layers,\n",
        "        \"neurons\": neurons,\n",
        "        \"l2_reg\": reg,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_acc\": val_acc\n",
        "    })\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = model\n",
        "        best_hyperparams = (num_layers, neurons, reg)\n",
        "\n",
        "        print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "        print(f\"Best Model: {best_model}\")\n",
        "        print(f\"Best Hyperparameters: Layers={num_layers}, Neurons={neurons}, L2={reg}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V81_UdAoZC7L",
        "outputId": "474a5c1a-89b1-4caa-c860-b4c7a77ccd37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy: 0.8533\n",
            "Best Model: <Sequential name=sequential_7, built=True>\n",
            "Best Hyperparameters: Layers=8, Neurons=256, L2=0.001\n"
          ]
        }
      ],
      "source": [
        "\n",
        "        print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "        print(f\"Best Model: {best_model}\")\n",
        "        print(f\"Best Hyperparameters: Layers={num_layers}, Neurons={neurons}, L2={reg}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiiuR5NOuWCD",
        "outputId": "fe79e3e1-e194-4f00-bc41-93437f790e5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Model with 8 layers, 312 neurons, L2=0.0005, Activation=leaky_relu\n",
            "Model Validation Accuracy: 0.8500\n",
            "Best Validation Accuracy: 0.8500\n",
            "Best Hyperparameters: Layers=8, Neurons=312, L2=0.0005, Activation=leaky_relu\n",
            "\n",
            "Testing Model with 8 layers, 312 neurons, L2=0.0001, Activation=leaky_relu\n",
            "Model Validation Accuracy: 0.8788\n",
            "Best Validation Accuracy: 0.8788\n",
            "Best Hyperparameters: Layers=8, Neurons=312, L2=0.0001, Activation=leaky_relu\n",
            "\n",
            "Testing Model with 8 layers, 280 neurons, L2=0.0005, Activation=gelu\n",
            "Model Validation Accuracy: 0.8687\n",
            "\n",
            "Testing Model with 8 layers, 328 neurons, L2=0.001, Activation=leaky_relu\n",
            "Model Validation Accuracy: 0.8263\n",
            "\n",
            "Testing Model with 8 layers, 312 neurons, L2=0.001, Activation=leaky_relu\n",
            "Model Validation Accuracy: 0.8117\n",
            "\n",
            "Testing Model with 8 layers, 312 neurons, L2=0.0001, Activation=leaky_relu\n",
            "Model Validation Accuracy: 0.8829\n",
            "Best Validation Accuracy: 0.8829\n",
            "Best Hyperparameters: Layers=8, Neurons=312, L2=0.0001, Activation=leaky_relu\n",
            "\n",
            "Testing Model with 8 layers, 296 neurons, L2=0.001, Activation=gelu\n",
            "Model Validation Accuracy: 0.8542\n",
            "\n",
            "Testing Model with 8 layers, 296 neurons, L2=0.0001, Activation=leaky_relu\n",
            "Model Validation Accuracy: 0.8775\n",
            "\n",
            "Testing Model with 8 layers, 296 neurons, L2=0.0005, Activation=leaky_relu\n",
            "Model Validation Accuracy: 0.8446\n",
            "\n",
            "Testing Model with 8 layers, 200 neurons, L2=0.001, Activation=gelu\n",
            "Model Validation Accuracy: 0.8704\n",
            "\n",
            "Testing Model with 8 layers, 344 neurons, L2=0.0001, Activation=gelu\n",
            "Model Validation Accuracy: 0.9008\n",
            "Best Validation Accuracy: 0.9008\n",
            "Best Hyperparameters: Layers=8, Neurons=344, L2=0.0001, Activation=gelu\n",
            "\n",
            "Testing Model with 8 layers, 264 neurons, L2=0.001, Activation=leaky_relu\n",
            "Model Validation Accuracy: 0.8250\n",
            "\n",
            "Testing Model with 8 layers, 344 neurons, L2=0.001, Activation=leaky_relu\n",
            "Model Validation Accuracy: 0.7983\n",
            "\n",
            "Testing Model with 8 layers, 296 neurons, L2=0.001, Activation=leaky_relu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#fourth random search - focus on layer 8 depth and L2 regularization. Also vary activation between leaky_relu and gelu\n",
        "\n",
        "import random\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "# TPU initialization\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Leave tpu= argument empty for Colab TPU\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# Define the hyperparameter space\n",
        "num_layers_options = list(range(8, 9))  # Reduce maximum number of layers to avoid excessive downsampling\n",
        "neurons_options = list(range(200, 360, 16))\n",
        "regs = [0.0005, 0.001, 0.0001]\n",
        "activation_options = ['leaky_relu', 'gelu']  # Add activation functions to explore\n",
        "\n",
        "# Set the number of random searches\n",
        "n_random_searches = 20\n",
        "\n",
        "best_model = None\n",
        "best_val_acc = 0\n",
        "best_hyperparams = None\n",
        "\n",
        "results_G = []\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Initial Random Search\n",
        "for _ in range(n_random_searches):\n",
        "    # Randomly select a combination of hyperparameters\n",
        "    num_layers = random.choice(num_layers_options)\n",
        "    neurons = random.choice(neurons_options)\n",
        "    reg = random.choice(regs)\n",
        "    activation = random.choice(activation_options)\n",
        "\n",
        "    print(f\"\\nTesting Model with {num_layers} layers, {neurons} neurons, L2={reg}, Activation={activation}\")\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(28, 28, 3)))\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            model.add(Conv2D(filters=neurons, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            if activation == 'leaky_relu':\n",
        "                model.add(LeakyReLU())\n",
        "            elif activation == 'gelu':\n",
        "                model.add(tf.keras.layers.Activation(tf.keras.activations.gelu))\n",
        "            # Apply MaxPooling2D every other layer to avoid excessive downsampling\n",
        "            if i % 2 == 1:\n",
        "                model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(neurons, activation=\"relu\"))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "        optimizer = AdamW(learning_rate=0.001)\n",
        "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, verbose=0)\n",
        "        checkpoint = ModelCheckpoint('model_checkpoint.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "        csv_logger = CSVLogger('training_log.csv', append=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            datagen.flow(X_train, y_train, batch_size=64),\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            epochs=100,  # Reduced number of epochs\n",
        "            callbacks=[early_stopping, checkpoint, csv_logger],\n",
        "            verbose=0  # Set verbosity to 0 to reduce output\n",
        "        )\n",
        "\n",
        "    best_epoch = np.argmax(history.history['val_accuracy'])\n",
        "    train_acc = history.history['accuracy'][best_epoch]\n",
        "    val_acc = history.history['val_accuracy'][best_epoch]\n",
        "    print(f\"Model Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    results_G.append({\n",
        "        \"num_layers\": num_layers,\n",
        "        \"neurons\": neurons,\n",
        "        \"l2_reg\": reg,\n",
        "        \"activation\": activation,  # Add activation to results\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_acc\": val_acc\n",
        "    })\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = model\n",
        "        best_hyperparams = (num_layers, neurons, reg, activation)\n",
        "\n",
        "        print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "        print(f\"Best Hyperparameters: Layers={num_layers}, Neurons={neurons}, L2={reg}, Activation={activation}\")\n",
        "\n",
        "# Summary of Results\n",
        "df_results_G = pd.DataFrame(results_G)\n",
        "print(f\"\\nBest Hyperparameters: {best_hyperparams}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw-UOimXd5hl"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame to a CSV file in your Google Drive\n",
        "df_results_G.to_csv('/content/drive/My Drive/results_r_layer8.csv', index=False)\n",
        "\n",
        "print(\"Results saved to Google Drive at /content/drive/My Drive/results_r_layer8.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeVVNSuCd5mQ",
        "outputId": "2df46027-4861-4c4d-98cd-cbe5e19b106b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Model with 8 layers, 332 neurons, L2=0.0001, Activation=gelu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Validation Accuracy: 0.8975\n",
            "Best Validation Accuracy: 0.8975\n",
            "Best Hyperparameters: Layers=8, Neurons=332, L2=0.0001, Activation=gelu\n",
            "\n",
            "Testing Model with 8 layers, 396 neurons, L2=5e-05, Activation=gelu\n",
            "Model Validation Accuracy: 0.8975\n",
            "\n",
            "Testing Model with 8 layers, 380 neurons, L2=5e-05, Activation=gelu\n",
            "Model Validation Accuracy: 0.9021\n",
            "Best Validation Accuracy: 0.9021\n",
            "Best Hyperparameters: Layers=8, Neurons=380, L2=5e-05, Activation=gelu\n",
            "\n",
            "Testing Model with 8 layers, 364 neurons, L2=0.0001, Activation=gelu\n",
            "Model Validation Accuracy: 0.9067\n",
            "Best Validation Accuracy: 0.9067\n",
            "Best Hyperparameters: Layers=8, Neurons=364, L2=0.0001, Activation=gelu\n",
            "\n",
            "Testing Model with 8 layers, 364 neurons, L2=5e-05, Activation=gelu\n",
            "Model Validation Accuracy: 0.9025\n",
            "\n",
            "Testing Model with 8 layers, 300 neurons, L2=1e-05, Activation=gelu\n",
            "Model Validation Accuracy: 0.9033\n",
            "\n",
            "Testing Model with 8 layers, 300 neurons, L2=1e-05, Activation=gelu\n",
            "Model Validation Accuracy: 0.9129\n",
            "Best Validation Accuracy: 0.9129\n",
            "Best Hyperparameters: Layers=8, Neurons=300, L2=1e-05, Activation=gelu\n",
            "\n",
            "Testing Model with 8 layers, 396 neurons, L2=1e-05, Activation=gelu\n",
            "Model Validation Accuracy: 0.9079\n",
            "\n",
            "Testing Model with 8 layers, 364 neurons, L2=0.0001, Activation=gelu\n",
            "Model Validation Accuracy: 0.8929\n",
            "\n",
            "Testing Model with 8 layers, 396 neurons, L2=1e-05, Activation=gelu\n",
            "Model Validation Accuracy: 0.9121\n",
            "\n",
            "Best Hyperparameters: (8, 300, 1e-05, 'gelu')\n",
            "Best Validation Accuracy: 0.9129\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#fifth random search - optimize layer 8 depth and L2 regularization further. Activation gelu\n",
        "\n",
        "import random\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "# TPU initialization\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Leave tpu= argument empty for Colab TPU\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# Define the hyperparameter space\n",
        "num_layers_options = list(range(8, 9))  # Reduce maximum number of layers to avoid excessive downsampling\n",
        "neurons_options = list(range(300, 400, 16))\n",
        "regs = [0.00005, 0.00001, 0.0001]\n",
        "activation_options = ['gelu']  # Add activation functions to explore\n",
        "\n",
        "# Set the number of random searches\n",
        "n_random_searches = 10\n",
        "\n",
        "best_model = None\n",
        "best_val_acc = 0\n",
        "best_hyperparams = None\n",
        "\n",
        "results_G = []\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Initial Random Search\n",
        "for _ in range(n_random_searches):\n",
        "    # Randomly select a combination of hyperparameters\n",
        "    num_layers = random.choice(num_layers_options)\n",
        "    neurons = random.choice(neurons_options)\n",
        "    reg = random.choice(regs)\n",
        "    activation = random.choice(activation_options)\n",
        "\n",
        "    print(f\"\\nTesting Model with {num_layers} layers, {neurons} neurons, L2={reg}, Activation={activation}\")\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(28, 28, 3)))\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            model.add(Conv2D(filters=neurons, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(reg)))\n",
        "            model.add(BatchNormalization())\n",
        "            if activation == 'leaky_relu':\n",
        "                model.add(LeakyReLU())\n",
        "            elif activation == 'gelu':\n",
        "                model.add(tf.keras.layers.Activation(tf.keras.activations.gelu))\n",
        "            # Apply MaxPooling2D every other layer to avoid excessive downsampling\n",
        "            if i % 2 == 1:\n",
        "                model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(neurons, activation=\"relu\"))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "        optimizer = AdamW(learning_rate=0.001)\n",
        "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, verbose=0)\n",
        "        checkpoint = ModelCheckpoint('model_checkpoint.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "        csv_logger = CSVLogger('training_log.csv', append=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            datagen.flow(X_train, y_train, batch_size=64),\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            epochs=100,  # Reduced number of epochs\n",
        "            callbacks=[early_stopping, checkpoint, csv_logger],\n",
        "            verbose=0  # Set verbosity to 0 to reduce output\n",
        "        )\n",
        "\n",
        "    best_epoch = np.argmax(history.history['val_accuracy'])\n",
        "    train_acc = history.history['accuracy'][best_epoch]\n",
        "    val_acc = history.history['val_accuracy'][best_epoch]\n",
        "    print(f\"Model Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    results_G.append({\n",
        "        \"num_layers\": num_layers,\n",
        "        \"neurons\": neurons,\n",
        "        \"l2_reg\": reg,\n",
        "        \"activation\": activation,  # Add activation to results\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_acc\": val_acc\n",
        "    })\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = model\n",
        "        best_hyperparams = (num_layers, neurons, reg, activation)\n",
        "\n",
        "        print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "        print(f\"Best Hyperparameters: Layers={num_layers}, Neurons={neurons}, L2={reg}, Activation={activation}\")\n",
        "\n",
        "# Summary of Results\n",
        "df_results_G = pd.DataFrame(results_G)\n",
        "print(f\"\\nBest Hyperparameters: {best_hyperparams}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHfM0Hl5d5qK",
        "outputId": "5db2b01d-4949-425a-97ec-67f3160551b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to Google Drive at /content/drive/My Drive/results_r_layer8.csv\n"
          ]
        }
      ],
      "source": [
        "# Save the DataFrame to a CSV file in your Google Drive\n",
        "df_results_G.to_csv('/content/drive/My Drive/results_r_layer8.csv', index=False)\n",
        "\n",
        "print(\"Results saved to Google Drive at /content/drive/My Drive/results_r_layer8.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0D0bUYud5tz",
        "outputId": "a93ee944-e508-4ce3-fda7-fc02776cf0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on TPU\n",
            "\n",
            "Testing Data Augmentation Config: {'rotation_range': 10, 'width_shift_range': 0.1, 'height_shift_range': 0.1, 'horizontal_flip': True}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Validation Accuracy: 0.9092\n",
            "New Best Augmentation Config Found!\n",
            "Best Validation Accuracy: 0.9092\n",
            "\n",
            "Testing Data Augmentation Config: {'rotation_range': 10, 'width_shift_range': 0.1, 'height_shift_range': 0.1, 'horizontal_flip': True}\n",
            "Model Validation Accuracy: 0.9071\n",
            "\n",
            "Testing Data Augmentation Config: {'rotation_range': 25, 'width_shift_range': 0.2, 'height_shift_range': 0.2, 'zoom_range': 0.2, 'horizontal_flip': True}\n",
            "Model Validation Accuracy: 0.9079\n",
            "\n",
            "Testing Data Augmentation Config: {'rotation_range': 20, 'width_shift_range': 0.2, 'height_shift_range': 0.2, 'zoom_range': 0.2, 'channel_shift_range': 30, 'horizontal_flip': True}\n",
            "Model Validation Accuracy: 0.1058\n",
            "\n",
            "Testing Data Augmentation Config: {'rotation_range': 15, 'width_shift_range': 0.15, 'height_shift_range': 0.15, 'shear_range': 10, 'brightness_range': [0.7, 1.3]}\n",
            "Model Validation Accuracy: 0.1308\n",
            "\n",
            "Testing Data Augmentation Config: {'rotation_range': 25, 'width_shift_range': 0.2, 'height_shift_range': 0.2, 'zoom_range': 0.2, 'horizontal_flip': True}\n",
            "Model Validation Accuracy: 0.9154\n",
            "New Best Augmentation Config Found!\n",
            "Best Validation Accuracy: 0.9154\n",
            "\n",
            "Testing Data Augmentation Config: {'rotation_range': 15, 'width_shift_range': 0.15, 'height_shift_range': 0.15, 'shear_range': 10, 'brightness_range': [0.7, 1.3]}\n",
            "Model Validation Accuracy: 0.1221\n",
            "\n",
            "Testing Data Augmentation Config: {'rotation_range': 10, 'width_shift_range': 0.1, 'height_shift_range': 0.1, 'horizontal_flip': True}\n",
            "Model Validation Accuracy: 0.9050\n",
            "\n",
            "Testing Data Augmentation Config: {'rotation_range': 25, 'width_shift_range': 0.2, 'height_shift_range': 0.2, 'shear_range': 15, 'brightness_range': [0.5, 1.5]}\n",
            "Model Validation Accuracy: 0.1138\n",
            "\n",
            "Testing Data Augmentation Config: {'rotation_range': 20, 'width_shift_range': 0.2, 'height_shift_range': 0.2, 'zoom_range': 0.2, 'channel_shift_range': 30, 'horizontal_flip': True}\n",
            "Model Validation Accuracy: 0.1092\n",
            "\n",
            "Best Data Augmentation Configuration:\n",
            "{'rotation_range': 25, 'width_shift_range': 0.2, 'height_shift_range': 0.2, 'zoom_range': 0.2, 'horizontal_flip': True}\n",
            "Best Validation Accuracy: 0.9154\n"
          ]
        }
      ],
      "source": [
        "# explore data augmentation further for 8 layers, 300 neurons, l2=1e-e05, gelu\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Input, Conv2D, Flatten, Dense, Dropout,\n",
        "                                     BatchNormalization, MaxPooling2D, GlobalAveragePooling2D)\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# TPU initialization (if available)\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    print(\"Running on TPU\")\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()  # Use normal strategy if TPU not available\n",
        "    print(\"Running on CPU/GPU\")\n",
        "\n",
        "# Define the hyperparameter space\n",
        "num_layers_options = [8]  # Fixed at 8 layers\n",
        "neurons_options = [300]   # Fixed at 300 neurons\n",
        "regs = [1e-5]  # Fixed L2 regularization\n",
        "activation_options = ['gelu']\n",
        "\n",
        "# Number of trials for data augmentation testing\n",
        "n_augmentation_trials = 10\n",
        "\n",
        "best_model = None\n",
        "best_val_acc = 0\n",
        "best_hyperparams = None\n",
        "\n",
        "results_G = []\n",
        "\n",
        "# **Improved Data Augmentation Configurations**\n",
        "augmentation_configs = [\n",
        "    {\"rotation_range\": 10, \"width_shift_range\": 0.1, \"height_shift_range\": 0.1, \"horizontal_flip\": True},\n",
        "    {\"rotation_range\": 25, \"width_shift_range\": 0.2, \"height_shift_range\": 0.2, \"zoom_range\": 0.2, \"horizontal_flip\": True},\n",
        "    {\"rotation_range\": 15, \"width_shift_range\": 0.15, \"height_shift_range\": 0.15, \"shear_range\": 10, \"brightness_range\": [0.7, 1.3]},\n",
        "    {\"rotation_range\": 20, \"width_shift_range\": 0.2, \"height_shift_range\": 0.2, \"zoom_range\": 0.2, \"channel_shift_range\": 30, \"horizontal_flip\": True},\n",
        "    {\"rotation_range\": 25, \"width_shift_range\": 0.2, \"height_shift_range\": 0.2, \"shear_range\": 15, \"brightness_range\": [0.5, 1.5]},\n",
        "]\n",
        "\n",
        "# Run augmentation trials\n",
        "for trial in range(n_augmentation_trials):\n",
        "    aug_config = random.choice(augmentation_configs)\n",
        "\n",
        "    print(f\"\\nTesting Data Augmentation Config: {aug_config}\")\n",
        "\n",
        "    datagen = ImageDataGenerator(**aug_config, fill_mode='nearest')\n",
        "    datagen.fit(X_train)\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(128, 128, 3)))  # Increased input size for Simpsons details\n",
        "\n",
        "        for i in range(8):  # Fixed 8 layers\n",
        "            model.add(Conv2D(filters=300, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(1e-5)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(tf.keras.layers.Activation(tf.keras.activations.gelu))\n",
        "            if i % 2 == 1:\n",
        "                model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(300, activation=\"relu\"))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "        optimizer = AdamW(learning_rate=0.001)\n",
        "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, verbose=0)\n",
        "        checkpoint = ModelCheckpoint('model_checkpoint.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "        csv_logger = CSVLogger('training_log.csv', append=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            datagen.flow(X_train, y_train, batch_size=64),\n",
        "            validation_data=(X_valid, y_valid),\n",
        "            epochs=75,  # Reduced epochs for efficiency\n",
        "            callbacks=[early_stopping, checkpoint, csv_logger],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "    best_epoch = np.argmax(history.history['val_accuracy'])\n",
        "    train_acc = history.history['accuracy'][best_epoch]\n",
        "    val_acc = history.history['val_accuracy'][best_epoch]\n",
        "    print(f\"Model Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    results_G.append({\n",
        "        \"augmentation_config\": aug_config,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_acc\": val_acc\n",
        "    })\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = model\n",
        "        best_hyperparams = aug_config\n",
        "\n",
        "        print(f\"New Best Augmentation Config Found!\")\n",
        "        print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "# Summary of Augmentation Results\n",
        "df_results_G = pd.DataFrame(results_G)\n",
        "print(\"\\nBest Data Augmentation Configuration:\")\n",
        "print(best_hyperparams)\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nN63sKH0d5x2",
        "outputId": "3fee48c6-db37-4462-ca35-97243292f570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to Google Drive at /content/drive/My Drive/results_augm_layer8.csv\n"
          ]
        }
      ],
      "source": [
        "# Save the DataFrame to a CSV file in your Google Drive\n",
        "df_results_G.to_csv('/content/drive/My Drive/results_augm_layer8.csv', index=False)\n",
        "\n",
        "print(\"Results saved to Google Drive at /content/drive/My Drive/results_augm_layer8.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lTKJio0d51f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a940d9de-9302-46df-8d5e-60dc7af7f40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Model with 8 layers, 300 neurons, L2=1e-05, Activation=gelu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 675ms/step - accuracy: 0.1170 - loss: 2.5504 - val_accuracy: 0.1396 - val_loss: 2.3054\n",
            "Epoch 2/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.2245 - loss: 2.1939 - val_accuracy: 0.1025 - val_loss: 2.4213\n",
            "Epoch 3/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3412 - loss: 1.9054 - val_accuracy: 0.1358 - val_loss: 2.3590\n",
            "Epoch 4/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4303 - loss: 1.6645 - val_accuracy: 0.2321 - val_loss: 2.8711\n",
            "Epoch 5/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.5172 - loss: 1.4415 - val_accuracy: 0.4767 - val_loss: 1.6990\n",
            "Epoch 6/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5798 - loss: 1.2811 - val_accuracy: 0.5383 - val_loss: 1.4058\n",
            "Epoch 7/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6109 - loss: 1.1692 - val_accuracy: 0.4792 - val_loss: 1.8055\n",
            "Epoch 8/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6457 - loss: 1.0777 - val_accuracy: 0.6571 - val_loss: 1.1337\n",
            "Epoch 9/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6672 - loss: 0.9916 - val_accuracy: 0.6812 - val_loss: 0.9517\n",
            "Epoch 10/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7175 - loss: 0.8552 - val_accuracy: 0.6938 - val_loss: 0.9399\n",
            "Epoch 11/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6983 - loss: 0.9211 - val_accuracy: 0.7300 - val_loss: 0.8130\n",
            "Epoch 12/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7122 - loss: 0.8681 - val_accuracy: 0.7029 - val_loss: 1.0541\n",
            "Epoch 13/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7290 - loss: 0.8435 - val_accuracy: 0.7179 - val_loss: 0.9335\n",
            "Epoch 14/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7579 - loss: 0.7682 - val_accuracy: 0.7667 - val_loss: 0.7768\n",
            "Epoch 15/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7814 - loss: 0.7003 - val_accuracy: 0.7525 - val_loss: 0.8000\n",
            "Epoch 16/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.7809 - loss: 0.6825 - val_accuracy: 0.7862 - val_loss: 0.7457\n",
            "Epoch 17/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.7802 - loss: 0.6986 - val_accuracy: 0.8033 - val_loss: 0.6980\n",
            "Epoch 18/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8058 - loss: 0.6518 - val_accuracy: 0.6929 - val_loss: 1.1531\n",
            "Epoch 19/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8138 - loss: 0.6353 - val_accuracy: 0.7717 - val_loss: 0.8297\n",
            "Epoch 20/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.8052 - loss: 0.6630 - val_accuracy: 0.8083 - val_loss: 0.6298\n",
            "Epoch 21/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8231 - loss: 0.6161 - val_accuracy: 0.7533 - val_loss: 0.8414\n",
            "Epoch 22/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8329 - loss: 0.5746 - val_accuracy: 0.7908 - val_loss: 0.7753\n",
            "Epoch 23/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.8266 - loss: 0.6216 - val_accuracy: 0.8492 - val_loss: 0.5561\n",
            "Epoch 24/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8430 - loss: 0.5530 - val_accuracy: 0.7542 - val_loss: 1.0030\n",
            "Epoch 25/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8428 - loss: 0.5324 - val_accuracy: 0.7742 - val_loss: 0.8496\n",
            "Epoch 26/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8456 - loss: 0.5637 - val_accuracy: 0.8188 - val_loss: 0.6039\n",
            "Epoch 27/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.8507 - loss: 0.5394 - val_accuracy: 0.8667 - val_loss: 0.4987\n",
            "Epoch 28/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.8515 - loss: 0.5342 - val_accuracy: 0.8683 - val_loss: 0.5322\n",
            "Epoch 29/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.8534 - loss: 0.5475 - val_accuracy: 0.7721 - val_loss: 0.7997\n",
            "Epoch 30/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8610 - loss: 0.5077 - val_accuracy: 0.8363 - val_loss: 0.6337\n",
            "Epoch 31/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8663 - loss: 0.5027 - val_accuracy: 0.7754 - val_loss: 0.8338\n",
            "Epoch 32/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.8669 - loss: 0.5146 - val_accuracy: 0.8808 - val_loss: 0.4585\n",
            "Epoch 33/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8768 - loss: 0.4819 - val_accuracy: 0.8771 - val_loss: 0.4849\n",
            "Epoch 34/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8763 - loss: 0.4698 - val_accuracy: 0.8783 - val_loss: 0.4807\n",
            "Epoch 35/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8922 - loss: 0.4491 - val_accuracy: 0.8533 - val_loss: 0.5694\n",
            "Epoch 36/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8810 - loss: 0.4733 - val_accuracy: 0.8413 - val_loss: 0.6241\n",
            "Epoch 37/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.8906 - loss: 0.4554 - val_accuracy: 0.8829 - val_loss: 0.4674\n",
            "Epoch 38/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8942 - loss: 0.4280 - val_accuracy: 0.8517 - val_loss: 0.6075\n",
            "Epoch 39/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8892 - loss: 0.4397 - val_accuracy: 0.8687 - val_loss: 0.5434\n",
            "Epoch 40/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8917 - loss: 0.4351 - val_accuracy: 0.8254 - val_loss: 0.6843\n",
            "Epoch 41/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9008 - loss: 0.4222 - val_accuracy: 0.8783 - val_loss: 0.5129\n",
            "Epoch 42/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9001 - loss: 0.4222 - val_accuracy: 0.7750 - val_loss: 1.1077\n",
            "Epoch 43/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8986 - loss: 0.4270 - val_accuracy: 0.8742 - val_loss: 0.5424\n",
            "Epoch 44/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9090 - loss: 0.3943 - val_accuracy: 0.8879 - val_loss: 0.5120\n",
            "Epoch 45/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9069 - loss: 0.4106 - val_accuracy: 0.8846 - val_loss: 0.5139\n",
            "Epoch 46/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9133 - loss: 0.3996 - val_accuracy: 0.8692 - val_loss: 0.5743\n",
            "Epoch 47/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9150 - loss: 0.3915 - val_accuracy: 0.8517 - val_loss: 0.6238\n",
            "Epoch 48/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9085 - loss: 0.3959 - val_accuracy: 0.8629 - val_loss: 0.6145\n",
            "Epoch 49/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9188 - loss: 0.3870 - val_accuracy: 0.8458 - val_loss: 0.7183\n",
            "Epoch 50/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9192 - loss: 0.3739 - val_accuracy: 0.8462 - val_loss: 0.7391\n",
            "Epoch 51/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9134 - loss: 0.4030 - val_accuracy: 0.8746 - val_loss: 0.5801\n",
            "Epoch 52/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9178 - loss: 0.4012 - val_accuracy: 0.8946 - val_loss: 0.4945\n",
            "Epoch 53/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9204 - loss: 0.3912 - val_accuracy: 0.8425 - val_loss: 0.6755\n",
            "Epoch 54/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9297 - loss: 0.3577 - val_accuracy: 0.8950 - val_loss: 0.5331\n",
            "Epoch 55/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9168 - loss: 0.3944 - val_accuracy: 0.8817 - val_loss: 0.5611\n",
            "Epoch 56/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9208 - loss: 0.3809 - val_accuracy: 0.8454 - val_loss: 0.7174\n",
            "Epoch 57/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9227 - loss: 0.3789 - val_accuracy: 0.8483 - val_loss: 0.7210\n",
            "Epoch 58/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9286 - loss: 0.3726 - val_accuracy: 0.8783 - val_loss: 0.5956\n",
            "Epoch 59/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9309 - loss: 0.3777 - val_accuracy: 0.8475 - val_loss: 0.7434\n",
            "Epoch 60/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9272 - loss: 0.3811 - val_accuracy: 0.8771 - val_loss: 0.5993\n",
            "Epoch 61/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9332 - loss: 0.3632 - val_accuracy: 0.8850 - val_loss: 0.6171\n",
            "Epoch 62/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9267 - loss: 0.3807 - val_accuracy: 0.9029 - val_loss: 0.5395\n",
            "Epoch 63/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9313 - loss: 0.3566 - val_accuracy: 0.9017 - val_loss: 0.5063\n",
            "Epoch 64/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9301 - loss: 0.3703 - val_accuracy: 0.8854 - val_loss: 0.6234\n",
            "Epoch 65/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9390 - loss: 0.3530 - val_accuracy: 0.8888 - val_loss: 0.6046\n",
            "Epoch 66/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9389 - loss: 0.3587 - val_accuracy: 0.9004 - val_loss: 0.5190\n",
            "Epoch 67/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9399 - loss: 0.3488 - val_accuracy: 0.9050 - val_loss: 0.5044\n",
            "Epoch 68/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9441 - loss: 0.3343 - val_accuracy: 0.8996 - val_loss: 0.5477\n",
            "Epoch 69/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9449 - loss: 0.3424 - val_accuracy: 0.8675 - val_loss: 0.6425\n",
            "Epoch 70/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9397 - loss: 0.3704 - val_accuracy: 0.8363 - val_loss: 0.8039\n",
            "Epoch 71/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9481 - loss: 0.3442 - val_accuracy: 0.8817 - val_loss: 0.6028\n",
            "Epoch 72/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.9375 - loss: 0.3603 - val_accuracy: 0.9087 - val_loss: 0.5087\n",
            "Epoch 73/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9437 - loss: 0.3415 - val_accuracy: 0.8662 - val_loss: 0.6989\n",
            "Epoch 74/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9410 - loss: 0.3697 - val_accuracy: 0.8496 - val_loss: 0.7732\n",
            "Epoch 75/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9493 - loss: 0.3350 - val_accuracy: 0.8771 - val_loss: 0.6783\n",
            "Epoch 76/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9434 - loss: 0.3556 - val_accuracy: 0.8904 - val_loss: 0.6165\n",
            "Epoch 77/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9474 - loss: 0.3439 - val_accuracy: 0.9071 - val_loss: 0.5462\n",
            "Epoch 78/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9460 - loss: 0.3513 - val_accuracy: 0.8842 - val_loss: 0.6154\n",
            "Epoch 79/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9531 - loss: 0.3438 - val_accuracy: 0.8850 - val_loss: 0.5969\n",
            "Epoch 80/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9516 - loss: 0.3477 - val_accuracy: 0.9046 - val_loss: 0.5390\n",
            "Epoch 81/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9505 - loss: 0.3472 - val_accuracy: 0.8854 - val_loss: 0.6360\n",
            "Epoch 82/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9427 - loss: 0.3591 - val_accuracy: 0.9013 - val_loss: 0.6058\n",
            "Epoch 83/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9501 - loss: 0.3467 - val_accuracy: 0.9033 - val_loss: 0.5254\n",
            "Epoch 84/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9623 - loss: 0.3154 - val_accuracy: 0.8821 - val_loss: 0.7080\n",
            "Epoch 85/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9564 - loss: 0.3280 - val_accuracy: 0.8854 - val_loss: 0.6993\n",
            "Epoch 86/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9563 - loss: 0.3213 - val_accuracy: 0.9058 - val_loss: 0.5961\n",
            "Epoch 87/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9535 - loss: 0.3380 - val_accuracy: 0.9050 - val_loss: 0.5733\n",
            "Epoch 88/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9427 - loss: 0.3677 - val_accuracy: 0.9058 - val_loss: 0.5758\n",
            "Epoch 89/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9487 - loss: 0.3555 - val_accuracy: 0.8963 - val_loss: 0.5741\n",
            "Epoch 90/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9530 - loss: 0.3605 - val_accuracy: 0.8925 - val_loss: 0.5992\n",
            "Epoch 91/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9569 - loss: 0.3341 - val_accuracy: 0.9054 - val_loss: 0.5960\n",
            "Epoch 92/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9596 - loss: 0.3320 - val_accuracy: 0.9129 - val_loss: 0.6000\n",
            "Epoch 93/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9559 - loss: 0.3337 - val_accuracy: 0.9154 - val_loss: 0.5732\n",
            "Epoch 94/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9676 - loss: 0.3208 - val_accuracy: 0.9050 - val_loss: 0.5949\n",
            "Epoch 95/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9578 - loss: 0.3503 - val_accuracy: 0.9100 - val_loss: 0.5500\n",
            "Epoch 96/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9571 - loss: 0.3378 - val_accuracy: 0.9029 - val_loss: 0.6082\n",
            "Epoch 97/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9633 - loss: 0.3175 - val_accuracy: 0.9004 - val_loss: 0.6006\n",
            "Epoch 98/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9499 - loss: 0.3498 - val_accuracy: 0.9058 - val_loss: 0.5876\n",
            "Epoch 99/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9610 - loss: 0.3344 - val_accuracy: 0.9133 - val_loss: 0.5473\n",
            "Epoch 100/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9582 - loss: 0.3425 - val_accuracy: 0.8746 - val_loss: 0.7860\n",
            "Epoch 101/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9622 - loss: 0.3376 - val_accuracy: 0.9025 - val_loss: 0.6629\n",
            "Epoch 102/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9601 - loss: 0.3318 - val_accuracy: 0.8879 - val_loss: 0.6884\n",
            "Epoch 103/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9552 - loss: 0.3548 - val_accuracy: 0.8975 - val_loss: 0.6497\n",
            "Epoch 104/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9517 - loss: 0.3493 - val_accuracy: 0.9100 - val_loss: 0.6374\n",
            "Epoch 105/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9612 - loss: 0.3306 - val_accuracy: 0.9013 - val_loss: 0.6012\n",
            "Epoch 106/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9625 - loss: 0.3437 - val_accuracy: 0.9067 - val_loss: 0.5864\n",
            "Epoch 107/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9542 - loss: 0.3563 - val_accuracy: 0.9100 - val_loss: 0.5698\n",
            "Epoch 108/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9630 - loss: 0.3352 - val_accuracy: 0.8775 - val_loss: 0.7785\n",
            "Epoch 109/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9636 - loss: 0.3385 - val_accuracy: 0.9062 - val_loss: 0.5863\n",
            "Epoch 110/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9674 - loss: 0.3249 - val_accuracy: 0.8892 - val_loss: 0.6816\n",
            "Epoch 111/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9685 - loss: 0.3215 - val_accuracy: 0.9054 - val_loss: 0.6367\n",
            "Epoch 112/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9662 - loss: 0.3301 - val_accuracy: 0.8975 - val_loss: 0.6274\n",
            "Epoch 113/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9650 - loss: 0.3239 - val_accuracy: 0.8896 - val_loss: 0.6938\n",
            "Epoch 114/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9702 - loss: 0.3173 - val_accuracy: 0.9129 - val_loss: 0.5946\n",
            "Epoch 115/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9639 - loss: 0.3277 - val_accuracy: 0.9054 - val_loss: 0.6455\n",
            "Epoch 116/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9685 - loss: 0.3264 - val_accuracy: 0.9058 - val_loss: 0.6009\n",
            "Epoch 117/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9690 - loss: 0.3144 - val_accuracy: 0.9200 - val_loss: 0.5703\n",
            "Epoch 118/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9622 - loss: 0.3399 - val_accuracy: 0.8867 - val_loss: 0.6907\n",
            "Epoch 119/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9653 - loss: 0.3255 - val_accuracy: 0.9071 - val_loss: 0.6167\n",
            "Epoch 120/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9673 - loss: 0.3322 - val_accuracy: 0.9125 - val_loss: 0.5965\n",
            "Epoch 121/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9664 - loss: 0.3277 - val_accuracy: 0.8975 - val_loss: 0.6572\n",
            "Epoch 122/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9656 - loss: 0.3299 - val_accuracy: 0.9108 - val_loss: 0.5831\n",
            "Epoch 123/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9603 - loss: 0.3566 - val_accuracy: 0.8850 - val_loss: 0.6796\n",
            "Epoch 124/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9671 - loss: 0.3305 - val_accuracy: 0.9075 - val_loss: 0.6618\n",
            "Epoch 125/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9713 - loss: 0.3135 - val_accuracy: 0.9096 - val_loss: 0.6175\n",
            "Epoch 126/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9612 - loss: 0.3613 - val_accuracy: 0.9142 - val_loss: 0.5812\n",
            "Epoch 127/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9670 - loss: 0.3364 - val_accuracy: 0.8946 - val_loss: 0.7113\n",
            "Epoch 128/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9631 - loss: 0.3464 - val_accuracy: 0.9025 - val_loss: 0.6469\n",
            "Epoch 129/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9690 - loss: 0.3327 - val_accuracy: 0.8850 - val_loss: 0.7780\n",
            "Epoch 130/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9613 - loss: 0.3493 - val_accuracy: 0.9083 - val_loss: 0.6315\n",
            "Epoch 131/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9723 - loss: 0.3203 - val_accuracy: 0.9087 - val_loss: 0.6056\n",
            "Epoch 132/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9666 - loss: 0.3434 - val_accuracy: 0.8938 - val_loss: 0.6804\n",
            "Epoch 133/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9738 - loss: 0.3227 - val_accuracy: 0.9100 - val_loss: 0.6464\n",
            "Epoch 134/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9718 - loss: 0.3327 - val_accuracy: 0.9017 - val_loss: 0.6697\n",
            "Epoch 135/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9698 - loss: 0.3503 - val_accuracy: 0.8996 - val_loss: 0.6792\n",
            "Epoch 136/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9666 - loss: 0.3385 - val_accuracy: 0.8587 - val_loss: 1.0449\n",
            "Epoch 137/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9583 - loss: 0.3719 - val_accuracy: 0.8929 - val_loss: 0.6821\n",
            "Epoch 138/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9723 - loss: 0.3253 - val_accuracy: 0.8963 - val_loss: 0.6383\n",
            "Epoch 139/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9698 - loss: 0.3291 - val_accuracy: 0.9200 - val_loss: 0.5903\n",
            "Epoch 140/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9613 - loss: 0.3629 - val_accuracy: 0.8967 - val_loss: 0.6862\n",
            "Epoch 141/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.9663 - loss: 0.3582 - val_accuracy: 0.9208 - val_loss: 0.6315\n",
            "Epoch 142/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9713 - loss: 0.3348 - val_accuracy: 0.9158 - val_loss: 0.6080\n",
            "Epoch 143/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9693 - loss: 0.3270 - val_accuracy: 0.9050 - val_loss: 0.6281\n",
            "Epoch 144/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9666 - loss: 0.3399 - val_accuracy: 0.9175 - val_loss: 0.5765\n",
            "Epoch 145/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9714 - loss: 0.3180 - val_accuracy: 0.8900 - val_loss: 0.7407\n",
            "Epoch 146/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9731 - loss: 0.3379 - val_accuracy: 0.9108 - val_loss: 0.6061\n",
            "Epoch 147/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9736 - loss: 0.3267 - val_accuracy: 0.8871 - val_loss: 0.7481\n",
            "Epoch 148/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9764 - loss: 0.3121 - val_accuracy: 0.9054 - val_loss: 0.6284\n",
            "Epoch 149/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9638 - loss: 0.3493 - val_accuracy: 0.9129 - val_loss: 0.6247\n",
            "Epoch 150/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9682 - loss: 0.3438 - val_accuracy: 0.9158 - val_loss: 0.6297\n",
            "Epoch 151/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9714 - loss: 0.3364 - val_accuracy: 0.9237 - val_loss: 0.5619\n",
            "Epoch 152/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9749 - loss: 0.3154 - val_accuracy: 0.9004 - val_loss: 0.7269\n",
            "Epoch 153/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9690 - loss: 0.3489 - val_accuracy: 0.9221 - val_loss: 0.5904\n",
            "Epoch 154/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9707 - loss: 0.3312 - val_accuracy: 0.9142 - val_loss: 0.6521\n",
            "Epoch 155/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9740 - loss: 0.3179 - val_accuracy: 0.8971 - val_loss: 0.7323\n",
            "Epoch 156/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9649 - loss: 0.3713 - val_accuracy: 0.9187 - val_loss: 0.6036\n",
            "Epoch 157/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9747 - loss: 0.3235 - val_accuracy: 0.9129 - val_loss: 0.6247\n",
            "Epoch 158/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9764 - loss: 0.3228 - val_accuracy: 0.9108 - val_loss: 0.6203\n",
            "Epoch 159/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9756 - loss: 0.3260 - val_accuracy: 0.9054 - val_loss: 0.6732\n",
            "Epoch 160/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9762 - loss: 0.3142 - val_accuracy: 0.9121 - val_loss: 0.6721\n",
            "Epoch 161/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9723 - loss: 0.3283 - val_accuracy: 0.9271 - val_loss: 0.5509\n",
            "Epoch 162/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9714 - loss: 0.3481 - val_accuracy: 0.9071 - val_loss: 0.6834\n",
            "Epoch 163/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9655 - loss: 0.3578 - val_accuracy: 0.9071 - val_loss: 0.6168\n",
            "Epoch 164/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9758 - loss: 0.3175 - val_accuracy: 0.9112 - val_loss: 0.6470\n",
            "Epoch 165/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9723 - loss: 0.3318 - val_accuracy: 0.9179 - val_loss: 0.6464\n",
            "Epoch 166/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9738 - loss: 0.3315 - val_accuracy: 0.9146 - val_loss: 0.6236\n",
            "Epoch 167/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9727 - loss: 0.3301 - val_accuracy: 0.9121 - val_loss: 0.6595\n",
            "Epoch 168/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9772 - loss: 0.3263 - val_accuracy: 0.9079 - val_loss: 0.7435\n",
            "Epoch 169/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9735 - loss: 0.3309 - val_accuracy: 0.8979 - val_loss: 0.6703\n",
            "Epoch 170/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9734 - loss: 0.3382 - val_accuracy: 0.9117 - val_loss: 0.6376\n",
            "Epoch 171/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9741 - loss: 0.3290 - val_accuracy: 0.9167 - val_loss: 0.6207\n",
            "Epoch 172/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9772 - loss: 0.3246 - val_accuracy: 0.9033 - val_loss: 0.6712\n",
            "Epoch 173/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9727 - loss: 0.3276 - val_accuracy: 0.9225 - val_loss: 0.5943\n",
            "Epoch 174/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9772 - loss: 0.3151 - val_accuracy: 0.9125 - val_loss: 0.6797\n",
            "Epoch 175/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9767 - loss: 0.3159 - val_accuracy: 0.9108 - val_loss: 0.6422\n",
            "Epoch 176/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9707 - loss: 0.3299 - val_accuracy: 0.9096 - val_loss: 0.6454\n",
            "Epoch 177/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9742 - loss: 0.3223 - val_accuracy: 0.9237 - val_loss: 0.5901\n",
            "Epoch 178/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9780 - loss: 0.3276 - val_accuracy: 0.9004 - val_loss: 0.6841\n",
            "Epoch 179/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9678 - loss: 0.3421 - val_accuracy: 0.9092 - val_loss: 0.6445\n",
            "Epoch 180/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9712 - loss: 0.3465 - val_accuracy: 0.8958 - val_loss: 0.6977\n",
            "Epoch 181/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.9720 - loss: 0.3447 - val_accuracy: 0.9129 - val_loss: 0.6480\n",
            "Epoch 182/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9741 - loss: 0.3303 - val_accuracy: 0.9104 - val_loss: 0.6019\n",
            "Epoch 183/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9782 - loss: 0.3110 - val_accuracy: 0.9221 - val_loss: 0.5925\n",
            "Epoch 184/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9791 - loss: 0.3251 - val_accuracy: 0.9071 - val_loss: 0.7038\n",
            "Epoch 185/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9722 - loss: 0.3420 - val_accuracy: 0.9008 - val_loss: 0.6518\n",
            "Epoch 186/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9763 - loss: 0.3292 - val_accuracy: 0.9092 - val_loss: 0.6339\n",
            "Epoch 187/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9738 - loss: 0.3400 - val_accuracy: 0.9000 - val_loss: 0.7173\n",
            "Epoch 188/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9782 - loss: 0.3253 - val_accuracy: 0.9083 - val_loss: 0.6334\n",
            "Epoch 189/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9737 - loss: 0.3263 - val_accuracy: 0.9104 - val_loss: 0.6544\n",
            "Epoch 190/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9774 - loss: 0.3270 - val_accuracy: 0.9067 - val_loss: 0.6931\n",
            "Epoch 191/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9795 - loss: 0.3140 - val_accuracy: 0.9196 - val_loss: 0.6311\n",
            "Epoch 191: early stopping\n",
            "Restoring model weights from the end of the best epoch: 161.\n",
            "Best Validation Accuracy: 0.9271 at epoch 161\n"
          ]
        }
      ],
      "source": [
        "# run up to 500 epochs on optimal model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, Flatten, Dense, Dropout, BatchNormalization,\n",
        "    MaxPooling2D, GlobalAveragePooling2D\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
        "\n",
        "# TPU initialization\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# Define fixed hyperparameters\n",
        "NUM_LAYERS = 8\n",
        "NEURONS = 300\n",
        "L2_REG = 1e-5\n",
        "ACTIVATION = 'gelu'\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "print(f\"\\nTraining Model with {NUM_LAYERS} layers, {NEURONS} neurons, L2={L2_REG}, Activation={ACTIVATION}\")\n",
        "\n",
        "with strategy.scope():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(28, 28, 3)))\n",
        "\n",
        "    for i in range(NUM_LAYERS):\n",
        "        model.add(Conv2D(filters=NEURONS, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(L2_REG)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(tf.keras.layers.Activation(tf.keras.activations.gelu))\n",
        "        if i % 2 == 1:  # Apply MaxPooling every other layer\n",
        "            model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(NEURONS, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    optimizer = AdamW(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, verbose=1)\n",
        "    checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "    csv_logger = CSVLogger('training_log.csv', append=True)\n",
        "\n",
        "    # Train Model\n",
        "    history = model.fit(\n",
        "        datagen.flow(X_train, y_train, batch_size=64),\n",
        "        validation_data=(X_valid, y_valid),\n",
        "        epochs=500,  # Training for up to 500 epochs\n",
        "        callbacks=[early_stopping, checkpoint, csv_logger],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "# Print best accuracy\n",
        "best_epoch = np.argmax(history.history['val_accuracy'])\n",
        "train_acc = history.history['accuracy'][best_epoch]\n",
        "val_acc = history.history['val_accuracy'][best_epoch]\n",
        "print(f\"Best Validation Accuracy: {val_acc:.4f} at epoch {best_epoch + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIf926JSd55A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf98ae4-67ee-48a3-ad8a-852292a9f2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/drive/MyDrive/Simpsons_Model/best_model.keras\n",
            "Training log saved to: /content/drive/MyDrive/Simpsons_Model/training_log.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define Google Drive path\n",
        "drive_path = \"/content/drive/MyDrive/Simpsons_Model\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Save the model in TensorFlow's `.keras` format\n",
        "model_save_path = os.path.join(drive_path, \"best_model.keras\")\n",
        "model.save(model_save_path)\n",
        "\n",
        "# Save the training log\n",
        "log_save_path = os.path.join(drive_path, \"training_log.csv\")\n",
        "csv_logger = CSVLogger(log_save_path, append=True)\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")\n",
        "print(f\"Training log saved to: {log_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaJzuKtSlBn-"
      },
      "outputs": [],
      "source": [
        "# run up to 500 epochs on optimal model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, Flatten, Dense, Dropout, BatchNormalization,\n",
        "    MaxPooling2D, GlobalAveragePooling2D\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
        "\n",
        "# TPU initialization\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# Define fixed hyperparameters\n",
        "NUM_LAYERS = 8\n",
        "NEURONS = 300\n",
        "L2_REG = 1e-5\n",
        "ACTIVATION = 'gelu'\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "print(f\"\\nTraining Model with {NUM_LAYERS} layers, {NEURONS} neurons, L2={L2_REG}, Activation={ACTIVATION}\")\n",
        "\n",
        "with strategy.scope():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(28, 28, 3)))\n",
        "\n",
        "    for i in range(NUM_LAYERS):\n",
        "        model.add(Conv2D(filters=NEURONS, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(L2_REG)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(tf.keras.layers.Activation(tf.keras.activations.gelu))\n",
        "        if i % 2 == 1:  # Apply MaxPooling every other layer\n",
        "            model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(NEURONS, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    optimizer = AdamW(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, verbose=1)\n",
        "    checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "    csv_logger = CSVLogger('training_log.csv', append=True)\n",
        "\n",
        "    # Train Model\n",
        "    history = model.fit(\n",
        "        datagen.flow(X_train, y_train, batch_size=64),\n",
        "        validation_data=(X_valid, y_valid),\n",
        "        epochs=500,  # Training for up to 500 epochs\n",
        "        callbacks=[early_stopping, checkpoint, csv_logger],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "# Print best accuracy\n",
        "best_epoch = np.argmax(history.history['val_accuracy'])\n",
        "train_acc = history.history['accuracy'][best_epoch]\n",
        "val_acc = history.history['val_accuracy'][best_epoch]\n",
        "print(f\"Best Validation Accuracy: {val_acc:.4f} at epoch {best_epoch + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuqhvCPHlB0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80916c4-bf0d-4d8a-b599-13ac7d476982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 710ms/step - accuracy: 0.1253 - loss: 2.5689 - val_accuracy: 0.0867 - val_loss: 2.4731 - learning_rate: 0.0010\n",
            "Epoch 2/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.2237 - loss: 2.1758 - val_accuracy: 0.1421 - val_loss: 2.3508 - learning_rate: 0.0010\n",
            "Epoch 3/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.3672 - loss: 1.8638 - val_accuracy: 0.1546 - val_loss: 2.3327 - learning_rate: 0.0010\n",
            "Epoch 4/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.4748 - loss: 1.5897 - val_accuracy: 0.3121 - val_loss: 2.1451 - learning_rate: 0.0010\n",
            "Epoch 5/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5496 - loss: 1.3583 - val_accuracy: 0.4367 - val_loss: 1.7597 - learning_rate: 0.0010\n",
            "Epoch 6/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.5993 - loss: 1.2129 - val_accuracy: 0.5250 - val_loss: 1.4687 - learning_rate: 0.0010\n",
            "Epoch 7/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.6364 - loss: 1.0841 - val_accuracy: 0.6379 - val_loss: 1.2482 - learning_rate: 0.0010\n",
            "Epoch 8/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6570 - loss: 1.0349 - val_accuracy: 0.4204 - val_loss: 2.4563 - learning_rate: 0.0010\n",
            "Epoch 9/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6794 - loss: 0.9738 - val_accuracy: 0.6771 - val_loss: 1.0381 - learning_rate: 0.0010\n",
            "Epoch 10/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.6997 - loss: 0.9149 - val_accuracy: 0.6908 - val_loss: 1.0405 - learning_rate: 0.0010\n",
            "Epoch 11/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7177 - loss: 0.8739 - val_accuracy: 0.7104 - val_loss: 0.8894 - learning_rate: 0.0010\n",
            "Epoch 12/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7315 - loss: 0.8355 - val_accuracy: 0.6300 - val_loss: 1.3819 - learning_rate: 0.0010\n",
            "Epoch 13/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7343 - loss: 0.8090 - val_accuracy: 0.7042 - val_loss: 1.1184 - learning_rate: 0.0010\n",
            "Epoch 14/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.7490 - loss: 0.7640 - val_accuracy: 0.7154 - val_loss: 0.9284 - learning_rate: 0.0010\n",
            "Epoch 15/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7590 - loss: 0.7485 - val_accuracy: 0.6608 - val_loss: 1.2694 - learning_rate: 0.0010\n",
            "Epoch 16/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.7857 - loss: 0.6944 - val_accuracy: 0.7671 - val_loss: 0.8259 - learning_rate: 0.0010\n",
            "Epoch 17/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7998 - loss: 0.6577 - val_accuracy: 0.8146 - val_loss: 0.6227 - learning_rate: 0.0010\n",
            "Epoch 18/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7991 - loss: 0.6745 - val_accuracy: 0.7800 - val_loss: 0.7976 - learning_rate: 0.0010\n",
            "Epoch 19/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8067 - loss: 0.6486 - val_accuracy: 0.5521 - val_loss: 2.6695 - learning_rate: 0.0010\n",
            "Epoch 20/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8163 - loss: 0.6518 - val_accuracy: 0.8071 - val_loss: 0.6423 - learning_rate: 0.0010\n",
            "Epoch 21/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8279 - loss: 0.5804 - val_accuracy: 0.7692 - val_loss: 0.8486 - learning_rate: 0.0010\n",
            "Epoch 22/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.8277 - loss: 0.6057 - val_accuracy: 0.8158 - val_loss: 0.6584 - learning_rate: 0.0010\n",
            "Epoch 23/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8228 - loss: 0.5965 - val_accuracy: 0.8037 - val_loss: 0.7296 - learning_rate: 0.0010\n",
            "Epoch 24/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8401 - loss: 0.5509 - val_accuracy: 0.8271 - val_loss: 0.6674 - learning_rate: 0.0010\n",
            "Epoch 25/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8570 - loss: 0.5132 - val_accuracy: 0.8367 - val_loss: 0.6111 - learning_rate: 0.0010\n",
            "Epoch 26/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8502 - loss: 0.5326 - val_accuracy: 0.8279 - val_loss: 0.6702 - learning_rate: 0.0010\n",
            "Epoch 27/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.8579 - loss: 0.5283 - val_accuracy: 0.8433 - val_loss: 0.5956 - learning_rate: 0.0010\n",
            "Epoch 28/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8535 - loss: 0.5248 - val_accuracy: 0.8500 - val_loss: 0.5919 - learning_rate: 0.0010\n",
            "Epoch 29/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8505 - loss: 0.5486 - val_accuracy: 0.8300 - val_loss: 0.6611 - learning_rate: 0.0010\n",
            "Epoch 30/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.8714 - loss: 0.4838 - val_accuracy: 0.8554 - val_loss: 0.5282 - learning_rate: 0.0010\n",
            "Epoch 31/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8837 - loss: 0.4590 - val_accuracy: 0.6675 - val_loss: 1.7667 - learning_rate: 0.0010\n",
            "Epoch 32/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.8698 - loss: 0.4845 - val_accuracy: 0.8646 - val_loss: 0.5163 - learning_rate: 0.0010\n",
            "Epoch 33/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8798 - loss: 0.4672 - val_accuracy: 0.8504 - val_loss: 0.6069 - learning_rate: 0.0010\n",
            "Epoch 34/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8783 - loss: 0.4645 - val_accuracy: 0.8217 - val_loss: 0.7145 - learning_rate: 0.0010\n",
            "Epoch 35/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8754 - loss: 0.4834 - val_accuracy: 0.8217 - val_loss: 0.7215 - learning_rate: 0.0010\n",
            "Epoch 36/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.8944 - loss: 0.4291 - val_accuracy: 0.8658 - val_loss: 0.5378 - learning_rate: 0.0010\n",
            "Epoch 37/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8776 - loss: 0.4637 - val_accuracy: 0.8850 - val_loss: 0.4732 - learning_rate: 0.0010\n",
            "Epoch 38/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8929 - loss: 0.4289 - val_accuracy: 0.8529 - val_loss: 0.6492 - learning_rate: 0.0010\n",
            "Epoch 39/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8899 - loss: 0.4321 - val_accuracy: 0.8125 - val_loss: 0.7621 - learning_rate: 0.0010\n",
            "Epoch 40/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8914 - loss: 0.4421 - val_accuracy: 0.8729 - val_loss: 0.5262 - learning_rate: 0.0010\n",
            "Epoch 41/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9013 - loss: 0.4154 - val_accuracy: 0.8367 - val_loss: 0.7283 - learning_rate: 0.0010\n",
            "Epoch 42/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9006 - loss: 0.4196 - val_accuracy: 0.8796 - val_loss: 0.5272 - learning_rate: 0.0010\n",
            "Epoch 43/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8977 - loss: 0.4412 - val_accuracy: 0.8767 - val_loss: 0.5199 - learning_rate: 0.0010\n",
            "Epoch 44/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9019 - loss: 0.4150 - val_accuracy: 0.8950 - val_loss: 0.4887 - learning_rate: 0.0010\n",
            "Epoch 45/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9122 - loss: 0.4011 - val_accuracy: 0.8571 - val_loss: 0.6545 - learning_rate: 0.0010\n",
            "Epoch 46/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9080 - loss: 0.4052 - val_accuracy: 0.8512 - val_loss: 0.6676 - learning_rate: 0.0010\n",
            "Epoch 47/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9046 - loss: 0.4357 - val_accuracy: 0.8871 - val_loss: 0.5449 - learning_rate: 0.0010\n",
            "Epoch 48/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9144 - loss: 0.3887 - val_accuracy: 0.8388 - val_loss: 0.6861 - learning_rate: 0.0010\n",
            "Epoch 49/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9168 - loss: 0.4002 - val_accuracy: 0.8838 - val_loss: 0.5580 - learning_rate: 0.0010\n",
            "Epoch 50/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9160 - loss: 0.3841 - val_accuracy: 0.8138 - val_loss: 0.7549 - learning_rate: 0.0010\n",
            "Epoch 51/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9140 - loss: 0.3888 - val_accuracy: 0.8637 - val_loss: 0.6502 - learning_rate: 0.0010\n",
            "Epoch 52/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9177 - loss: 0.3833 - val_accuracy: 0.8871 - val_loss: 0.5544 - learning_rate: 0.0010\n",
            "Epoch 53/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9356 - loss: 0.3516 - val_accuracy: 0.8371 - val_loss: 0.7080 - learning_rate: 0.0010\n",
            "Epoch 54/500\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9122 - loss: 0.4096\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9123 - loss: 0.4093 - val_accuracy: 0.8900 - val_loss: 0.5374 - learning_rate: 0.0010\n",
            "Epoch 55/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9301 - loss: 0.3536 - val_accuracy: 0.9075 - val_loss: 0.4740 - learning_rate: 5.0000e-04\n",
            "Epoch 56/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9440 - loss: 0.3224 - val_accuracy: 0.9146 - val_loss: 0.4606 - learning_rate: 5.0000e-04\n",
            "Epoch 57/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9497 - loss: 0.3063 - val_accuracy: 0.9133 - val_loss: 0.4469 - learning_rate: 5.0000e-04\n",
            "Epoch 58/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9491 - loss: 0.2960 - val_accuracy: 0.8925 - val_loss: 0.5202 - learning_rate: 5.0000e-04\n",
            "Epoch 59/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9534 - loss: 0.2818 - val_accuracy: 0.9054 - val_loss: 0.5131 - learning_rate: 5.0000e-04\n",
            "Epoch 60/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9570 - loss: 0.2773 - val_accuracy: 0.8975 - val_loss: 0.5719 - learning_rate: 5.0000e-04\n",
            "Epoch 61/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9588 - loss: 0.2670 - val_accuracy: 0.9162 - val_loss: 0.4572 - learning_rate: 5.0000e-04\n",
            "Epoch 62/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9534 - loss: 0.2853 - val_accuracy: 0.9013 - val_loss: 0.5400 - learning_rate: 5.0000e-04\n",
            "Epoch 63/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9562 - loss: 0.2688 - val_accuracy: 0.9137 - val_loss: 0.4649 - learning_rate: 5.0000e-04\n",
            "Epoch 64/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9573 - loss: 0.2736 - val_accuracy: 0.9079 - val_loss: 0.5248 - learning_rate: 5.0000e-04\n",
            "Epoch 65/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9618 - loss: 0.2550 - val_accuracy: 0.8971 - val_loss: 0.5484 - learning_rate: 5.0000e-04\n",
            "Epoch 66/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9589 - loss: 0.2745 - val_accuracy: 0.9021 - val_loss: 0.5328 - learning_rate: 5.0000e-04\n",
            "Epoch 67/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9546 - loss: 0.2697 - val_accuracy: 0.9121 - val_loss: 0.4848 - learning_rate: 5.0000e-04\n",
            "Epoch 68/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9593 - loss: 0.2717 - val_accuracy: 0.9033 - val_loss: 0.5088 - learning_rate: 5.0000e-04\n",
            "Epoch 69/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9646 - loss: 0.2560 - val_accuracy: 0.9171 - val_loss: 0.4698 - learning_rate: 5.0000e-04\n",
            "Epoch 70/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9608 - loss: 0.2557 - val_accuracy: 0.8983 - val_loss: 0.5789 - learning_rate: 5.0000e-04\n",
            "Epoch 71/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9670 - loss: 0.2594 - val_accuracy: 0.9204 - val_loss: 0.4616 - learning_rate: 5.0000e-04\n",
            "Epoch 72/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9585 - loss: 0.2685 - val_accuracy: 0.9158 - val_loss: 0.4550 - learning_rate: 5.0000e-04\n",
            "Epoch 73/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9625 - loss: 0.2538 - val_accuracy: 0.9196 - val_loss: 0.4874 - learning_rate: 5.0000e-04\n",
            "Epoch 74/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9613 - loss: 0.2616 - val_accuracy: 0.9112 - val_loss: 0.4647 - learning_rate: 5.0000e-04\n",
            "Epoch 75/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9626 - loss: 0.2611 - val_accuracy: 0.9162 - val_loss: 0.4730 - learning_rate: 5.0000e-04\n",
            "Epoch 76/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9668 - loss: 0.2475 - val_accuracy: 0.9100 - val_loss: 0.5096 - learning_rate: 5.0000e-04\n",
            "Epoch 77/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9658 - loss: 0.2619 - val_accuracy: 0.9125 - val_loss: 0.4893 - learning_rate: 5.0000e-04\n",
            "Epoch 78/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9658 - loss: 0.2421 - val_accuracy: 0.9083 - val_loss: 0.5011 - learning_rate: 5.0000e-04\n",
            "Epoch 79/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9660 - loss: 0.2569 - val_accuracy: 0.9121 - val_loss: 0.4868 - learning_rate: 5.0000e-04\n",
            "Epoch 80/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9674 - loss: 0.2553 - val_accuracy: 0.9100 - val_loss: 0.5132 - learning_rate: 5.0000e-04\n",
            "Epoch 81/500\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9628 - loss: 0.2650\n",
            "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9628 - loss: 0.2648 - val_accuracy: 0.9087 - val_loss: 0.5376 - learning_rate: 5.0000e-04\n",
            "Epoch 82/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9727 - loss: 0.2332 - val_accuracy: 0.9212 - val_loss: 0.4471 - learning_rate: 2.5000e-04\n",
            "Epoch 83/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9726 - loss: 0.2278 - val_accuracy: 0.9200 - val_loss: 0.4574 - learning_rate: 2.5000e-04\n",
            "Epoch 84/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9743 - loss: 0.2143 - val_accuracy: 0.9275 - val_loss: 0.4379 - learning_rate: 2.5000e-04\n",
            "Epoch 85/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9790 - loss: 0.2057 - val_accuracy: 0.9208 - val_loss: 0.4648 - learning_rate: 2.5000e-04\n",
            "Epoch 86/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9747 - loss: 0.2155 - val_accuracy: 0.9242 - val_loss: 0.4885 - learning_rate: 2.5000e-04\n",
            "Epoch 87/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.9776 - loss: 0.2052 - val_accuracy: 0.9287 - val_loss: 0.4505 - learning_rate: 2.5000e-04\n",
            "Epoch 88/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9782 - loss: 0.2048 - val_accuracy: 0.9287 - val_loss: 0.4547 - learning_rate: 2.5000e-04\n",
            "Epoch 89/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9791 - loss: 0.1987 - val_accuracy: 0.9221 - val_loss: 0.4812 - learning_rate: 2.5000e-04\n",
            "Epoch 90/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9817 - loss: 0.1942 - val_accuracy: 0.9237 - val_loss: 0.4710 - learning_rate: 2.5000e-04\n",
            "Epoch 91/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9772 - loss: 0.2032 - val_accuracy: 0.9200 - val_loss: 0.4707 - learning_rate: 2.5000e-04\n",
            "Epoch 92/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9807 - loss: 0.2018 - val_accuracy: 0.9254 - val_loss: 0.4621 - learning_rate: 2.5000e-04\n",
            "Epoch 93/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9833 - loss: 0.1856 - val_accuracy: 0.9246 - val_loss: 0.4764 - learning_rate: 2.5000e-04\n",
            "Epoch 94/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9804 - loss: 0.1969 - val_accuracy: 0.9196 - val_loss: 0.5481 - learning_rate: 2.5000e-04\n",
            "Epoch 95/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.9826 - loss: 0.1889 - val_accuracy: 0.9171 - val_loss: 0.4991 - learning_rate: 2.5000e-04\n",
            "Epoch 96/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9767 - loss: 0.2046 - val_accuracy: 0.9175 - val_loss: 0.5261 - learning_rate: 2.5000e-04\n",
            "Epoch 97/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9815 - loss: 0.1951\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9815 - loss: 0.1951 - val_accuracy: 0.9225 - val_loss: 0.4791 - learning_rate: 2.5000e-04\n",
            "Epoch 98/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9824 - loss: 0.1896 - val_accuracy: 0.9283 - val_loss: 0.4529 - learning_rate: 1.2500e-04\n",
            "Epoch 99/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9835 - loss: 0.1810 - val_accuracy: 0.9242 - val_loss: 0.4641 - learning_rate: 1.2500e-04\n",
            "Epoch 100/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9797 - loss: 0.1931 - val_accuracy: 0.9283 - val_loss: 0.4746 - learning_rate: 1.2500e-04\n",
            "Epoch 101/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9863 - loss: 0.1725 - val_accuracy: 0.9287 - val_loss: 0.4700 - learning_rate: 1.2500e-04\n",
            "Epoch 102/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9848 - loss: 0.1809 - val_accuracy: 0.9325 - val_loss: 0.4543 - learning_rate: 1.2500e-04\n",
            "Epoch 103/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9867 - loss: 0.1718 - val_accuracy: 0.9267 - val_loss: 0.4865 - learning_rate: 1.2500e-04\n",
            "Epoch 104/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9831 - loss: 0.1943 - val_accuracy: 0.9275 - val_loss: 0.4791 - learning_rate: 1.2500e-04\n",
            "Epoch 105/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9865 - loss: 0.1753 - val_accuracy: 0.9271 - val_loss: 0.4804 - learning_rate: 1.2500e-04\n",
            "Epoch 106/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9904 - loss: 0.1774 - val_accuracy: 0.9325 - val_loss: 0.4653 - learning_rate: 1.2500e-04\n",
            "Epoch 107/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9918 - loss: 0.1585 - val_accuracy: 0.9300 - val_loss: 0.4650 - learning_rate: 1.2500e-04\n",
            "Epoch 108/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9885 - loss: 0.1647 - val_accuracy: 0.9308 - val_loss: 0.4719 - learning_rate: 1.2500e-04\n",
            "Epoch 109/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9885 - loss: 0.1717 - val_accuracy: 0.9325 - val_loss: 0.4806 - learning_rate: 1.2500e-04\n",
            "Epoch 110/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9887 - loss: 0.1659 - val_accuracy: 0.9221 - val_loss: 0.5106 - learning_rate: 1.2500e-04\n",
            "Epoch 111/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9892 - loss: 0.1661 - val_accuracy: 0.9254 - val_loss: 0.4992 - learning_rate: 1.2500e-04\n",
            "Epoch 112/500\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9893 - loss: 0.1718\n",
            "Epoch 112: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9893 - loss: 0.1717 - val_accuracy: 0.9258 - val_loss: 0.4835 - learning_rate: 1.2500e-04\n",
            "Epoch 113/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9878 - loss: 0.1707 - val_accuracy: 0.9292 - val_loss: 0.4792 - learning_rate: 6.2500e-05\n",
            "Epoch 114/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9845 - loss: 0.1729 - val_accuracy: 0.9283 - val_loss: 0.4702 - learning_rate: 6.2500e-05\n",
            "Epoch 115/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9871 - loss: 0.1711 - val_accuracy: 0.9271 - val_loss: 0.4788 - learning_rate: 6.2500e-05\n",
            "Epoch 116/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9868 - loss: 0.1757 - val_accuracy: 0.9308 - val_loss: 0.4641 - learning_rate: 6.2500e-05\n",
            "Epoch 117/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9955 - loss: 0.1508 - val_accuracy: 0.9304 - val_loss: 0.4727 - learning_rate: 6.2500e-05\n",
            "Epoch 118/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9892 - loss: 0.1673 - val_accuracy: 0.9262 - val_loss: 0.4855 - learning_rate: 6.2500e-05\n",
            "Epoch 119/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9880 - loss: 0.1694 - val_accuracy: 0.9317 - val_loss: 0.4776 - learning_rate: 6.2500e-05\n",
            "Epoch 120/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9888 - loss: 0.1623 - val_accuracy: 0.9333 - val_loss: 0.4700 - learning_rate: 6.2500e-05\n",
            "Epoch 121/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9899 - loss: 0.1625 - val_accuracy: 0.9258 - val_loss: 0.4772 - learning_rate: 6.2500e-05\n",
            "Epoch 122/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9905 - loss: 0.1604 - val_accuracy: 0.9296 - val_loss: 0.4905 - learning_rate: 6.2500e-05\n",
            "Epoch 123/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9881 - loss: 0.1641 - val_accuracy: 0.9304 - val_loss: 0.4819 - learning_rate: 6.2500e-05\n",
            "Epoch 124/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9904 - loss: 0.1605 - val_accuracy: 0.9292 - val_loss: 0.4812 - learning_rate: 6.2500e-05\n",
            "Epoch 125/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9870 - loss: 0.1670 - val_accuracy: 0.9279 - val_loss: 0.4965 - learning_rate: 6.2500e-05\n",
            "Epoch 126/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9918 - loss: 0.1545 - val_accuracy: 0.9287 - val_loss: 0.4809 - learning_rate: 6.2500e-05\n",
            "Epoch 127/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9875 - loss: 0.1683 - val_accuracy: 0.9312 - val_loss: 0.4675 - learning_rate: 6.2500e-05\n",
            "Epoch 128/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9943 - loss: 0.1500 - val_accuracy: 0.9342 - val_loss: 0.4670 - learning_rate: 6.2500e-05\n",
            "Epoch 129/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9935 - loss: 0.1525 - val_accuracy: 0.9300 - val_loss: 0.4660 - learning_rate: 6.2500e-05\n",
            "Epoch 130/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9886 - loss: 0.1630 - val_accuracy: 0.9317 - val_loss: 0.4575 - learning_rate: 6.2500e-05\n",
            "Epoch 131/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9898 - loss: 0.1564 - val_accuracy: 0.9287 - val_loss: 0.4784 - learning_rate: 6.2500e-05\n",
            "Epoch 132/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9888 - loss: 0.1573 - val_accuracy: 0.9317 - val_loss: 0.4805 - learning_rate: 6.2500e-05\n",
            "Epoch 133/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9897 - loss: 0.1573 - val_accuracy: 0.9283 - val_loss: 0.4832 - learning_rate: 6.2500e-05\n",
            "Epoch 134/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9910 - loss: 0.1546 - val_accuracy: 0.9325 - val_loss: 0.4838 - learning_rate: 6.2500e-05\n",
            "Epoch 135/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9920 - loss: 0.1526 - val_accuracy: 0.9333 - val_loss: 0.4811 - learning_rate: 6.2500e-05\n",
            "Epoch 136/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9900 - loss: 0.1583 - val_accuracy: 0.9317 - val_loss: 0.4790 - learning_rate: 6.2500e-05\n",
            "Epoch 137/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9885 - loss: 0.1620 - val_accuracy: 0.9312 - val_loss: 0.4726 - learning_rate: 6.2500e-05\n",
            "Epoch 138/500\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9890 - loss: 0.1711\n",
            "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9891 - loss: 0.1708 - val_accuracy: 0.9304 - val_loss: 0.4794 - learning_rate: 6.2500e-05\n",
            "Epoch 139/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9930 - loss: 0.1503 - val_accuracy: 0.9321 - val_loss: 0.4738 - learning_rate: 3.1250e-05\n",
            "Epoch 140/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9915 - loss: 0.1492 - val_accuracy: 0.9350 - val_loss: 0.4702 - learning_rate: 3.1250e-05\n",
            "Epoch 141/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9908 - loss: 0.1555 - val_accuracy: 0.9346 - val_loss: 0.4662 - learning_rate: 3.1250e-05\n",
            "Epoch 142/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9932 - loss: 0.1503 - val_accuracy: 0.9333 - val_loss: 0.4653 - learning_rate: 3.1250e-05\n",
            "Epoch 143/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9935 - loss: 0.1464 - val_accuracy: 0.9325 - val_loss: 0.4772 - learning_rate: 3.1250e-05\n",
            "Epoch 144/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9934 - loss: 0.1509 - val_accuracy: 0.9342 - val_loss: 0.4752 - learning_rate: 3.1250e-05\n",
            "Epoch 145/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9914 - loss: 0.1520 - val_accuracy: 0.9325 - val_loss: 0.4812 - learning_rate: 3.1250e-05\n",
            "Epoch 146/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9916 - loss: 0.1496 - val_accuracy: 0.9317 - val_loss: 0.4750 - learning_rate: 3.1250e-05\n",
            "Epoch 147/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9916 - loss: 0.1574 - val_accuracy: 0.9329 - val_loss: 0.4683 - learning_rate: 3.1250e-05\n",
            "Epoch 148/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9942 - loss: 0.1517 - val_accuracy: 0.9312 - val_loss: 0.4746 - learning_rate: 3.1250e-05\n",
            "Epoch 149/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9914 - loss: 0.1540 - val_accuracy: 0.9321 - val_loss: 0.4732 - learning_rate: 3.1250e-05\n",
            "Epoch 150/500\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9918 - loss: 0.1504\n",
            "Epoch 150: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.9918 - loss: 0.1503 - val_accuracy: 0.9317 - val_loss: 0.4771 - learning_rate: 3.1250e-05\n",
            "Epoch 151/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9915 - loss: 0.1466 - val_accuracy: 0.9337 - val_loss: 0.4722 - learning_rate: 1.5625e-05\n",
            "Epoch 152/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9911 - loss: 0.1490 - val_accuracy: 0.9337 - val_loss: 0.4705 - learning_rate: 1.5625e-05\n",
            "Epoch 153/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9897 - loss: 0.1598 - val_accuracy: 0.9337 - val_loss: 0.4696 - learning_rate: 1.5625e-05\n",
            "Epoch 154/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9930 - loss: 0.1456 - val_accuracy: 0.9333 - val_loss: 0.4711 - learning_rate: 1.5625e-05\n",
            "Epoch 155/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9932 - loss: 0.1482 - val_accuracy: 0.9333 - val_loss: 0.4703 - learning_rate: 1.5625e-05\n",
            "Epoch 156/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9914 - loss: 0.1589 - val_accuracy: 0.9333 - val_loss: 0.4708 - learning_rate: 1.5625e-05\n",
            "Epoch 157/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9895 - loss: 0.1552 - val_accuracy: 0.9346 - val_loss: 0.4689 - learning_rate: 1.5625e-05\n",
            "Epoch 158/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9952 - loss: 0.1434 - val_accuracy: 0.9337 - val_loss: 0.4721 - learning_rate: 1.5625e-05\n",
            "Epoch 159/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9933 - loss: 0.1433 - val_accuracy: 0.9333 - val_loss: 0.4750 - learning_rate: 1.5625e-05\n",
            "Epoch 160/500\n",
            "\u001b[1m86/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9929 - loss: 0.1487\n",
            "Epoch 160: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.9929 - loss: 0.1487 - val_accuracy: 0.9321 - val_loss: 0.4770 - learning_rate: 1.5625e-05\n",
            "Epoch 161/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9911 - loss: 0.1511 - val_accuracy: 0.9312 - val_loss: 0.4778 - learning_rate: 7.8125e-06\n",
            "Epoch 162/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9928 - loss: 0.1468 - val_accuracy: 0.9300 - val_loss: 0.4769 - learning_rate: 7.8125e-06\n",
            "Epoch 163/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9934 - loss: 0.1508 - val_accuracy: 0.9312 - val_loss: 0.4777 - learning_rate: 7.8125e-06\n",
            "Epoch 164/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9921 - loss: 0.1513 - val_accuracy: 0.9325 - val_loss: 0.4768 - learning_rate: 7.8125e-06\n",
            "Epoch 165/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9911 - loss: 0.1513 - val_accuracy: 0.9321 - val_loss: 0.4774 - learning_rate: 7.8125e-06\n",
            "Epoch 166/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9926 - loss: 0.1474 - val_accuracy: 0.9333 - val_loss: 0.4776 - learning_rate: 7.8125e-06\n",
            "Epoch 167/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9950 - loss: 0.1434 - val_accuracy: 0.9321 - val_loss: 0.4766 - learning_rate: 7.8125e-06\n",
            "Epoch 168/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.9909 - loss: 0.1503 - val_accuracy: 0.9329 - val_loss: 0.4757 - learning_rate: 7.8125e-06\n",
            "Epoch 169/500\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9951 - loss: 0.1412 - val_accuracy: 0.9329 - val_loss: 0.4755 - learning_rate: 7.8125e-06\n",
            "Epoch 170/500\n",
            "\u001b[1m87/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9922 - loss: 0.1509\n",
            "Epoch 170: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.9922 - loss: 0.1509 - val_accuracy: 0.9321 - val_loss: 0.4787 - learning_rate: 7.8125e-06\n",
            "Epoch 170: early stopping\n",
            "Restoring model weights from the end of the best epoch: 140.\n",
            "Best Validation Accuracy: 0.9350\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Input, Conv2D, Activation, Flatten, Dense,\n",
        "                                     Dropout, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D)\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# TPU initialization\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# **Hyperparameters**\n",
        "num_layers = 8\n",
        "neurons = 300\n",
        "l2_reg = 1e-5\n",
        "activation = \"gelu\"\n",
        "learning_rate = 0.001  # Initial LR\n",
        "\n",
        "# **Data Augmentation**\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# **Model Building**\n",
        "with strategy.scope():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(28, 28, 3)))\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        model.add(Conv2D(filters=neurons, kernel_size=(3, 3), padding='same', kernel_regularizer=l2(l2_reg)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(tf.keras.activations.gelu))\n",
        "        if i % 2 == 1:\n",
        "            model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(neurons, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    optimizer = AdamW(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# **Callbacks**\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/simpsons_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "csv_logger = CSVLogger('/content/drive/MyDrive/training_log.csv', append=True)\n",
        "\n",
        "# **Training**\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=64),\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    epochs=500,\n",
        "    callbacks=[early_stopping, reduce_lr, checkpoint, csv_logger],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# **Save Model to Google Drive**\n",
        "model.save('/content/drive/MyDrive/simpsons_final_model.keras')\n",
        "\n",
        "# **Print Final Best Validation Accuracy**\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irU6xh-ilB-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dbe5209-66c2-4c84-ae60-38f133b8a6e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Validation Accuracy: 0.9329\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXxaMLenlCGA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODkV5ZFalCrc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXyGEwXVSQUD",
        "outputId": "c7aa74f4-28f6-4658-83d1-dfedccbdd5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Preprocess all images of entire dataset\n",
        "X_train = pi_resnet(X_train)\n",
        "X_valid = pi_resnet(X_valid)\n",
        "\n",
        "X_train_resized = tf.image.resize(X_train, (32, 32))\n",
        "X_valid_resized = tf.image.resize(X_valid, (32, 32))\n",
        "\n",
        "# Import ResNet50 for transfer learning\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "base_model.trainable = False  # Freeze base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_r5bS3CSQUD",
        "outputId": "9cd7c30a-709f-478a-fbe0-3ed6c6459cf5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "data_augmentation = models.Sequential([\n",
        "    layers.InputLayer(input_shape=input_shape),\n",
        "    layers.RandomFlip('horizontal'),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomTranslation(0.1, 0.1),\n",
        "    #layers.RandomContrast(0.1),\n",
        "])\n",
        "\n",
        "custom_cnn = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "    layers.BatchNormalization(),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "\n",
        "    layers.Conv2D(3, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "])\n",
        "\n",
        "\n",
        "model = models.Sequential([\n",
        "    data_augmentation,\n",
        "    custom_cnn,\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(512),\n",
        "    layers.LeakyReLU(alpha=0.3),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(256),\n",
        "    layers.LeakyReLU(alpha=0.3),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(128),\n",
        "    layers.LeakyReLU(alpha=0.3),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfiA0bXWSQUE",
        "outputId": "35525842-9cd4-4b3c-9723-9424d7d30dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train_resized shape: (5600, 32, 32, 3)\n",
            "y_train shape: (5600, 10)\n",
            "X_valid_resized shape: (2400, 32, 32, 3)\n",
            "y_valid shape: (2400, 10)\n"
          ]
        }
      ],
      "source": [
        "print(f\"X_train_resized shape: {X_train_resized.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid_resized shape: {X_valid_resized.shape}\")\n",
        "print(f\"y_valid shape: {y_valid.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "Pb4T-DhHSQUE",
        "outputId": "bf585123-cb30-432f-e95d-929f949b4790"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">21,519</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ sequential (\u001b[38;5;33mSequential\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │          \u001b[38;5;34m21,519\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2048\u001b[0m)          │      \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,049,088\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,823,833</span> (94.70 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,823,833\u001b[0m (94.70 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,235,923</span> (4.71 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,235,923\u001b[0m (4.71 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,910</span> (89.98 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,910\u001b[0m (89.98 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "Tf_I2UQ9SQUE",
        "outputId": "e96144c9-0bc0-4506-8a6f-a0f62c1fa3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 4s/step - accuracy: 0.1077 - loss: 2.5933 - val_accuracy: 0.0983 - val_loss: 2.3527\n",
            "Epoch 2/1000\n",
            "\u001b[1m42/88\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:38\u001b[0m 3s/step - accuracy: 0.1028 - loss: 2.3637"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-82bca8d2a09c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(X_train_resized, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_resized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     epochs=1000, batch_size=64)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train_resized, y_train,\n",
        "                    validation_data=(X_valid_resized, y_valid),\n",
        "                    epochs=1000, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NV9zHQISQUE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTCdbr5lSQUF",
        "outputId": "d003ae9d-d31c-4077-b49c-719a90260db0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Henri\\python\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"my_model.h5\")  # Saves in HDF5 format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWzbWI9ZSQUF",
        "outputId": "45f6d8aa-57f1-4517-b891-ceac5b41096e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], marker='o', linestyle='none', label='Training Loss')\n",
        "if 'val_loss' in history.history:\n",
        "    plt.plot(history.history['val_loss'], marker='o', linestyle='none', label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], marker='o', linestyle='none', label='Training Accuracy')\n",
        "if 'val_accuracy' in history.history:\n",
        "    plt.plot(history.history['val_accuracy'], marker='o', linestyle='none', label='Validation Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}